{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc576154",
   "metadata": {},
   "source": [
    "# Simple binary classification\n",
    "\n",
    "On this notebook we will review some of the techniques and tools we often encounter when working with classification problems. We will give a glimpse on some of the most commonly used models for binary classification in order to use them on the sonar dataset (https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)). We will choose a performance function to study the performance of each model on the data and determine which suits the problem better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aae6ba",
   "metadata": {},
   "source": [
    "# Reading the data\n",
    "\n",
    "First we read the dataset and give a labe $v_i, 1\\leq i \\leq 60$ to each of the $60$ characteristics, and we call the last column the name $t$ for the type of the material (rock or mine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c93c052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>v1</th><th>v2</th><th>v3</th><th>v4</th><th>v5</th><th>v6</th><th>v7</th><th>v8</th><th>v9</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>208 rows × 61 columns (omitted printing of 52 columns)</p><tr><th>1</th><td>0.02</td><td>0.0371</td><td>0.0428</td><td>0.0207</td><td>0.0954</td><td>0.0986</td><td>0.1539</td><td>0.1601</td><td>0.3109</td></tr><tr><th>2</th><td>0.0453</td><td>0.0523</td><td>0.0843</td><td>0.0689</td><td>0.1183</td><td>0.2583</td><td>0.2156</td><td>0.3481</td><td>0.3337</td></tr><tr><th>3</th><td>0.0262</td><td>0.0582</td><td>0.1099</td><td>0.1083</td><td>0.0974</td><td>0.228</td><td>0.2431</td><td>0.3771</td><td>0.5598</td></tr><tr><th>4</th><td>0.01</td><td>0.0171</td><td>0.0623</td><td>0.0205</td><td>0.0205</td><td>0.0368</td><td>0.1098</td><td>0.1276</td><td>0.0598</td></tr><tr><th>5</th><td>0.0762</td><td>0.0666</td><td>0.0481</td><td>0.0394</td><td>0.059</td><td>0.0649</td><td>0.1209</td><td>0.2467</td><td>0.3564</td></tr><tr><th>6</th><td>0.0286</td><td>0.0453</td><td>0.0277</td><td>0.0174</td><td>0.0384</td><td>0.099</td><td>0.1201</td><td>0.1833</td><td>0.2105</td></tr><tr><th>7</th><td>0.0317</td><td>0.0956</td><td>0.1321</td><td>0.1408</td><td>0.1674</td><td>0.171</td><td>0.0731</td><td>0.1401</td><td>0.2083</td></tr><tr><th>8</th><td>0.0519</td><td>0.0548</td><td>0.0842</td><td>0.0319</td><td>0.1158</td><td>0.0922</td><td>0.1027</td><td>0.0613</td><td>0.1465</td></tr><tr><th>9</th><td>0.0223</td><td>0.0375</td><td>0.0484</td><td>0.0475</td><td>0.0647</td><td>0.0591</td><td>0.0753</td><td>0.0098</td><td>0.0684</td></tr><tr><th>10</th><td>0.0164</td><td>0.0173</td><td>0.0347</td><td>0.007</td><td>0.0187</td><td>0.0671</td><td>0.1056</td><td>0.0697</td><td>0.0962</td></tr><tr><th>11</th><td>0.0039</td><td>0.0063</td><td>0.0152</td><td>0.0336</td><td>0.031</td><td>0.0284</td><td>0.0396</td><td>0.0272</td><td>0.0323</td></tr><tr><th>12</th><td>0.0123</td><td>0.0309</td><td>0.0169</td><td>0.0313</td><td>0.0358</td><td>0.0102</td><td>0.0182</td><td>0.0579</td><td>0.1122</td></tr><tr><th>13</th><td>0.0079</td><td>0.0086</td><td>0.0055</td><td>0.025</td><td>0.0344</td><td>0.0546</td><td>0.0528</td><td>0.0958</td><td>0.1009</td></tr><tr><th>14</th><td>0.009</td><td>0.0062</td><td>0.0253</td><td>0.0489</td><td>0.1197</td><td>0.1589</td><td>0.1392</td><td>0.0987</td><td>0.0955</td></tr><tr><th>15</th><td>0.0124</td><td>0.0433</td><td>0.0604</td><td>0.0449</td><td>0.0597</td><td>0.0355</td><td>0.0531</td><td>0.0343</td><td>0.1052</td></tr><tr><th>16</th><td>0.0298</td><td>0.0615</td><td>0.065</td><td>0.0921</td><td>0.1615</td><td>0.2294</td><td>0.2176</td><td>0.2033</td><td>0.1459</td></tr><tr><th>17</th><td>0.0352</td><td>0.0116</td><td>0.0191</td><td>0.0469</td><td>0.0737</td><td>0.1185</td><td>0.1683</td><td>0.1541</td><td>0.1466</td></tr><tr><th>18</th><td>0.0192</td><td>0.0607</td><td>0.0378</td><td>0.0774</td><td>0.1388</td><td>0.0809</td><td>0.0568</td><td>0.0219</td><td>0.1037</td></tr><tr><th>19</th><td>0.027</td><td>0.0092</td><td>0.0145</td><td>0.0278</td><td>0.0412</td><td>0.0757</td><td>0.1026</td><td>0.1138</td><td>0.0794</td></tr><tr><th>20</th><td>0.0126</td><td>0.0149</td><td>0.0641</td><td>0.1732</td><td>0.2565</td><td>0.2559</td><td>0.2947</td><td>0.411</td><td>0.4983</td></tr><tr><th>21</th><td>0.0473</td><td>0.0509</td><td>0.0819</td><td>0.1252</td><td>0.1783</td><td>0.307</td><td>0.3008</td><td>0.2362</td><td>0.383</td></tr><tr><th>22</th><td>0.0664</td><td>0.0575</td><td>0.0842</td><td>0.0372</td><td>0.0458</td><td>0.0771</td><td>0.0771</td><td>0.113</td><td>0.2353</td></tr><tr><th>23</th><td>0.0099</td><td>0.0484</td><td>0.0299</td><td>0.0297</td><td>0.0652</td><td>0.1077</td><td>0.2363</td><td>0.2385</td><td>0.0075</td></tr><tr><th>24</th><td>0.0115</td><td>0.015</td><td>0.0136</td><td>0.0076</td><td>0.0211</td><td>0.1058</td><td>0.1023</td><td>0.044</td><td>0.0931</td></tr><tr><th>25</th><td>0.0293</td><td>0.0644</td><td>0.039</td><td>0.0173</td><td>0.0476</td><td>0.0816</td><td>0.0993</td><td>0.0315</td><td>0.0736</td></tr><tr><th>26</th><td>0.0201</td><td>0.0026</td><td>0.0138</td><td>0.0062</td><td>0.0133</td><td>0.0151</td><td>0.0541</td><td>0.021</td><td>0.0505</td></tr><tr><th>27</th><td>0.0151</td><td>0.032</td><td>0.0599</td><td>0.105</td><td>0.1163</td><td>0.1734</td><td>0.1679</td><td>0.1119</td><td>0.0889</td></tr><tr><th>28</th><td>0.0177</td><td>0.03</td><td>0.0288</td><td>0.0394</td><td>0.063</td><td>0.0526</td><td>0.0688</td><td>0.0633</td><td>0.0624</td></tr><tr><th>29</th><td>0.01</td><td>0.0275</td><td>0.019</td><td>0.0371</td><td>0.0416</td><td>0.0201</td><td>0.0314</td><td>0.0651</td><td>0.1896</td></tr><tr><th>30</th><td>0.0189</td><td>0.0308</td><td>0.0197</td><td>0.0622</td><td>0.008</td><td>0.0789</td><td>0.144</td><td>0.1451</td><td>0.1789</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccc}\n",
       "\t& v1 & v2 & v3 & v4 & v5 & v6 & v7 & v8 & v9 & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 0.02 & 0.0371 & 0.0428 & 0.0207 & 0.0954 & 0.0986 & 0.1539 & 0.1601 & 0.3109 & $\\dots$ \\\\\n",
       "\t2 & 0.0453 & 0.0523 & 0.0843 & 0.0689 & 0.1183 & 0.2583 & 0.2156 & 0.3481 & 0.3337 & $\\dots$ \\\\\n",
       "\t3 & 0.0262 & 0.0582 & 0.1099 & 0.1083 & 0.0974 & 0.228 & 0.2431 & 0.3771 & 0.5598 & $\\dots$ \\\\\n",
       "\t4 & 0.01 & 0.0171 & 0.0623 & 0.0205 & 0.0205 & 0.0368 & 0.1098 & 0.1276 & 0.0598 & $\\dots$ \\\\\n",
       "\t5 & 0.0762 & 0.0666 & 0.0481 & 0.0394 & 0.059 & 0.0649 & 0.1209 & 0.2467 & 0.3564 & $\\dots$ \\\\\n",
       "\t6 & 0.0286 & 0.0453 & 0.0277 & 0.0174 & 0.0384 & 0.099 & 0.1201 & 0.1833 & 0.2105 & $\\dots$ \\\\\n",
       "\t7 & 0.0317 & 0.0956 & 0.1321 & 0.1408 & 0.1674 & 0.171 & 0.0731 & 0.1401 & 0.2083 & $\\dots$ \\\\\n",
       "\t8 & 0.0519 & 0.0548 & 0.0842 & 0.0319 & 0.1158 & 0.0922 & 0.1027 & 0.0613 & 0.1465 & $\\dots$ \\\\\n",
       "\t9 & 0.0223 & 0.0375 & 0.0484 & 0.0475 & 0.0647 & 0.0591 & 0.0753 & 0.0098 & 0.0684 & $\\dots$ \\\\\n",
       "\t10 & 0.0164 & 0.0173 & 0.0347 & 0.007 & 0.0187 & 0.0671 & 0.1056 & 0.0697 & 0.0962 & $\\dots$ \\\\\n",
       "\t11 & 0.0039 & 0.0063 & 0.0152 & 0.0336 & 0.031 & 0.0284 & 0.0396 & 0.0272 & 0.0323 & $\\dots$ \\\\\n",
       "\t12 & 0.0123 & 0.0309 & 0.0169 & 0.0313 & 0.0358 & 0.0102 & 0.0182 & 0.0579 & 0.1122 & $\\dots$ \\\\\n",
       "\t13 & 0.0079 & 0.0086 & 0.0055 & 0.025 & 0.0344 & 0.0546 & 0.0528 & 0.0958 & 0.1009 & $\\dots$ \\\\\n",
       "\t14 & 0.009 & 0.0062 & 0.0253 & 0.0489 & 0.1197 & 0.1589 & 0.1392 & 0.0987 & 0.0955 & $\\dots$ \\\\\n",
       "\t15 & 0.0124 & 0.0433 & 0.0604 & 0.0449 & 0.0597 & 0.0355 & 0.0531 & 0.0343 & 0.1052 & $\\dots$ \\\\\n",
       "\t16 & 0.0298 & 0.0615 & 0.065 & 0.0921 & 0.1615 & 0.2294 & 0.2176 & 0.2033 & 0.1459 & $\\dots$ \\\\\n",
       "\t17 & 0.0352 & 0.0116 & 0.0191 & 0.0469 & 0.0737 & 0.1185 & 0.1683 & 0.1541 & 0.1466 & $\\dots$ \\\\\n",
       "\t18 & 0.0192 & 0.0607 & 0.0378 & 0.0774 & 0.1388 & 0.0809 & 0.0568 & 0.0219 & 0.1037 & $\\dots$ \\\\\n",
       "\t19 & 0.027 & 0.0092 & 0.0145 & 0.0278 & 0.0412 & 0.0757 & 0.1026 & 0.1138 & 0.0794 & $\\dots$ \\\\\n",
       "\t20 & 0.0126 & 0.0149 & 0.0641 & 0.1732 & 0.2565 & 0.2559 & 0.2947 & 0.411 & 0.4983 & $\\dots$ \\\\\n",
       "\t21 & 0.0473 & 0.0509 & 0.0819 & 0.1252 & 0.1783 & 0.307 & 0.3008 & 0.2362 & 0.383 & $\\dots$ \\\\\n",
       "\t22 & 0.0664 & 0.0575 & 0.0842 & 0.0372 & 0.0458 & 0.0771 & 0.0771 & 0.113 & 0.2353 & $\\dots$ \\\\\n",
       "\t23 & 0.0099 & 0.0484 & 0.0299 & 0.0297 & 0.0652 & 0.1077 & 0.2363 & 0.2385 & 0.0075 & $\\dots$ \\\\\n",
       "\t24 & 0.0115 & 0.015 & 0.0136 & 0.0076 & 0.0211 & 0.1058 & 0.1023 & 0.044 & 0.0931 & $\\dots$ \\\\\n",
       "\t25 & 0.0293 & 0.0644 & 0.039 & 0.0173 & 0.0476 & 0.0816 & 0.0993 & 0.0315 & 0.0736 & $\\dots$ \\\\\n",
       "\t26 & 0.0201 & 0.0026 & 0.0138 & 0.0062 & 0.0133 & 0.0151 & 0.0541 & 0.021 & 0.0505 & $\\dots$ \\\\\n",
       "\t27 & 0.0151 & 0.032 & 0.0599 & 0.105 & 0.1163 & 0.1734 & 0.1679 & 0.1119 & 0.0889 & $\\dots$ \\\\\n",
       "\t28 & 0.0177 & 0.03 & 0.0288 & 0.0394 & 0.063 & 0.0526 & 0.0688 & 0.0633 & 0.0624 & $\\dots$ \\\\\n",
       "\t29 & 0.01 & 0.0275 & 0.019 & 0.0371 & 0.0416 & 0.0201 & 0.0314 & 0.0651 & 0.1896 & $\\dots$ \\\\\n",
       "\t30 & 0.0189 & 0.0308 & 0.0197 & 0.0622 & 0.008 & 0.0789 & 0.144 & 0.1451 & 0.1789 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "208×61 typename(DataFrame). Omitted printing of 54 columns\n",
       "│ Row │ v1      │ v2      │ v3      │ v4      │ v5      │ v6      │ v7      │\n",
       "│     │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │\n",
       "├─────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n",
       "│ 1   │ 0.02    │ 0.0371  │ 0.0428  │ 0.0207  │ 0.0954  │ 0.0986  │ 0.1539  │\n",
       "│ 2   │ 0.0453  │ 0.0523  │ 0.0843  │ 0.0689  │ 0.1183  │ 0.2583  │ 0.2156  │\n",
       "│ 3   │ 0.0262  │ 0.0582  │ 0.1099  │ 0.1083  │ 0.0974  │ 0.228   │ 0.2431  │\n",
       "│ 4   │ 0.01    │ 0.0171  │ 0.0623  │ 0.0205  │ 0.0205  │ 0.0368  │ 0.1098  │\n",
       "│ 5   │ 0.0762  │ 0.0666  │ 0.0481  │ 0.0394  │ 0.059   │ 0.0649  │ 0.1209  │\n",
       "│ 6   │ 0.0286  │ 0.0453  │ 0.0277  │ 0.0174  │ 0.0384  │ 0.099   │ 0.1201  │\n",
       "│ 7   │ 0.0317  │ 0.0956  │ 0.1321  │ 0.1408  │ 0.1674  │ 0.171   │ 0.0731  │\n",
       "│ 8   │ 0.0519  │ 0.0548  │ 0.0842  │ 0.0319  │ 0.1158  │ 0.0922  │ 0.1027  │\n",
       "│ 9   │ 0.0223  │ 0.0375  │ 0.0484  │ 0.0475  │ 0.0647  │ 0.0591  │ 0.0753  │\n",
       "│ 10  │ 0.0164  │ 0.0173  │ 0.0347  │ 0.007   │ 0.0187  │ 0.0671  │ 0.1056  │\n",
       "⋮\n",
       "│ 198 │ 0.0366  │ 0.0421  │ 0.0504  │ 0.025   │ 0.0596  │ 0.0252  │ 0.0958  │\n",
       "│ 199 │ 0.0238  │ 0.0318  │ 0.0422  │ 0.0399  │ 0.0788  │ 0.0766  │ 0.0881  │\n",
       "│ 200 │ 0.0116  │ 0.0744  │ 0.0367  │ 0.0225  │ 0.0076  │ 0.0545  │ 0.111   │\n",
       "│ 201 │ 0.0131  │ 0.0387  │ 0.0329  │ 0.0078  │ 0.0721  │ 0.1341  │ 0.1626  │\n",
       "│ 202 │ 0.0335  │ 0.0258  │ 0.0398  │ 0.057   │ 0.0529  │ 0.1091  │ 0.1709  │\n",
       "│ 203 │ 0.0272  │ 0.0378  │ 0.0488  │ 0.0848  │ 0.1127  │ 0.1103  │ 0.1349  │\n",
       "│ 204 │ 0.0187  │ 0.0346  │ 0.0168  │ 0.0177  │ 0.0393  │ 0.163   │ 0.2028  │\n",
       "│ 205 │ 0.0323  │ 0.0101  │ 0.0298  │ 0.0564  │ 0.076   │ 0.0958  │ 0.099   │\n",
       "│ 206 │ 0.0522  │ 0.0437  │ 0.018   │ 0.0292  │ 0.0351  │ 0.1171  │ 0.1257  │\n",
       "│ 207 │ 0.0303  │ 0.0353  │ 0.049   │ 0.0608  │ 0.0167  │ 0.1354  │ 0.1465  │\n",
       "│ 208 │ 0.026   │ 0.0363  │ 0.0136  │ 0.0272  │ 0.0214  │ 0.0338  │ 0.0655  │"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV, DataFrames\n",
    "\n",
    "data1 = DataFrame(CSV.read(\"sonar1.all-data\", DataFrame))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144867a5",
   "metadata": {},
   "source": [
    "We see that the column of labels is of categorical type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23d52e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208-element PooledArrays.PooledVector{String, UInt32, Vector{UInt32}}:\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " ⋮\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16c991e",
   "metadata": {},
   "source": [
    "The latter is not ideal for some of the models we are going to use, like the regression models, so we need to assign a binary label to the values of $M$ and $R$, namely $1$ and $0$ respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d54dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we call some of the packages that we need for this notebook\n",
    "using Plots\n",
    "using GLM\n",
    "using Lathe\n",
    "using StatsBase\n",
    "using MLBase\n",
    "using ClassImbalance\n",
    "using ROCAnalysis\n",
    "using MLDataPattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5c4b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we replace the categorical labels for numerical ones\n",
    "data1.t .= replace.(data1.t, \"R\" => \"0\");  \n",
    "data1.t .= replace.(data1.t, \"M\" => \"1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f937d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we parse the last column to float number so that the models treat the values as numbers and not strings\n",
    "data1.t = parse.(Float64, data1.t);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b6c6b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208×61 Matrix{Float64}:\n",
       " 0.02    0.0371  0.0428  0.0207  …  0.018   0.0084  0.009   0.0032  0.0\n",
       " 0.0453  0.0523  0.0843  0.0689     0.014   0.0049  0.0052  0.0044  0.0\n",
       " 0.0262  0.0582  0.1099  0.1083     0.0316  0.0164  0.0095  0.0078  0.0\n",
       " 0.01    0.0171  0.0623  0.0205     0.005   0.0044  0.004   0.0117  0.0\n",
       " 0.0762  0.0666  0.0481  0.0394     0.0072  0.0048  0.0107  0.0094  0.0\n",
       " 0.0286  0.0453  0.0277  0.0174  …  0.0057  0.0027  0.0051  0.0062  0.0\n",
       " 0.0317  0.0956  0.1321  0.1408     0.0092  0.0143  0.0036  0.0103  0.0\n",
       " 0.0519  0.0548  0.0842  0.0319     0.0085  0.0047  0.0048  0.0053  0.0\n",
       " 0.0223  0.0375  0.0484  0.0475     0.0065  0.0093  0.0059  0.0022  0.0\n",
       " 0.0164  0.0173  0.0347  0.007      0.0032  0.0035  0.0056  0.004   0.0\n",
       " 0.0039  0.0063  0.0152  0.0336  …  0.0042  0.0003  0.0053  0.0036  0.0\n",
       " 0.0123  0.0309  0.0169  0.0313     0.0026  0.0092  0.0009  0.0044  0.0\n",
       " 0.0079  0.0086  0.0055  0.025      0.0059  0.0058  0.0059  0.0032  0.0\n",
       " ⋮                               ⋱                                  ⋮\n",
       " 0.005   0.0017  0.027   0.045      0.0024  0.0063  0.0017  0.0028  1.0\n",
       " 0.0366  0.0421  0.0504  0.025      0.0025  0.0017  0.0027  0.0027  1.0\n",
       " 0.0238  0.0318  0.0422  0.0399     0.0028  0.0013  0.0035  0.006   1.0\n",
       " 0.0116  0.0744  0.0367  0.0225     0.0037  0.0044  0.0057  0.0035  1.0\n",
       " 0.0131  0.0387  0.0329  0.0078  …  0.004   0.0009  0.0015  0.0085  1.0\n",
       " 0.0335  0.0258  0.0398  0.057      0.0045  0.0022  0.0005  0.0031  1.0\n",
       " 0.0272  0.0378  0.0488  0.0848     0.0054  0.0051  0.0065  0.0103  1.0\n",
       " 0.0187  0.0346  0.0168  0.0177     0.0065  0.0115  0.0193  0.0157  1.0\n",
       " 0.0323  0.0101  0.0298  0.0564     0.0034  0.0032  0.0062  0.0067  1.0\n",
       " 0.0522  0.0437  0.018   0.0292  …  0.014   0.0138  0.0077  0.0031  1.0\n",
       " 0.0303  0.0353  0.049   0.0608     0.0034  0.0079  0.0036  0.0048  1.0\n",
       " 0.026   0.0363  0.0136  0.0272     0.004   0.0036  0.0061  0.0115  1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see the data as a matrix and note that the data is of type Float64\n",
    "A = Matrix(data1[:, 1:61])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b1d95",
   "metadata": {},
   "source": [
    "# Fixing class imbalance\n",
    "\n",
    "Now we perform some previous adjustments to the data before we start training with the models. First, we can see that we have an imbalance on the labels of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7375f50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Float64, Int64} with 2 entries:\n",
       "  0.0 => 97\n",
       "  1.0 => 111"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function countmap makes a dictionary and counts how much of each type has the column of 0's and 1's\n",
    "countmap(data1.t, alg = :dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b56bb7",
   "metadata": {},
   "source": [
    "We see that we have more Rocks than Mines, which is not ideal to use when training since it can yield to the model making false predictions. In order to fix that, we use the smote function from the ClassImbalance package, which basically takes the data and oversamples the smaller class, and undersamples the bigger class, and then balances them. See https://docs.juliahub.com/ClassImbalance/2Pjhq/0.8.7/autodocs/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e238418f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.055190842921485045 0.10654403845014279 … 0.010744737227115586 1.0; 0.06092555568490212 0.10236512329507327 … 0.011536351525261862 1.0; … ; 0.029267295078158225 0.06099083219303244 … 0.010264985135203186 1.0; 0.11509465825283761 0.2071876148765947 … 0.02021217458125348 1.0], [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0  …  1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = data1.t; # 0 = majority, 1 = minority\n",
    "X1 = A\n",
    "X0, y0 = smote(X1, y1, k = 5, pct_under = 150, pct_over = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf79e87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "666×61 Matrix{Float64}:\n",
       " 0.0551908  0.106544   0.114441   0.130378   …  0.0202619   0.0107447   1.0\n",
       " 0.0609256  0.102365   0.109162   0.137642      0.0232983   0.0115364   1.0\n",
       " 0.0545872  0.113696   0.118691   0.142543      0.0212974   0.0114938   1.0\n",
       " 0.0712     0.0901     0.1276     0.1497        0.0095      0.0068      1.0\n",
       " 0.0067     0.0096     0.0024     0.0058        0.0051      0.0031      0.0\n",
       " 0.0084     0.0153     0.0291     0.0432     …  0.0072      0.0045      0.0\n",
       " 0.019      0.0038     0.0642     0.0452        0.0055      0.0122      0.0\n",
       " 0.0131     0.0387     0.0329     0.0078        0.0015      0.0085      1.0\n",
       " 0.0079     0.0086     0.0055     0.025         0.0059      0.0032      0.0\n",
       " 0.0353     0.0713     0.0326     0.0272        0.0093      0.0053      0.0\n",
       " 0.026      0.0192     0.0254     0.0061     …  0.0044      0.0077      0.0\n",
       " 0.0598141  0.05267    0.0553135  0.0466552     0.0120306   0.0101718   1.0\n",
       " 0.0196297  0.0427703  0.0536532  0.0763964     0.0163798   0.00794275  1.0\n",
       " ⋮                                           ⋱                          ⋮\n",
       " 0.013      0.0006     0.0088     0.0456        0.0023      0.0016      0.0\n",
       " 0.0599     0.0474     0.0498     0.0387     …  0.0112      0.01        1.0\n",
       " 0.0330593  0.0629647  0.0599787  0.0897923     0.0104877   0.00624383  1.0\n",
       " 0.01       0.0171     0.0623     0.0205        0.004       0.0117      0.0\n",
       " 0.0253     0.0808     0.0507     0.0244        0.0081      0.0053      0.0\n",
       " 0.0374     0.0586     0.0628     0.0534        0.0098      0.0126      1.0\n",
       " 0.0099     0.0484     0.0299     0.0297     …  0.0106      0.0134      0.0\n",
       " 0.0522     0.0437     0.018      0.0292        0.0077      0.0031      1.0\n",
       " 0.0139     0.0222     0.0089     0.0108        0.0039      0.0048      0.0\n",
       " 0.0220538  0.0395867  0.0586939  0.0782818     0.00840082  0.00584872  1.0\n",
       " 0.0292673  0.0609908  0.0723571  0.0952809     0.0199025   0.010265    1.0\n",
       " 0.115095   0.207188   0.269312   0.378661   …  0.0159435   0.0202122   1.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0113ac70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Float64, Int64} with 2 entries:\n",
       "  0.0 => 333\n",
       "  1.0 => 333"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countmap(y0, alg = :dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c25a3f7",
   "metadata": {},
   "source": [
    "We see that we wound up with a bigger dataset which has equal amount of observations of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d09c650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>x1</th><th>x2</th><th>x3</th><th>x4</th><th>x5</th><th>x6</th><th>x7</th><th>x8</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>666 rows × 61 columns (omitted printing of 53 columns)</p><tr><th>1</th><td>0.0551908</td><td>0.106544</td><td>0.114441</td><td>0.130378</td><td>0.138722</td><td>0.0773536</td><td>0.0988919</td><td>0.0609983</td></tr><tr><th>2</th><td>0.0609256</td><td>0.102365</td><td>0.109162</td><td>0.137642</td><td>0.178345</td><td>0.0792192</td><td>0.115255</td><td>0.0777299</td></tr><tr><th>3</th><td>0.0545872</td><td>0.113696</td><td>0.118691</td><td>0.142543</td><td>0.139727</td><td>0.0606311</td><td>0.0822241</td><td>0.0697095</td></tr><tr><th>4</th><td>0.0712</td><td>0.0901</td><td>0.1276</td><td>0.1497</td><td>0.1284</td><td>0.1165</td><td>0.1285</td><td>0.1684</td></tr><tr><th>5</th><td>0.0067</td><td>0.0096</td><td>0.0024</td><td>0.0058</td><td>0.0197</td><td>0.0618</td><td>0.0432</td><td>0.0951</td></tr><tr><th>6</th><td>0.0084</td><td>0.0153</td><td>0.0291</td><td>0.0432</td><td>0.0951</td><td>0.0752</td><td>0.0414</td><td>0.0259</td></tr><tr><th>7</th><td>0.019</td><td>0.0038</td><td>0.0642</td><td>0.0452</td><td>0.0333</td><td>0.069</td><td>0.0901</td><td>0.1454</td></tr><tr><th>8</th><td>0.0131</td><td>0.0387</td><td>0.0329</td><td>0.0078</td><td>0.0721</td><td>0.1341</td><td>0.1626</td><td>0.1902</td></tr><tr><th>9</th><td>0.0079</td><td>0.0086</td><td>0.0055</td><td>0.025</td><td>0.0344</td><td>0.0546</td><td>0.0528</td><td>0.0958</td></tr><tr><th>10</th><td>0.0353</td><td>0.0713</td><td>0.0326</td><td>0.0272</td><td>0.037</td><td>0.0792</td><td>0.1083</td><td>0.0687</td></tr><tr><th>11</th><td>0.026</td><td>0.0192</td><td>0.0254</td><td>0.0061</td><td>0.0352</td><td>0.0701</td><td>0.1263</td><td>0.108</td></tr><tr><th>12</th><td>0.0598141</td><td>0.05267</td><td>0.0553135</td><td>0.0466552</td><td>0.105536</td><td>0.075782</td><td>0.0851496</td><td>0.0463111</td></tr><tr><th>13</th><td>0.0196297</td><td>0.0427703</td><td>0.0536532</td><td>0.0763964</td><td>0.0842833</td><td>0.0874583</td><td>0.106551</td><td>0.234061</td></tr><tr><th>14</th><td>0.0369696</td><td>0.0506365</td><td>0.0610604</td><td>0.0871369</td><td>0.0913171</td><td>0.0812356</td><td>0.0440225</td><td>0.0953988</td></tr><tr><th>15</th><td>0.0116</td><td>0.0179</td><td>0.0449</td><td>0.1096</td><td>0.1913</td><td>0.0924</td><td>0.0761</td><td>0.1092</td></tr><tr><th>16</th><td>0.0418943</td><td>0.0608925</td><td>0.0782239</td><td>0.0955013</td><td>0.139533</td><td>0.137437</td><td>0.113059</td><td>0.133339</td></tr><tr><th>17</th><td>0.0353</td><td>0.0713</td><td>0.0326</td><td>0.0272</td><td>0.037</td><td>0.0792</td><td>0.1083</td><td>0.0687</td></tr><tr><th>18</th><td>0.111456</td><td>0.190747</td><td>0.248609</td><td>0.345565</td><td>0.332589</td><td>0.163657</td><td>0.175037</td><td>0.0261809</td></tr><tr><th>19</th><td>0.0856</td><td>0.0454</td><td>0.0382</td><td>0.0203</td><td>0.0385</td><td>0.0534</td><td>0.214</td><td>0.311</td></tr><tr><th>20</th><td>0.0235</td><td>0.0291</td><td>0.0749</td><td>0.0519</td><td>0.0227</td><td>0.0834</td><td>0.0677</td><td>0.2002</td></tr><tr><th>21</th><td>0.0129</td><td>0.0141</td><td>0.0309</td><td>0.0375</td><td>0.0767</td><td>0.0787</td><td>0.0662</td><td>0.1108</td></tr><tr><th>22</th><td>0.0291</td><td>0.04</td><td>0.0771</td><td>0.0809</td><td>0.0521</td><td>0.1051</td><td>0.0145</td><td>0.0674</td></tr><tr><th>23</th><td>0.0698957</td><td>0.130168</td><td>0.162643</td><td>0.18585</td><td>0.211958</td><td>0.172065</td><td>0.143629</td><td>0.0676793</td></tr><tr><th>24</th><td>0.0569465</td><td>0.117835</td><td>0.123189</td><td>0.144339</td><td>0.14085</td><td>0.0590995</td><td>0.0862533</td><td>0.0719299</td></tr><tr><th>25</th><td>0.0373</td><td>0.0281</td><td>0.0232</td><td>0.0225</td><td>0.0179</td><td>0.0733</td><td>0.0841</td><td>0.1031</td></tr><tr><th>26</th><td>0.0257</td><td>0.0447</td><td>0.0388</td><td>0.0239</td><td>0.1315</td><td>0.1323</td><td>0.1608</td><td>0.2145</td></tr><tr><th>27</th><td>0.0426858</td><td>0.0466743</td><td>0.0681659</td><td>0.0995467</td><td>0.131364</td><td>0.142627</td><td>0.145632</td><td>0.0978696</td></tr><tr><th>28</th><td>0.0253</td><td>0.0808</td><td>0.0507</td><td>0.0244</td><td>0.1724</td><td>0.3823</td><td>0.3729</td><td>0.3583</td></tr><tr><th>29</th><td>0.0193503</td><td>0.0381769</td><td>0.0347308</td><td>0.0458504</td><td>0.0498448</td><td>0.127742</td><td>0.164384</td><td>0.216457</td></tr><tr><th>30</th><td>0.053528</td><td>0.106998</td><td>0.114419</td><td>0.13415</td><td>0.129678</td><td>0.0545189</td><td>0.081217</td><td>0.0702952</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccc}\n",
       "\t& x1 & x2 & x3 & x4 & x5 & x6 & x7 & x8 & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 0.0551908 & 0.106544 & 0.114441 & 0.130378 & 0.138722 & 0.0773536 & 0.0988919 & 0.0609983 & $\\dots$ \\\\\n",
       "\t2 & 0.0609256 & 0.102365 & 0.109162 & 0.137642 & 0.178345 & 0.0792192 & 0.115255 & 0.0777299 & $\\dots$ \\\\\n",
       "\t3 & 0.0545872 & 0.113696 & 0.118691 & 0.142543 & 0.139727 & 0.0606311 & 0.0822241 & 0.0697095 & $\\dots$ \\\\\n",
       "\t4 & 0.0712 & 0.0901 & 0.1276 & 0.1497 & 0.1284 & 0.1165 & 0.1285 & 0.1684 & $\\dots$ \\\\\n",
       "\t5 & 0.0067 & 0.0096 & 0.0024 & 0.0058 & 0.0197 & 0.0618 & 0.0432 & 0.0951 & $\\dots$ \\\\\n",
       "\t6 & 0.0084 & 0.0153 & 0.0291 & 0.0432 & 0.0951 & 0.0752 & 0.0414 & 0.0259 & $\\dots$ \\\\\n",
       "\t7 & 0.019 & 0.0038 & 0.0642 & 0.0452 & 0.0333 & 0.069 & 0.0901 & 0.1454 & $\\dots$ \\\\\n",
       "\t8 & 0.0131 & 0.0387 & 0.0329 & 0.0078 & 0.0721 & 0.1341 & 0.1626 & 0.1902 & $\\dots$ \\\\\n",
       "\t9 & 0.0079 & 0.0086 & 0.0055 & 0.025 & 0.0344 & 0.0546 & 0.0528 & 0.0958 & $\\dots$ \\\\\n",
       "\t10 & 0.0353 & 0.0713 & 0.0326 & 0.0272 & 0.037 & 0.0792 & 0.1083 & 0.0687 & $\\dots$ \\\\\n",
       "\t11 & 0.026 & 0.0192 & 0.0254 & 0.0061 & 0.0352 & 0.0701 & 0.1263 & 0.108 & $\\dots$ \\\\\n",
       "\t12 & 0.0598141 & 0.05267 & 0.0553135 & 0.0466552 & 0.105536 & 0.075782 & 0.0851496 & 0.0463111 & $\\dots$ \\\\\n",
       "\t13 & 0.0196297 & 0.0427703 & 0.0536532 & 0.0763964 & 0.0842833 & 0.0874583 & 0.106551 & 0.234061 & $\\dots$ \\\\\n",
       "\t14 & 0.0369696 & 0.0506365 & 0.0610604 & 0.0871369 & 0.0913171 & 0.0812356 & 0.0440225 & 0.0953988 & $\\dots$ \\\\\n",
       "\t15 & 0.0116 & 0.0179 & 0.0449 & 0.1096 & 0.1913 & 0.0924 & 0.0761 & 0.1092 & $\\dots$ \\\\\n",
       "\t16 & 0.0418943 & 0.0608925 & 0.0782239 & 0.0955013 & 0.139533 & 0.137437 & 0.113059 & 0.133339 & $\\dots$ \\\\\n",
       "\t17 & 0.0353 & 0.0713 & 0.0326 & 0.0272 & 0.037 & 0.0792 & 0.1083 & 0.0687 & $\\dots$ \\\\\n",
       "\t18 & 0.111456 & 0.190747 & 0.248609 & 0.345565 & 0.332589 & 0.163657 & 0.175037 & 0.0261809 & $\\dots$ \\\\\n",
       "\t19 & 0.0856 & 0.0454 & 0.0382 & 0.0203 & 0.0385 & 0.0534 & 0.214 & 0.311 & $\\dots$ \\\\\n",
       "\t20 & 0.0235 & 0.0291 & 0.0749 & 0.0519 & 0.0227 & 0.0834 & 0.0677 & 0.2002 & $\\dots$ \\\\\n",
       "\t21 & 0.0129 & 0.0141 & 0.0309 & 0.0375 & 0.0767 & 0.0787 & 0.0662 & 0.1108 & $\\dots$ \\\\\n",
       "\t22 & 0.0291 & 0.04 & 0.0771 & 0.0809 & 0.0521 & 0.1051 & 0.0145 & 0.0674 & $\\dots$ \\\\\n",
       "\t23 & 0.0698957 & 0.130168 & 0.162643 & 0.18585 & 0.211958 & 0.172065 & 0.143629 & 0.0676793 & $\\dots$ \\\\\n",
       "\t24 & 0.0569465 & 0.117835 & 0.123189 & 0.144339 & 0.14085 & 0.0590995 & 0.0862533 & 0.0719299 & $\\dots$ \\\\\n",
       "\t25 & 0.0373 & 0.0281 & 0.0232 & 0.0225 & 0.0179 & 0.0733 & 0.0841 & 0.1031 & $\\dots$ \\\\\n",
       "\t26 & 0.0257 & 0.0447 & 0.0388 & 0.0239 & 0.1315 & 0.1323 & 0.1608 & 0.2145 & $\\dots$ \\\\\n",
       "\t27 & 0.0426858 & 0.0466743 & 0.0681659 & 0.0995467 & 0.131364 & 0.142627 & 0.145632 & 0.0978696 & $\\dots$ \\\\\n",
       "\t28 & 0.0253 & 0.0808 & 0.0507 & 0.0244 & 0.1724 & 0.3823 & 0.3729 & 0.3583 & $\\dots$ \\\\\n",
       "\t29 & 0.0193503 & 0.0381769 & 0.0347308 & 0.0458504 & 0.0498448 & 0.127742 & 0.164384 & 0.216457 & $\\dots$ \\\\\n",
       "\t30 & 0.053528 & 0.106998 & 0.114419 & 0.13415 & 0.129678 & 0.0545189 & 0.081217 & 0.0702952 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "666×61 typename(DataFrame). Omitted printing of 55 columns\n",
       "│ Row │ x1        │ x2        │ x3        │ x4        │ x5        │ x6        │\n",
       "│     │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │\n",
       "├─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┤\n",
       "│ 1   │ 0.0551908 │ 0.106544  │ 0.114441  │ 0.130378  │ 0.138722  │ 0.0773536 │\n",
       "│ 2   │ 0.0609256 │ 0.102365  │ 0.109162  │ 0.137642  │ 0.178345  │ 0.0792192 │\n",
       "│ 3   │ 0.0545872 │ 0.113696  │ 0.118691  │ 0.142543  │ 0.139727  │ 0.0606311 │\n",
       "│ 4   │ 0.0712    │ 0.0901    │ 0.1276    │ 0.1497    │ 0.1284    │ 0.1165    │\n",
       "│ 5   │ 0.0067    │ 0.0096    │ 0.0024    │ 0.0058    │ 0.0197    │ 0.0618    │\n",
       "│ 6   │ 0.0084    │ 0.0153    │ 0.0291    │ 0.0432    │ 0.0951    │ 0.0752    │\n",
       "│ 7   │ 0.019     │ 0.0038    │ 0.0642    │ 0.0452    │ 0.0333    │ 0.069     │\n",
       "│ 8   │ 0.0131    │ 0.0387    │ 0.0329    │ 0.0078    │ 0.0721    │ 0.1341    │\n",
       "│ 9   │ 0.0079    │ 0.0086    │ 0.0055    │ 0.025     │ 0.0344    │ 0.0546    │\n",
       "│ 10  │ 0.0353    │ 0.0713    │ 0.0326    │ 0.0272    │ 0.037     │ 0.0792    │\n",
       "⋮\n",
       "│ 656 │ 0.0599    │ 0.0474    │ 0.0498    │ 0.0387    │ 0.1026    │ 0.0773    │\n",
       "│ 657 │ 0.0330593 │ 0.0629647 │ 0.0599787 │ 0.0897923 │ 0.116824  │ 0.10774   │\n",
       "│ 658 │ 0.01      │ 0.0171    │ 0.0623    │ 0.0205    │ 0.0205    │ 0.0368    │\n",
       "│ 659 │ 0.0253    │ 0.0808    │ 0.0507    │ 0.0244    │ 0.1724    │ 0.3823    │\n",
       "│ 660 │ 0.0374    │ 0.0586    │ 0.0628    │ 0.0534    │ 0.0255    │ 0.1422    │\n",
       "│ 661 │ 0.0099    │ 0.0484    │ 0.0299    │ 0.0297    │ 0.0652    │ 0.1077    │\n",
       "│ 662 │ 0.0522    │ 0.0437    │ 0.018     │ 0.0292    │ 0.0351    │ 0.1171    │\n",
       "│ 663 │ 0.0139    │ 0.0222    │ 0.0089    │ 0.0108    │ 0.0215    │ 0.0136    │\n",
       "│ 664 │ 0.0220538 │ 0.0395867 │ 0.0586939 │ 0.0782818 │ 0.11098   │ 0.0744572 │\n",
       "│ 665 │ 0.0292673 │ 0.0609908 │ 0.0723571 │ 0.0952809 │ 0.0813796 │ 0.0797377 │\n",
       "│ 666 │ 0.115095  │ 0.207188  │ 0.269312  │ 0.378661  │ 0.36137   │ 0.170785  │"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we convert the data back into a DataFrame\n",
    "data1_balanced = DataFrame(X0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17ed12bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we recover the original name of the dataset\n",
    "data1 = data1_balanced;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e48c7e",
   "metadata": {},
   "source": [
    "# Splitting the data\n",
    "\n",
    "The next step is to divide the data into two sets, namely Training and Testing sets; we adjust the models to the training data and then study the performance of the model on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0da2e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use TrainTestSplit from the Lathe package\n",
    "using Lathe.preprocess: TrainTestSplit\n",
    "\n",
    "train, test = TrainTestSplit(data1,.75);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d182e085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>x1</th><th>x2</th><th>x3</th><th>x4</th><th>x5</th><th>x6</th><th>x7</th><th>x8</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>523 rows × 61 columns (omitted printing of 53 columns)</p><tr><th>1</th><td>0.0545872</td><td>0.113696</td><td>0.118691</td><td>0.142543</td><td>0.139727</td><td>0.0606311</td><td>0.0822241</td><td>0.0697095</td></tr><tr><th>2</th><td>0.0712</td><td>0.0901</td><td>0.1276</td><td>0.1497</td><td>0.1284</td><td>0.1165</td><td>0.1285</td><td>0.1684</td></tr><tr><th>3</th><td>0.0067</td><td>0.0096</td><td>0.0024</td><td>0.0058</td><td>0.0197</td><td>0.0618</td><td>0.0432</td><td>0.0951</td></tr><tr><th>4</th><td>0.0084</td><td>0.0153</td><td>0.0291</td><td>0.0432</td><td>0.0951</td><td>0.0752</td><td>0.0414</td><td>0.0259</td></tr><tr><th>5</th><td>0.0131</td><td>0.0387</td><td>0.0329</td><td>0.0078</td><td>0.0721</td><td>0.1341</td><td>0.1626</td><td>0.1902</td></tr><tr><th>6</th><td>0.0079</td><td>0.0086</td><td>0.0055</td><td>0.025</td><td>0.0344</td><td>0.0546</td><td>0.0528</td><td>0.0958</td></tr><tr><th>7</th><td>0.0598141</td><td>0.05267</td><td>0.0553135</td><td>0.0466552</td><td>0.105536</td><td>0.075782</td><td>0.0851496</td><td>0.0463111</td></tr><tr><th>8</th><td>0.0196297</td><td>0.0427703</td><td>0.0536532</td><td>0.0763964</td><td>0.0842833</td><td>0.0874583</td><td>0.106551</td><td>0.234061</td></tr><tr><th>9</th><td>0.0369696</td><td>0.0506365</td><td>0.0610604</td><td>0.0871369</td><td>0.0913171</td><td>0.0812356</td><td>0.0440225</td><td>0.0953988</td></tr><tr><th>10</th><td>0.0116</td><td>0.0179</td><td>0.0449</td><td>0.1096</td><td>0.1913</td><td>0.0924</td><td>0.0761</td><td>0.1092</td></tr><tr><th>11</th><td>0.0418943</td><td>0.0608925</td><td>0.0782239</td><td>0.0955013</td><td>0.139533</td><td>0.137437</td><td>0.113059</td><td>0.133339</td></tr><tr><th>12</th><td>0.0353</td><td>0.0713</td><td>0.0326</td><td>0.0272</td><td>0.037</td><td>0.0792</td><td>0.1083</td><td>0.0687</td></tr><tr><th>13</th><td>0.111456</td><td>0.190747</td><td>0.248609</td><td>0.345565</td><td>0.332589</td><td>0.163657</td><td>0.175037</td><td>0.0261809</td></tr><tr><th>14</th><td>0.0856</td><td>0.0454</td><td>0.0382</td><td>0.0203</td><td>0.0385</td><td>0.0534</td><td>0.214</td><td>0.311</td></tr><tr><th>15</th><td>0.0235</td><td>0.0291</td><td>0.0749</td><td>0.0519</td><td>0.0227</td><td>0.0834</td><td>0.0677</td><td>0.2002</td></tr><tr><th>16</th><td>0.0129</td><td>0.0141</td><td>0.0309</td><td>0.0375</td><td>0.0767</td><td>0.0787</td><td>0.0662</td><td>0.1108</td></tr><tr><th>17</th><td>0.0291</td><td>0.04</td><td>0.0771</td><td>0.0809</td><td>0.0521</td><td>0.1051</td><td>0.0145</td><td>0.0674</td></tr><tr><th>18</th><td>0.0698957</td><td>0.130168</td><td>0.162643</td><td>0.18585</td><td>0.211958</td><td>0.172065</td><td>0.143629</td><td>0.0676793</td></tr><tr><th>19</th><td>0.0569465</td><td>0.117835</td><td>0.123189</td><td>0.144339</td><td>0.14085</td><td>0.0590995</td><td>0.0862533</td><td>0.0719299</td></tr><tr><th>20</th><td>0.0373</td><td>0.0281</td><td>0.0232</td><td>0.0225</td><td>0.0179</td><td>0.0733</td><td>0.0841</td><td>0.1031</td></tr><tr><th>21</th><td>0.0257</td><td>0.0447</td><td>0.0388</td><td>0.0239</td><td>0.1315</td><td>0.1323</td><td>0.1608</td><td>0.2145</td></tr><tr><th>22</th><td>0.0426858</td><td>0.0466743</td><td>0.0681659</td><td>0.0995467</td><td>0.131364</td><td>0.142627</td><td>0.145632</td><td>0.0978696</td></tr><tr><th>23</th><td>0.0253</td><td>0.0808</td><td>0.0507</td><td>0.0244</td><td>0.1724</td><td>0.3823</td><td>0.3729</td><td>0.3583</td></tr><tr><th>24</th><td>0.0193503</td><td>0.0381769</td><td>0.0347308</td><td>0.0458504</td><td>0.0498448</td><td>0.127742</td><td>0.164384</td><td>0.216457</td></tr><tr><th>25</th><td>0.053528</td><td>0.106998</td><td>0.114419</td><td>0.13415</td><td>0.129678</td><td>0.0545189</td><td>0.081217</td><td>0.0702952</td></tr><tr><th>26</th><td>0.0079</td><td>0.0086</td><td>0.0055</td><td>0.025</td><td>0.0344</td><td>0.0546</td><td>0.0528</td><td>0.0958</td></tr><tr><th>27</th><td>0.0108</td><td>0.0086</td><td>0.0058</td><td>0.046</td><td>0.0752</td><td>0.0887</td><td>0.1015</td><td>0.0494</td></tr><tr><th>28</th><td>0.027</td><td>0.0092</td><td>0.0145</td><td>0.0278</td><td>0.0412</td><td>0.0757</td><td>0.1026</td><td>0.1138</td></tr><tr><th>29</th><td>0.0511048</td><td>0.070979</td><td>0.0882916</td><td>0.0929376</td><td>0.11306</td><td>0.0959068</td><td>0.0922865</td><td>0.116279</td></tr><tr><th>30</th><td>0.0454</td><td>0.0472</td><td>0.0697</td><td>0.1021</td><td>0.1397</td><td>0.1493</td><td>0.1487</td><td>0.0771</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccc}\n",
       "\t& x1 & x2 & x3 & x4 & x5 & x6 & x7 & x8 & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 0.0545872 & 0.113696 & 0.118691 & 0.142543 & 0.139727 & 0.0606311 & 0.0822241 & 0.0697095 & $\\dots$ \\\\\n",
       "\t2 & 0.0712 & 0.0901 & 0.1276 & 0.1497 & 0.1284 & 0.1165 & 0.1285 & 0.1684 & $\\dots$ \\\\\n",
       "\t3 & 0.0067 & 0.0096 & 0.0024 & 0.0058 & 0.0197 & 0.0618 & 0.0432 & 0.0951 & $\\dots$ \\\\\n",
       "\t4 & 0.0084 & 0.0153 & 0.0291 & 0.0432 & 0.0951 & 0.0752 & 0.0414 & 0.0259 & $\\dots$ \\\\\n",
       "\t5 & 0.0131 & 0.0387 & 0.0329 & 0.0078 & 0.0721 & 0.1341 & 0.1626 & 0.1902 & $\\dots$ \\\\\n",
       "\t6 & 0.0079 & 0.0086 & 0.0055 & 0.025 & 0.0344 & 0.0546 & 0.0528 & 0.0958 & $\\dots$ \\\\\n",
       "\t7 & 0.0598141 & 0.05267 & 0.0553135 & 0.0466552 & 0.105536 & 0.075782 & 0.0851496 & 0.0463111 & $\\dots$ \\\\\n",
       "\t8 & 0.0196297 & 0.0427703 & 0.0536532 & 0.0763964 & 0.0842833 & 0.0874583 & 0.106551 & 0.234061 & $\\dots$ \\\\\n",
       "\t9 & 0.0369696 & 0.0506365 & 0.0610604 & 0.0871369 & 0.0913171 & 0.0812356 & 0.0440225 & 0.0953988 & $\\dots$ \\\\\n",
       "\t10 & 0.0116 & 0.0179 & 0.0449 & 0.1096 & 0.1913 & 0.0924 & 0.0761 & 0.1092 & $\\dots$ \\\\\n",
       "\t11 & 0.0418943 & 0.0608925 & 0.0782239 & 0.0955013 & 0.139533 & 0.137437 & 0.113059 & 0.133339 & $\\dots$ \\\\\n",
       "\t12 & 0.0353 & 0.0713 & 0.0326 & 0.0272 & 0.037 & 0.0792 & 0.1083 & 0.0687 & $\\dots$ \\\\\n",
       "\t13 & 0.111456 & 0.190747 & 0.248609 & 0.345565 & 0.332589 & 0.163657 & 0.175037 & 0.0261809 & $\\dots$ \\\\\n",
       "\t14 & 0.0856 & 0.0454 & 0.0382 & 0.0203 & 0.0385 & 0.0534 & 0.214 & 0.311 & $\\dots$ \\\\\n",
       "\t15 & 0.0235 & 0.0291 & 0.0749 & 0.0519 & 0.0227 & 0.0834 & 0.0677 & 0.2002 & $\\dots$ \\\\\n",
       "\t16 & 0.0129 & 0.0141 & 0.0309 & 0.0375 & 0.0767 & 0.0787 & 0.0662 & 0.1108 & $\\dots$ \\\\\n",
       "\t17 & 0.0291 & 0.04 & 0.0771 & 0.0809 & 0.0521 & 0.1051 & 0.0145 & 0.0674 & $\\dots$ \\\\\n",
       "\t18 & 0.0698957 & 0.130168 & 0.162643 & 0.18585 & 0.211958 & 0.172065 & 0.143629 & 0.0676793 & $\\dots$ \\\\\n",
       "\t19 & 0.0569465 & 0.117835 & 0.123189 & 0.144339 & 0.14085 & 0.0590995 & 0.0862533 & 0.0719299 & $\\dots$ \\\\\n",
       "\t20 & 0.0373 & 0.0281 & 0.0232 & 0.0225 & 0.0179 & 0.0733 & 0.0841 & 0.1031 & $\\dots$ \\\\\n",
       "\t21 & 0.0257 & 0.0447 & 0.0388 & 0.0239 & 0.1315 & 0.1323 & 0.1608 & 0.2145 & $\\dots$ \\\\\n",
       "\t22 & 0.0426858 & 0.0466743 & 0.0681659 & 0.0995467 & 0.131364 & 0.142627 & 0.145632 & 0.0978696 & $\\dots$ \\\\\n",
       "\t23 & 0.0253 & 0.0808 & 0.0507 & 0.0244 & 0.1724 & 0.3823 & 0.3729 & 0.3583 & $\\dots$ \\\\\n",
       "\t24 & 0.0193503 & 0.0381769 & 0.0347308 & 0.0458504 & 0.0498448 & 0.127742 & 0.164384 & 0.216457 & $\\dots$ \\\\\n",
       "\t25 & 0.053528 & 0.106998 & 0.114419 & 0.13415 & 0.129678 & 0.0545189 & 0.081217 & 0.0702952 & $\\dots$ \\\\\n",
       "\t26 & 0.0079 & 0.0086 & 0.0055 & 0.025 & 0.0344 & 0.0546 & 0.0528 & 0.0958 & $\\dots$ \\\\\n",
       "\t27 & 0.0108 & 0.0086 & 0.0058 & 0.046 & 0.0752 & 0.0887 & 0.1015 & 0.0494 & $\\dots$ \\\\\n",
       "\t28 & 0.027 & 0.0092 & 0.0145 & 0.0278 & 0.0412 & 0.0757 & 0.1026 & 0.1138 & $\\dots$ \\\\\n",
       "\t29 & 0.0511048 & 0.070979 & 0.0882916 & 0.0929376 & 0.11306 & 0.0959068 & 0.0922865 & 0.116279 & $\\dots$ \\\\\n",
       "\t30 & 0.0454 & 0.0472 & 0.0697 & 0.1021 & 0.1397 & 0.1493 & 0.1487 & 0.0771 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "523×61 typename(DataFrame). Omitted printing of 55 columns\n",
       "│ Row │ x1        │ x2        │ x3        │ x4        │ x5        │ x6        │\n",
       "│     │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │\n",
       "├─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┤\n",
       "│ 1   │ 0.0545872 │ 0.113696  │ 0.118691  │ 0.142543  │ 0.139727  │ 0.0606311 │\n",
       "│ 2   │ 0.0712    │ 0.0901    │ 0.1276    │ 0.1497    │ 0.1284    │ 0.1165    │\n",
       "│ 3   │ 0.0067    │ 0.0096    │ 0.0024    │ 0.0058    │ 0.0197    │ 0.0618    │\n",
       "│ 4   │ 0.0084    │ 0.0153    │ 0.0291    │ 0.0432    │ 0.0951    │ 0.0752    │\n",
       "│ 5   │ 0.0131    │ 0.0387    │ 0.0329    │ 0.0078    │ 0.0721    │ 0.1341    │\n",
       "│ 6   │ 0.0079    │ 0.0086    │ 0.0055    │ 0.025     │ 0.0344    │ 0.0546    │\n",
       "│ 7   │ 0.0598141 │ 0.05267   │ 0.0553135 │ 0.0466552 │ 0.105536  │ 0.075782  │\n",
       "│ 8   │ 0.0196297 │ 0.0427703 │ 0.0536532 │ 0.0763964 │ 0.0842833 │ 0.0874583 │\n",
       "│ 9   │ 0.0369696 │ 0.0506365 │ 0.0610604 │ 0.0871369 │ 0.0913171 │ 0.0812356 │\n",
       "│ 10  │ 0.0116    │ 0.0179    │ 0.0449    │ 0.1096    │ 0.1913    │ 0.0924    │\n",
       "⋮\n",
       "│ 513 │ 0.0394    │ 0.042     │ 0.0446    │ 0.0551    │ 0.0597    │ 0.1416    │\n",
       "│ 514 │ 0.0036    │ 0.0078    │ 0.0092    │ 0.0387    │ 0.053     │ 0.1197    │\n",
       "│ 515 │ 0.013     │ 0.0006    │ 0.0088    │ 0.0456    │ 0.0525    │ 0.0778    │\n",
       "│ 516 │ 0.0330593 │ 0.0629647 │ 0.0599787 │ 0.0897923 │ 0.116824  │ 0.10774   │\n",
       "│ 517 │ 0.0253    │ 0.0808    │ 0.0507    │ 0.0244    │ 0.1724    │ 0.3823    │\n",
       "│ 518 │ 0.0374    │ 0.0586    │ 0.0628    │ 0.0534    │ 0.0255    │ 0.1422    │\n",
       "│ 519 │ 0.0099    │ 0.0484    │ 0.0299    │ 0.0297    │ 0.0652    │ 0.1077    │\n",
       "│ 520 │ 0.0522    │ 0.0437    │ 0.018     │ 0.0292    │ 0.0351    │ 0.1171    │\n",
       "│ 521 │ 0.0139    │ 0.0222    │ 0.0089    │ 0.0108    │ 0.0215    │ 0.0136    │\n",
       "│ 522 │ 0.0220538 │ 0.0395867 │ 0.0586939 │ 0.0782818 │ 0.11098   │ 0.0744572 │\n",
       "│ 523 │ 0.115095  │ 0.207188  │ 0.269312  │ 0.378661  │ 0.36137   │ 0.170785  │"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c53f65d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>x1</th><th>x2</th><th>x3</th><th>x4</th><th>x5</th><th>x6</th><th>x7</th><th>x8</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>143 rows × 61 columns (omitted printing of 53 columns)</p><tr><th>1</th><td>0.0551908</td><td>0.106544</td><td>0.114441</td><td>0.130378</td><td>0.138722</td><td>0.0773536</td><td>0.0988919</td><td>0.0609983</td></tr><tr><th>2</th><td>0.0609256</td><td>0.102365</td><td>0.109162</td><td>0.137642</td><td>0.178345</td><td>0.0792192</td><td>0.115255</td><td>0.0777299</td></tr><tr><th>3</th><td>0.019</td><td>0.0038</td><td>0.0642</td><td>0.0452</td><td>0.0333</td><td>0.069</td><td>0.0901</td><td>0.1454</td></tr><tr><th>4</th><td>0.0353</td><td>0.0713</td><td>0.0326</td><td>0.0272</td><td>0.037</td><td>0.0792</td><td>0.1083</td><td>0.0687</td></tr><tr><th>5</th><td>0.026</td><td>0.0192</td><td>0.0254</td><td>0.0061</td><td>0.0352</td><td>0.0701</td><td>0.1263</td><td>0.108</td></tr><tr><th>6</th><td>0.0201</td><td>0.0116</td><td>0.0123</td><td>0.0245</td><td>0.0547</td><td>0.0208</td><td>0.0891</td><td>0.0836</td></tr><tr><th>7</th><td>0.0251473</td><td>0.0464832</td><td>0.0557241</td><td>0.0727519</td><td>0.0643764</td><td>0.0962323</td><td>0.127121</td><td>0.262736</td></tr><tr><th>8</th><td>0.0139</td><td>0.0222</td><td>0.0089</td><td>0.0108</td><td>0.0215</td><td>0.0136</td><td>0.0659</td><td>0.0954</td></tr><tr><th>9</th><td>0.0206</td><td>0.0132</td><td>0.0533</td><td>0.0569</td><td>0.0647</td><td>0.1432</td><td>0.1344</td><td>0.2041</td></tr><tr><th>10</th><td>0.0025</td><td>0.0309</td><td>0.0171</td><td>0.0228</td><td>0.0434</td><td>0.1224</td><td>0.1947</td><td>0.1661</td></tr><tr><th>11</th><td>0.0189</td><td>0.0308</td><td>0.0197</td><td>0.0622</td><td>0.008</td><td>0.0789</td><td>0.144</td><td>0.1451</td></tr><tr><th>12</th><td>0.0353</td><td>0.0713</td><td>0.0326</td><td>0.0272</td><td>0.037</td><td>0.0792</td><td>0.1083</td><td>0.0687</td></tr><tr><th>13</th><td>0.0216</td><td>0.0124</td><td>0.0174</td><td>0.0152</td><td>0.0608</td><td>0.1026</td><td>0.1139</td><td>0.0877</td></tr><tr><th>14</th><td>0.0195</td><td>0.0142</td><td>0.0181</td><td>0.0406</td><td>0.0391</td><td>0.0249</td><td>0.0892</td><td>0.0973</td></tr><tr><th>15</th><td>0.0114</td><td>0.0222</td><td>0.0269</td><td>0.0384</td><td>0.1217</td><td>0.2062</td><td>0.1489</td><td>0.0929</td></tr><tr><th>16</th><td>0.0039</td><td>0.0063</td><td>0.0152</td><td>0.0336</td><td>0.031</td><td>0.0284</td><td>0.0396</td><td>0.0272</td></tr><tr><th>17</th><td>0.0778877</td><td>0.1042</td><td>0.0962501</td><td>0.111292</td><td>0.108187</td><td>0.09001</td><td>0.0900969</td><td>0.116495</td></tr><tr><th>18</th><td>0.0221</td><td>0.0065</td><td>0.0164</td><td>0.0487</td><td>0.0519</td><td>0.0849</td><td>0.0812</td><td>0.1833</td></tr><tr><th>19</th><td>0.0188</td><td>0.037</td><td>0.0953</td><td>0.0824</td><td>0.0249</td><td>0.0488</td><td>0.1424</td><td>0.1972</td></tr><tr><th>20</th><td>0.0136368</td><td>0.0176273</td><td>0.018445</td><td>0.0367143</td><td>0.0448751</td><td>0.0746134</td><td>0.0800402</td><td>0.0458158</td></tr><tr><th>21</th><td>0.0303</td><td>0.0353</td><td>0.049</td><td>0.0608</td><td>0.0167</td><td>0.1354</td><td>0.1465</td><td>0.1123</td></tr><tr><th>22</th><td>0.0501513</td><td>0.0727699</td><td>0.108919</td><td>0.0814168</td><td>0.124797</td><td>0.144457</td><td>0.0972721</td><td>0.122472</td></tr><tr><th>23</th><td>0.0195</td><td>0.0213</td><td>0.0058</td><td>0.019</td><td>0.0319</td><td>0.0571</td><td>0.1004</td><td>0.0668</td></tr><tr><th>24</th><td>0.0125</td><td>0.0152</td><td>0.0218</td><td>0.0175</td><td>0.0362</td><td>0.0696</td><td>0.0873</td><td>0.0616</td></tr><tr><th>25</th><td>0.0162</td><td>0.0041</td><td>0.0239</td><td>0.0441</td><td>0.063</td><td>0.0921</td><td>0.1368</td><td>0.1078</td></tr><tr><th>26</th><td>0.0336</td><td>0.0294</td><td>0.0476</td><td>0.0539</td><td>0.0794</td><td>0.0804</td><td>0.1136</td><td>0.1228</td></tr><tr><th>27</th><td>0.0115</td><td>0.015</td><td>0.0136</td><td>0.0076</td><td>0.0211</td><td>0.1058</td><td>0.1023</td><td>0.044</td></tr><tr><th>28</th><td>0.0526</td><td>0.0563</td><td>0.1219</td><td>0.1206</td><td>0.0246</td><td>0.1022</td><td>0.0539</td><td>0.0439</td></tr><tr><th>29</th><td>0.0208</td><td>0.0186</td><td>0.0131</td><td>0.0211</td><td>0.061</td><td>0.0613</td><td>0.0612</td><td>0.0506</td></tr><tr><th>30</th><td>0.0039</td><td>0.0063</td><td>0.0152</td><td>0.0336</td><td>0.031</td><td>0.0284</td><td>0.0396</td><td>0.0272</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccc}\n",
       "\t& x1 & x2 & x3 & x4 & x5 & x6 & x7 & x8 & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 0.0551908 & 0.106544 & 0.114441 & 0.130378 & 0.138722 & 0.0773536 & 0.0988919 & 0.0609983 & $\\dots$ \\\\\n",
       "\t2 & 0.0609256 & 0.102365 & 0.109162 & 0.137642 & 0.178345 & 0.0792192 & 0.115255 & 0.0777299 & $\\dots$ \\\\\n",
       "\t3 & 0.019 & 0.0038 & 0.0642 & 0.0452 & 0.0333 & 0.069 & 0.0901 & 0.1454 & $\\dots$ \\\\\n",
       "\t4 & 0.0353 & 0.0713 & 0.0326 & 0.0272 & 0.037 & 0.0792 & 0.1083 & 0.0687 & $\\dots$ \\\\\n",
       "\t5 & 0.026 & 0.0192 & 0.0254 & 0.0061 & 0.0352 & 0.0701 & 0.1263 & 0.108 & $\\dots$ \\\\\n",
       "\t6 & 0.0201 & 0.0116 & 0.0123 & 0.0245 & 0.0547 & 0.0208 & 0.0891 & 0.0836 & $\\dots$ \\\\\n",
       "\t7 & 0.0251473 & 0.0464832 & 0.0557241 & 0.0727519 & 0.0643764 & 0.0962323 & 0.127121 & 0.262736 & $\\dots$ \\\\\n",
       "\t8 & 0.0139 & 0.0222 & 0.0089 & 0.0108 & 0.0215 & 0.0136 & 0.0659 & 0.0954 & $\\dots$ \\\\\n",
       "\t9 & 0.0206 & 0.0132 & 0.0533 & 0.0569 & 0.0647 & 0.1432 & 0.1344 & 0.2041 & $\\dots$ \\\\\n",
       "\t10 & 0.0025 & 0.0309 & 0.0171 & 0.0228 & 0.0434 & 0.1224 & 0.1947 & 0.1661 & $\\dots$ \\\\\n",
       "\t11 & 0.0189 & 0.0308 & 0.0197 & 0.0622 & 0.008 & 0.0789 & 0.144 & 0.1451 & $\\dots$ \\\\\n",
       "\t12 & 0.0353 & 0.0713 & 0.0326 & 0.0272 & 0.037 & 0.0792 & 0.1083 & 0.0687 & $\\dots$ \\\\\n",
       "\t13 & 0.0216 & 0.0124 & 0.0174 & 0.0152 & 0.0608 & 0.1026 & 0.1139 & 0.0877 & $\\dots$ \\\\\n",
       "\t14 & 0.0195 & 0.0142 & 0.0181 & 0.0406 & 0.0391 & 0.0249 & 0.0892 & 0.0973 & $\\dots$ \\\\\n",
       "\t15 & 0.0114 & 0.0222 & 0.0269 & 0.0384 & 0.1217 & 0.2062 & 0.1489 & 0.0929 & $\\dots$ \\\\\n",
       "\t16 & 0.0039 & 0.0063 & 0.0152 & 0.0336 & 0.031 & 0.0284 & 0.0396 & 0.0272 & $\\dots$ \\\\\n",
       "\t17 & 0.0778877 & 0.1042 & 0.0962501 & 0.111292 & 0.108187 & 0.09001 & 0.0900969 & 0.116495 & $\\dots$ \\\\\n",
       "\t18 & 0.0221 & 0.0065 & 0.0164 & 0.0487 & 0.0519 & 0.0849 & 0.0812 & 0.1833 & $\\dots$ \\\\\n",
       "\t19 & 0.0188 & 0.037 & 0.0953 & 0.0824 & 0.0249 & 0.0488 & 0.1424 & 0.1972 & $\\dots$ \\\\\n",
       "\t20 & 0.0136368 & 0.0176273 & 0.018445 & 0.0367143 & 0.0448751 & 0.0746134 & 0.0800402 & 0.0458158 & $\\dots$ \\\\\n",
       "\t21 & 0.0303 & 0.0353 & 0.049 & 0.0608 & 0.0167 & 0.1354 & 0.1465 & 0.1123 & $\\dots$ \\\\\n",
       "\t22 & 0.0501513 & 0.0727699 & 0.108919 & 0.0814168 & 0.124797 & 0.144457 & 0.0972721 & 0.122472 & $\\dots$ \\\\\n",
       "\t23 & 0.0195 & 0.0213 & 0.0058 & 0.019 & 0.0319 & 0.0571 & 0.1004 & 0.0668 & $\\dots$ \\\\\n",
       "\t24 & 0.0125 & 0.0152 & 0.0218 & 0.0175 & 0.0362 & 0.0696 & 0.0873 & 0.0616 & $\\dots$ \\\\\n",
       "\t25 & 0.0162 & 0.0041 & 0.0239 & 0.0441 & 0.063 & 0.0921 & 0.1368 & 0.1078 & $\\dots$ \\\\\n",
       "\t26 & 0.0336 & 0.0294 & 0.0476 & 0.0539 & 0.0794 & 0.0804 & 0.1136 & 0.1228 & $\\dots$ \\\\\n",
       "\t27 & 0.0115 & 0.015 & 0.0136 & 0.0076 & 0.0211 & 0.1058 & 0.1023 & 0.044 & $\\dots$ \\\\\n",
       "\t28 & 0.0526 & 0.0563 & 0.1219 & 0.1206 & 0.0246 & 0.1022 & 0.0539 & 0.0439 & $\\dots$ \\\\\n",
       "\t29 & 0.0208 & 0.0186 & 0.0131 & 0.0211 & 0.061 & 0.0613 & 0.0612 & 0.0506 & $\\dots$ \\\\\n",
       "\t30 & 0.0039 & 0.0063 & 0.0152 & 0.0336 & 0.031 & 0.0284 & 0.0396 & 0.0272 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "143×61 typename(DataFrame). Omitted printing of 55 columns\n",
       "│ Row │ x1        │ x2        │ x3        │ x4        │ x5        │ x6        │\n",
       "│     │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │\n",
       "├─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┤\n",
       "│ 1   │ 0.0551908 │ 0.106544  │ 0.114441  │ 0.130378  │ 0.138722  │ 0.0773536 │\n",
       "│ 2   │ 0.0609256 │ 0.102365  │ 0.109162  │ 0.137642  │ 0.178345  │ 0.0792192 │\n",
       "│ 3   │ 0.019     │ 0.0038    │ 0.0642    │ 0.0452    │ 0.0333    │ 0.069     │\n",
       "│ 4   │ 0.0353    │ 0.0713    │ 0.0326    │ 0.0272    │ 0.037     │ 0.0792    │\n",
       "│ 5   │ 0.026     │ 0.0192    │ 0.0254    │ 0.0061    │ 0.0352    │ 0.0701    │\n",
       "│ 6   │ 0.0201    │ 0.0116    │ 0.0123    │ 0.0245    │ 0.0547    │ 0.0208    │\n",
       "│ 7   │ 0.0251473 │ 0.0464832 │ 0.0557241 │ 0.0727519 │ 0.0643764 │ 0.0962323 │\n",
       "│ 8   │ 0.0139    │ 0.0222    │ 0.0089    │ 0.0108    │ 0.0215    │ 0.0136    │\n",
       "│ 9   │ 0.0206    │ 0.0132    │ 0.0533    │ 0.0569    │ 0.0647    │ 0.1432    │\n",
       "│ 10  │ 0.0025    │ 0.0309    │ 0.0171    │ 0.0228    │ 0.0434    │ 0.1224    │\n",
       "⋮\n",
       "│ 133 │ 0.0086    │ 0.0215    │ 0.0242    │ 0.0445    │ 0.0667    │ 0.0771    │\n",
       "│ 134 │ 0.0093    │ 0.0185    │ 0.0056    │ 0.0064    │ 0.026     │ 0.0458    │\n",
       "│ 135 │ 0.0065    │ 0.0122    │ 0.0068    │ 0.0108    │ 0.0217    │ 0.0284    │\n",
       "│ 136 │ 0.0189    │ 0.0308    │ 0.0197    │ 0.0622    │ 0.008     │ 0.0789    │\n",
       "│ 137 │ 0.053977  │ 0.0867265 │ 0.127044  │ 0.103776  │ 0.12306   │ 0.104619  │\n",
       "│ 138 │ 0.0151422 │ 0.0280683 │ 0.0492757 │ 0.0965563 │ 0.137416  │ 0.0901913 │\n",
       "│ 139 │ 0.0093    │ 0.0185    │ 0.0056    │ 0.0064    │ 0.026     │ 0.0458    │\n",
       "│ 140 │ 0.033133  │ 0.0123907 │ 0.0574278 │ 0.0991607 │ 0.0474003 │ 0.11121   │\n",
       "│ 141 │ 0.0599    │ 0.0474    │ 0.0498    │ 0.0387    │ 0.1026    │ 0.0773    │\n",
       "│ 142 │ 0.01      │ 0.0171    │ 0.0623    │ 0.0205    │ 0.0205    │ 0.0368    │\n",
       "│ 143 │ 0.0292673 │ 0.0609908 │ 0.0723571 │ 0.0952809 │ 0.0813796 │ 0.0797377 │"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d29c87",
   "metadata": {},
   "source": [
    "We see that the function gave us a training set of size $523$, while the testing set is of size $143$.<br>\n",
    "\n",
    "We are now ready to start training with some models in order to make predictions. First we approach the regression models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0574177a",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "The first classification model we are going to use is Linear Regression. The model basically assigns to each observation a probability of being of one or another type.<br>\n",
    "\n",
    "For this model we make use of the package GLM (Generalized Linear Models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f47473a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>y_actual</th><th>y_predicted</th><th>prob_predicted</th></tr><tr><th></th><th>Float64</th><th>Int64</th><th>Float64⍰</th></tr></thead><tbody><p>143 rows × 3 columns</p><tr><th>1</th><td>1.0</td><td>1</td><td>1.0</td></tr><tr><th>2</th><td>1.0</td><td>1</td><td>1.0</td></tr><tr><th>3</th><td>0.0</td><td>0</td><td>9.62143e-92</td></tr><tr><th>4</th><td>0.0</td><td>0</td><td>5.83262e-10</td></tr><tr><th>5</th><td>0.0</td><td>0</td><td>0.0</td></tr><tr><th>6</th><td>0.0</td><td>0</td><td>0.0</td></tr><tr><th>7</th><td>1.0</td><td>1</td><td>1.0</td></tr><tr><th>8</th><td>0.0</td><td>0</td><td>2.11919e-9</td></tr><tr><th>9</th><td>0.0</td><td>0</td><td>2.13968e-11</td></tr><tr><th>10</th><td>0.0</td><td>0</td><td>1.36418e-9</td></tr><tr><th>11</th><td>0.0</td><td>0</td><td>0.0</td></tr><tr><th>12</th><td>0.0</td><td>0</td><td>5.83262e-10</td></tr><tr><th>13</th><td>0.0</td><td>0</td><td>0.0</td></tr><tr><th>14</th><td>0.0</td><td>0</td><td>0.0</td></tr><tr><th>15</th><td>1.0</td><td>1</td><td>1.0</td></tr><tr><th>16</th><td>0.0</td><td>0</td><td>4.4894e-155</td></tr><tr><th>17</th><td>1.0</td><td>1</td><td>1.0</td></tr><tr><th>18</th><td>1.0</td><td>1</td><td>1.0</td></tr><tr><th>19</th><td>0.0</td><td>0</td><td>0.0</td></tr><tr><th>20</th><td>1.0</td><td>1</td><td>1.0</td></tr><tr><th>21</th><td>1.0</td><td>1</td><td>1.0</td></tr><tr><th>22</th><td>1.0</td><td>1</td><td>1.0</td></tr><tr><th>23</th><td>0.0</td><td>0</td><td>0.0</td></tr><tr><th>24</th><td>0.0</td><td>0</td><td>8.55611e-251</td></tr><tr><th>25</th><td>1.0</td><td>1</td><td>1.0</td></tr><tr><th>26</th><td>0.0</td><td>0</td><td>0.0</td></tr><tr><th>27</th><td>0.0</td><td>0</td><td>1.00401e-311</td></tr><tr><th>28</th><td>1.0</td><td>1</td><td>1.0</td></tr><tr><th>29</th><td>0.0</td><td>0</td><td>0.0</td></tr><tr><th>30</th><td>0.0</td><td>0</td><td>4.4894e-155</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& y\\_actual & y\\_predicted & prob\\_predicted\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Int64 & Float64⍰\\\\\n",
       "\t\\hline\n",
       "\t1 & 1.0 & 1 & 1.0 \\\\\n",
       "\t2 & 1.0 & 1 & 1.0 \\\\\n",
       "\t3 & 0.0 & 0 & 9.62143e-92 \\\\\n",
       "\t4 & 0.0 & 0 & 5.83262e-10 \\\\\n",
       "\t5 & 0.0 & 0 & 0.0 \\\\\n",
       "\t6 & 0.0 & 0 & 0.0 \\\\\n",
       "\t7 & 1.0 & 1 & 1.0 \\\\\n",
       "\t8 & 0.0 & 0 & 2.11919e-9 \\\\\n",
       "\t9 & 0.0 & 0 & 2.13968e-11 \\\\\n",
       "\t10 & 0.0 & 0 & 1.36418e-9 \\\\\n",
       "\t11 & 0.0 & 0 & 0.0 \\\\\n",
       "\t12 & 0.0 & 0 & 5.83262e-10 \\\\\n",
       "\t13 & 0.0 & 0 & 0.0 \\\\\n",
       "\t14 & 0.0 & 0 & 0.0 \\\\\n",
       "\t15 & 1.0 & 1 & 1.0 \\\\\n",
       "\t16 & 0.0 & 0 & 4.4894e-155 \\\\\n",
       "\t17 & 1.0 & 1 & 1.0 \\\\\n",
       "\t18 & 1.0 & 1 & 1.0 \\\\\n",
       "\t19 & 0.0 & 0 & 0.0 \\\\\n",
       "\t20 & 1.0 & 1 & 1.0 \\\\\n",
       "\t21 & 1.0 & 1 & 1.0 \\\\\n",
       "\t22 & 1.0 & 1 & 1.0 \\\\\n",
       "\t23 & 0.0 & 0 & 0.0 \\\\\n",
       "\t24 & 0.0 & 0 & 8.55611e-251 \\\\\n",
       "\t25 & 1.0 & 1 & 1.0 \\\\\n",
       "\t26 & 0.0 & 0 & 0.0 \\\\\n",
       "\t27 & 0.0 & 0 & 1.00401e-311 \\\\\n",
       "\t28 & 1.0 & 1 & 1.0 \\\\\n",
       "\t29 & 0.0 & 0 & 0.0 \\\\\n",
       "\t30 & 0.0 & 0 & 4.4894e-155 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "143×3 typename(DataFrame)\n",
       "│ Row │ y_actual │ y_predicted │ prob_predicted │\n",
       "│     │ \u001b[90mFloat64\u001b[39m  │ \u001b[90mInt64\u001b[39m       │ \u001b[90mFloat64⍰\u001b[39m       │\n",
       "├─────┼──────────┼─────────────┼────────────────┤\n",
       "│ 1   │ 1.0      │ 1           │ 1.0            │\n",
       "│ 2   │ 1.0      │ 1           │ 1.0            │\n",
       "│ 3   │ 0.0      │ 0           │ 9.62143e-92    │\n",
       "│ 4   │ 0.0      │ 0           │ 5.83262e-10    │\n",
       "│ 5   │ 0.0      │ 0           │ 0.0            │\n",
       "│ 6   │ 0.0      │ 0           │ 0.0            │\n",
       "│ 7   │ 1.0      │ 1           │ 1.0            │\n",
       "│ 8   │ 0.0      │ 0           │ 2.11919e-9     │\n",
       "│ 9   │ 0.0      │ 0           │ 2.13968e-11    │\n",
       "│ 10  │ 0.0      │ 0           │ 1.36418e-9     │\n",
       "⋮\n",
       "│ 133 │ 0.0      │ 0           │ 9.81528e-18    │\n",
       "│ 134 │ 0.0      │ 0           │ 0.0            │\n",
       "│ 135 │ 0.0      │ 0           │ 0.0            │\n",
       "│ 136 │ 0.0      │ 0           │ 0.0            │\n",
       "│ 137 │ 1.0      │ 1           │ 1.0            │\n",
       "│ 138 │ 1.0      │ 1           │ 1.0            │\n",
       "│ 139 │ 0.0      │ 0           │ 0.0            │\n",
       "│ 140 │ 1.0      │ 1           │ 1.0            │\n",
       "│ 141 │ 1.0      │ 1           │ 1.0            │\n",
       "│ 142 │ 0.0      │ 0           │ 4.16106e-10    │\n",
       "│ 143 │ 1.0      │ 1           │ 1.0            │"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We build the model on the train data\n",
    "fm = @formula(x61 ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x15 + x16 + x17 + x18 + x19 + x20 + x21 + x22 + x23 + x24 + x25 + x26 + x27 + x28 + x29 + x30 + x31 + x32 + x33 + x34 + x35 + x36 + x37 + x38 + x39 + x40 + x41 + x42 + x43 + x44 + x45 + x46 + x47 + x48 + x49 + x50 + x51 + x52 + x53 + x54 + x55 + x56 + x57 + x58 + x59 + x60)\n",
    "logit = glm(fm, train, Binomial(), ProbitLink())\n",
    "\n",
    "# We make the prediction on the test data\n",
    "prediction = GLM.predict(logit,test)\n",
    "\n",
    "# Now we convert the probability that the model assigned to one of the label using 0.5 as the threshold\n",
    "prediction_class = [if x < 0.5 0 else 1 end for x in prediction]\n",
    "\n",
    "#here we can contrast the actual data (1st col) with the prediction made by the model(2nd col)\n",
    "prediction_df = DataFrame(y_actual = test.x61, y_predicted = prediction_class, prob_predicted = prediction) #model predicted correctly and 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5591d8b",
   "metadata": {},
   "source": [
    "# Performance measure: Accuracy\n",
    "\n",
    "But what does the above information tells us about how good the model worked? In order to measure the performance of the model, one of the most simple ways to measure the performance is the acuracy: it basically tells you how many labels of the test data were correctly predicted by the model, out of all the test data. This measure should be treated carefully because sometimes it can give a misleading value when the data is not proprely balanced (see https://www.machinelearningplus.com/julia/logistic-regression-in-julia-practical-guide-with-examples/), but for now we just want to have an idea on how the models compare to others on the same training and testing data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bec4e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to measure accuracy, we take the mean of the following list\n",
    "# this counts 1 if the model predicted correctly and 0 otherwise\n",
    "prediction_df.correctly_classified = prediction_df.y_actual .== prediction_df.y_predicted;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "96dd70b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is : 97.9020979020979 %"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "accuracy = mean(prediction_df.correctly_classified)\n",
    "print(\"Accuracy of the model is : \",accuracy*100, \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc6d7d",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Next up we have linear regression. This is not usually used as a classification model, basically because it doesn't do a very good job predicting labels for unseen data (see: https://stats.stackexchange.com/questions/22381/why-not-approach-classification-through-regression), something we will see next up on the performance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1a51b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we set up the model\n",
    "N = 523 #size of train data\n",
    "A = Matrix(train[:, 1:60])\n",
    "A = [ones(N,1) A];\n",
    "b = Array(train.x61);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6b1f98af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we solve the linear system\n",
    "xhat=A\\b;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dde4d067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>x1</th></tr><tr><th></th><th>Float64</th></tr></thead><tbody><p>143 rows × 1 columns</p><tr><th>1</th><td>0.516064</td></tr><tr><th>2</th><td>0.262142</td></tr><tr><th>3</th><td>0.0712967</td></tr><tr><th>4</th><td>-0.220874</td></tr><tr><th>5</th><td>-0.105759</td></tr><tr><th>6</th><td>-0.186986</td></tr><tr><th>7</th><td>0.314711</td></tr><tr><th>8</th><td>-0.0768633</td></tr><tr><th>9</th><td>-0.114551</td></tr><tr><th>10</th><td>-0.0711872</td></tr><tr><th>11</th><td>0.0933555</td></tr><tr><th>12</th><td>-0.220874</td></tr><tr><th>13</th><td>-0.230961</td></tr><tr><th>14</th><td>0.0438209</td></tr><tr><th>15</th><td>0.129675</td></tr><tr><th>16</th><td>0.000876212</td></tr><tr><th>17</th><td>0.367408</td></tr><tr><th>18</th><td>0.185395</td></tr><tr><th>19</th><td>-0.170959</td></tr><tr><th>20</th><td>0.0236162</td></tr><tr><th>21</th><td>0.11165</td></tr><tr><th>22</th><td>0.293655</td></tr><tr><th>23</th><td>-0.0378497</td></tr><tr><th>24</th><td>-0.261394</td></tr><tr><th>25</th><td>0.0538021</td></tr><tr><th>26</th><td>-0.246385</td></tr><tr><th>27</th><td>-0.113584</td></tr><tr><th>28</th><td>0.320355</td></tr><tr><th>29</th><td>-0.183855</td></tr><tr><th>30</th><td>0.000876212</td></tr><tr><th>&vellip;</th><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|c}\n",
       "\t& x1\\\\\n",
       "\t\\hline\n",
       "\t& Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 0.516064 \\\\\n",
       "\t2 & 0.262142 \\\\\n",
       "\t3 & 0.0712967 \\\\\n",
       "\t4 & -0.220874 \\\\\n",
       "\t5 & -0.105759 \\\\\n",
       "\t6 & -0.186986 \\\\\n",
       "\t7 & 0.314711 \\\\\n",
       "\t8 & -0.0768633 \\\\\n",
       "\t9 & -0.114551 \\\\\n",
       "\t10 & -0.0711872 \\\\\n",
       "\t11 & 0.0933555 \\\\\n",
       "\t12 & -0.220874 \\\\\n",
       "\t13 & -0.230961 \\\\\n",
       "\t14 & 0.0438209 \\\\\n",
       "\t15 & 0.129675 \\\\\n",
       "\t16 & 0.000876212 \\\\\n",
       "\t17 & 0.367408 \\\\\n",
       "\t18 & 0.185395 \\\\\n",
       "\t19 & -0.170959 \\\\\n",
       "\t20 & 0.0236162 \\\\\n",
       "\t21 & 0.11165 \\\\\n",
       "\t22 & 0.293655 \\\\\n",
       "\t23 & -0.0378497 \\\\\n",
       "\t24 & -0.261394 \\\\\n",
       "\t25 & 0.0538021 \\\\\n",
       "\t26 & -0.246385 \\\\\n",
       "\t27 & -0.113584 \\\\\n",
       "\t28 & 0.320355 \\\\\n",
       "\t29 & -0.183855 \\\\\n",
       "\t30 & 0.000876212 \\\\\n",
       "\t$\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "143×1 typename(DataFrame)\n",
       "│ Row │ x1         │\n",
       "│     │ \u001b[90mFloat64\u001b[39m    │\n",
       "├─────┼────────────┤\n",
       "│ 1   │ 0.516064   │\n",
       "│ 2   │ 0.262142   │\n",
       "│ 3   │ 0.0712967  │\n",
       "│ 4   │ -0.220874  │\n",
       "│ 5   │ -0.105759  │\n",
       "│ 6   │ -0.186986  │\n",
       "│ 7   │ 0.314711   │\n",
       "│ 8   │ -0.0768633 │\n",
       "│ 9   │ -0.114551  │\n",
       "│ 10  │ -0.0711872 │\n",
       "⋮\n",
       "│ 133 │ -0.038521  │\n",
       "│ 134 │ -0.0930912 │\n",
       "│ 135 │ -0.164431  │\n",
       "│ 136 │ 0.0933555  │\n",
       "│ 137 │ 0.194334   │\n",
       "│ 138 │ 0.473325   │\n",
       "│ 139 │ -0.0930912 │\n",
       "│ 140 │ 0.445427   │\n",
       "│ 141 │ 0.244167   │\n",
       "│ 142 │ -0.152556  │\n",
       "│ 143 │ 0.37278    │"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we make the prediction on the test data\n",
    "pred = xhat[1]*ones(143,1) + xhat[2]*test.x1 + xhat[3]*test.x2 + xhat[4]*test.x3 + xhat[5]*test.x4 + xhat[6]*test.x5 + xhat[7]*test.x6 + xhat[8]*test.x7 + xhat[9]*test.x8 + xhat[10]*test.x9 + xhat[11]*test.x10 + xhat[12]*test.x11 + xhat[13]*test.x12 + xhat[14]*test.x13 + xhat[15]*test.x14 + xhat[16]*test.x15 + xhat[17]*test.x16 + xhat[18]*test.x17 + xhat[19]*test.x18 + xhat[20]*test.x19 + xhat[21]*test.x20 + xhat[22]*test.x21 + xhat[23]*test.x22 + xhat[24]*test.x23 + xhat[25]*test.x24 + xhat[26]*test.x25 + xhat[27]*test.x26; + xhat[28]*test.x27 + xhat[29]*test.x28 + xhat[30]*test.x29 + xhat[31]*test.x30 + xhat[32]*test.x31 + xhat[33]*test.x32 + xhat[34]*test.x33 + xhat[35]*test.x34 + xhat[36]*test.x35 + xhat[37]*test.x36 + xhat[38]*test.x37 + xhat[39]*test.x38 + xhat[40]*test.x39 + xhat[41]*test.x40 + xhat[42]*test.x41 + xhat[43]*test.x42 + xhat[44]*test.x43 + xhat[45]*test.x44 + xhat[46]*test.x45 + xhat[47]*test.x46 + xhat[48]*test.x47 + xhat[49]*test.x48 + xhat[50]*test.x49 + xhat[51]*test.x50 + xhat[52]*test.x51 + xhat[53]*test.x52 + xhat[54]*test.x53 + xhat[55]*test.x54 + xhat[56]*test.x55 + xhat[57]*test.x56 + xhat[58]*test.x57 + xhat[59]*test.x58 + xhat[60]*test.x59 + xhat[61]*test.x60\n",
    "\n",
    "#we see the probatilities that the model gave to each of the observations\n",
    "pred = DataFrame(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e62d033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we use 0.5 as a threshold once again and assign the prediction label to the data\n",
    "pred_class = [if x < 0.5 0 else 1 end for x in pred.x1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e4939089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we do something similar to the logistic regression model in order to measure accuracy\n",
    "pred_df = DataFrame(y_actual = test.x61, y_predicted = pred_class, prob_predicted = pred);\n",
    "pred_df.correctly_classified = pred_df.y_actual .== pred_df.y_predicted;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "abca4389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is : 62.23776223776224 %"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "accuracy = mean(pred_df.correctly_classified)\n",
    "print(\"Accuracy of the model is : \",accuracy*100, \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5fcf4a",
   "metadata": {},
   "source": [
    "We can see that the accuracy is significantly less than the one for the logistic regression model, and just a few points above the baseline performance for classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b960b5",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "Now we use the Support Vector Machine model for the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "554756da",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LIBSVM # SVM library\n",
    "using RDatasets\n",
    "using Printf\n",
    "using Statistics, LinearAlgebra, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5869f991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is: 82.52%\n"
     ]
    }
   ],
   "source": [
    "# Here we set up the model on the train data\n",
    "X = Matrix(train[:, 1:60])' # the X input has to be transposed\n",
    "y = train.x61\n",
    "X0 = Matrix(test[:, 1:60])'\n",
    "model = svmtrain(X, y); # svmtrain makes the svm model\n",
    "\n",
    "# Now we test on the test data\n",
    "(predicted_labels, decision_values) = svmpredict(model, X0);\n",
    "\n",
    "# And finally we measure accuracy\n",
    "@printf \"Accuracy of the model is: %.2f%%\\n\" mean((predicted_labels .== test.x61))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf0e24",
   "metadata": {},
   "source": [
    "We can see that the model did better than Linear Regression but not better than Logistic Regression in terms of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b98345a",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Next up we have Decision Trees: the model creates a tree which classifies the data by determining which features and with what thresholds split the data into uniform subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "75951535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 4, Threshold 0.0606\n",
      "L-> Feature 12, Threshold 0.22875\n",
      "    L-> Feature 19, Threshold 0.8186\n",
      "        L-> Feature 1, Threshold 0.0382\n",
      "            L-> Feature 28, Threshold 0.9929\n",
      "                L-> Feature 4, Threshold 0.05776505489091939\n",
      "                    L-> Feature 29, Threshold 0.19855\n",
      "                        L-> 1.0 : 1/1\n",
      "                        R-> 0.0 : 159/159\n",
      "                    R-> 1.0 : 1/1\n",
      "                R-> 1.0 : 1/1\n",
      "            R-> Feature 15, Threshold 0.5374218518637437\n",
      "                L-> 1.0 : 3/3\n",
      "                R-> 0.0 : 2/2\n",
      "        R-> Feature 27, Threshold 0.83045\n",
      "            L-> 1.0 : 6/6\n",
      "            R-> 0.0 : 3/3\n",
      "    R-> Feature 27, Threshold 0.7672713000639921\n",
      "        L-> Feature 55, Threshold 0.016444641023191486\n",
      "            L-> Feature 51, Threshold 0.021096420459993263\n",
      "                L-> Feature 2, Threshold 0.01925\n",
      "                    L-> Feature 11, Threshold 0.19531994396532976\n",
      "                        L-> 0.0 : 1/1\n",
      "                        R-> 1.0 : 3/3\n",
      "                    R-> Feature 58, Threshold 0.01667166449307539\n",
      "                        L-> Feature 30, Threshold 0.9278500000000001\n",
      "                            L-> 0.0 : 49/49\n",
      "                            R-> 1.0 : 1/1\n",
      "                        R-> 1.0 : 1/1\n",
      "                R-> 1.0 : 5/5\n",
      "            R-> 1.0 : 7/7\n",
      "        R-> Feature 8, Threshold 0.07465\n",
      "            L-> Feature 49, Threshold 0.02855\n",
      "                L-> 0.0 : 9/9\n",
      "                R-> 1.0 : 4/4\n",
      "            R-> 1.0 : 43/43\n",
      "R-> Feature 37, Threshold 0.6084225765539633\n",
      "    L-> Feature 8, Threshold 0.34704999999999997\n",
      "        L-> Feature 5, Threshold 0.01225\n",
      "            L-> 0.0 : 3/3\n",
      "            R-> Feature 30, Threshold 0.15125\n",
      "                L-> 0.0 : 1/1\n",
      "                R-> 1.0 : 182/182\n",
      "        R-> 0.0 : 7/7\n",
      "    R-> Feature 35, Threshold 0.62015\n",
      "        L-> 0.0 : 20/20\n",
      "        R-> Feature 24, Threshold 0.8089500000000001\n",
      "            L-> 1.0 : 10/10\n",
      "            R-> 0.0 : 1/1\n"
     ]
    }
   ],
   "source": [
    "using DecisionTree # This is the Julia library for Decision Trees model\n",
    "\n",
    "Xt = Matrix(train[:, 1:60]);\n",
    "yt = train.x61;\n",
    "\n",
    "tmodel = DecisionTreeClassifier() #this function creates a tree model to which one can specify or not the desired depht of the tree.\n",
    "\n",
    "DecisionTree.fit!(tmodel, Xt, yt) # we fit the previous model to the train data\n",
    "# and we can see the tree, which features it took and the corresponding thresholds\n",
    "print_tree(tmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "311cf695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the model is: 95.1048951048951%"
     ]
    }
   ],
   "source": [
    "# here we test the model on the test data\n",
    "Xt0 = Matrix(test[:,1:60])\n",
    "yt = DecisionTree.predict(tmodel, Xt0) # the DecisionTree Package has its own predict function\n",
    "\n",
    "# and finally we measure the accuracy of the model\n",
    "accuracy = mean(yt .== test.x61)\n",
    "print(\"accuracy of the model is: \", accuracy*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317200a0",
   "metadata": {},
   "source": [
    "# KNN\n",
    "Finally, we have the K-Nearest Neighbors model for classification: given a point $x$ that we want to classify, what the model does is to take the nearest $k$ neighbors of $x$, and see of which type are most of those neighbors in order to assign to $x$ such label. In order to avoid equal amount of neighbors of different type, one should choose and odd value of $k$. Also $k$ shall not be too big since taking too many neighbors can be counterproductive.<br>\n",
    "\n",
    "The algorithm itself sounds and is indeed very expensive in terms of computation time, but thanks to the $k$-dimensional tree structure (see https://en.wikipedia.org/wiki/K-d_tree), we can partition the set of point in order to search for neighbors in efficient time.<br>\n",
    "\n",
    "The NearestNeighbors package of julia implements the concepts mentioned above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "52965bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143×5 Matrix{Int64}:\n",
       "  25  489  431    1   19\n",
       " 296  489  274  484   25\n",
       " 390  405  185   91  354\n",
       " 171   84   83   12  337\n",
       " 389  459  428  277  406\n",
       " 286  267  194  444   54\n",
       " 403  442  409  125   98\n",
       " 521  263  316  363  142\n",
       "  75  256  275  282  320\n",
       " 428  277  406  389  459\n",
       " 248  446  488  165  516\n",
       " 171   84   83   12  337\n",
       " 479   85  161  411   62\n",
       "   ⋮                 \n",
       " 195  122  300  333    3\n",
       " 161  479   85  411  515\n",
       " 457  443  310  208  376\n",
       " 427  356  166  204  514\n",
       " 248  446  488  165  516\n",
       " 262  383  202   97  107\n",
       " 251   18  274  216  296\n",
       " 457  443  310  208  376\n",
       " 330  302  433  149  303\n",
       " 228    7  440  179    9\n",
       " 258  188   71  415  366\n",
       " 442  409  403  239   96"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using NearestNeighbors #we add the knn package for julia\n",
    "# remember  X = Matrix(train[:, 1:60])'\n",
    "#           y = train.x61\n",
    "#           X0 = Matrix(test[:, 1:60])'\n",
    "\n",
    "kdtree = KDTree(X) # create the k-dimensional tree of the data\n",
    "\n",
    "k = 5 # choose an odd and not too big value for k\n",
    "\n",
    "index_knn, distances = knn(kdtree, X0, k, true) #this gives the indexes of the respective 5 nearest neighbors and the given distances to each point on the test data X0\n",
    "\n",
    "# now we put back that information in ma matrix where we can read it\n",
    "\n",
    "index_knn_matrix = hcat(index_knn...)\n",
    "# each row are the index of the nearest neighbors to the respective observation of the test dataset\n",
    "index_knn_matrix_t = permutedims(index_knn_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ab8cf7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143-element Vector{Float64}:\n",
       " 1.0\n",
       " 1.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 1.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 1.0\n",
       " 0.0\n",
       " 0.0\n",
       " ⋮\n",
       " 1.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 0.0\n",
       " 1.0\n",
       " 1.0\n",
       " 0.0\n",
       " 1.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here we take the classes (labels) of the nearest neighbors to each observation from the test data\n",
    "knn_classes = y[index_knn_matrix_t]\n",
    "\n",
    "# now we make the prediction y_hat by taking the classes that appear the most on those 5 neighbors, \n",
    "# we do that by counting the classes with countmap function and then with the argmax function from StastBase package:\n",
    "\n",
    "y_hat = [\n",
    "    argmax(countmap(knn_classes[i, :], alg = :dict))\n",
    "    for i in 1:143\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cc02121d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the model: 95.1048951048951%"
     ]
    }
   ],
   "source": [
    "# and finally we check accuracy\n",
    "\n",
    "accuracy = mean(y_hat .== test.x61)\n",
    "\n",
    "print(\"accuracy of the model: \", accuracy*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b18ca73",
   "metadata": {},
   "source": [
    "# Which model did better?\n",
    "\n",
    "We can see, based merely on the accuracy of each model, that the one that did better was the logistic regression model, although the decision trees and the knn also had a really high accuracy. For further studies and in order to determine which of the models work better in a more deep way, we can use other performance measures like the confussion matrix.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
