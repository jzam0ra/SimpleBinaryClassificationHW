{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84bf27e5",
   "metadata": {},
   "source": [
    "# Simple binary classification\n",
    "\n",
    "On this notebook we will review some of the techniques and tools we often encounter when working with classification problems. We will give a glimpse on some of the most commonly used models for binary classification in order to use them on the sonar dataset (https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)). We will choose a performance function to study the performance of each model on the data and determine which suits the problem better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69ee6e1",
   "metadata": {},
   "source": [
    "# Reading the data\n",
    "\n",
    "First we read the dataset and give a labe $v_i, 1\\leq i \\leq 60$ to each of the $60$ characteristics, and we call the last column the name $t$ for the type of the material (rock or mine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5a597d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>208 rows × 61 columns (omitted printing of 52 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>v1</th><th>v2</th><th>v3</th><th>v4</th><th>v5</th><th>v6</th><th>v7</th><th>v8</th><th>v9</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>0.02</td><td>0.0371</td><td>0.0428</td><td>0.0207</td><td>0.0954</td><td>0.0986</td><td>0.1539</td><td>0.1601</td><td>0.3109</td></tr><tr><th>2</th><td>0.0453</td><td>0.0523</td><td>0.0843</td><td>0.0689</td><td>0.1183</td><td>0.2583</td><td>0.2156</td><td>0.3481</td><td>0.3337</td></tr><tr><th>3</th><td>0.0262</td><td>0.0582</td><td>0.1099</td><td>0.1083</td><td>0.0974</td><td>0.228</td><td>0.2431</td><td>0.3771</td><td>0.5598</td></tr><tr><th>4</th><td>0.01</td><td>0.0171</td><td>0.0623</td><td>0.0205</td><td>0.0205</td><td>0.0368</td><td>0.1098</td><td>0.1276</td><td>0.0598</td></tr><tr><th>5</th><td>0.0762</td><td>0.0666</td><td>0.0481</td><td>0.0394</td><td>0.059</td><td>0.0649</td><td>0.1209</td><td>0.2467</td><td>0.3564</td></tr><tr><th>6</th><td>0.0286</td><td>0.0453</td><td>0.0277</td><td>0.0174</td><td>0.0384</td><td>0.099</td><td>0.1201</td><td>0.1833</td><td>0.2105</td></tr><tr><th>7</th><td>0.0317</td><td>0.0956</td><td>0.1321</td><td>0.1408</td><td>0.1674</td><td>0.171</td><td>0.0731</td><td>0.1401</td><td>0.2083</td></tr><tr><th>8</th><td>0.0519</td><td>0.0548</td><td>0.0842</td><td>0.0319</td><td>0.1158</td><td>0.0922</td><td>0.1027</td><td>0.0613</td><td>0.1465</td></tr><tr><th>9</th><td>0.0223</td><td>0.0375</td><td>0.0484</td><td>0.0475</td><td>0.0647</td><td>0.0591</td><td>0.0753</td><td>0.0098</td><td>0.0684</td></tr><tr><th>10</th><td>0.0164</td><td>0.0173</td><td>0.0347</td><td>0.007</td><td>0.0187</td><td>0.0671</td><td>0.1056</td><td>0.0697</td><td>0.0962</td></tr><tr><th>11</th><td>0.0039</td><td>0.0063</td><td>0.0152</td><td>0.0336</td><td>0.031</td><td>0.0284</td><td>0.0396</td><td>0.0272</td><td>0.0323</td></tr><tr><th>12</th><td>0.0123</td><td>0.0309</td><td>0.0169</td><td>0.0313</td><td>0.0358</td><td>0.0102</td><td>0.0182</td><td>0.0579</td><td>0.1122</td></tr><tr><th>13</th><td>0.0079</td><td>0.0086</td><td>0.0055</td><td>0.025</td><td>0.0344</td><td>0.0546</td><td>0.0528</td><td>0.0958</td><td>0.1009</td></tr><tr><th>14</th><td>0.009</td><td>0.0062</td><td>0.0253</td><td>0.0489</td><td>0.1197</td><td>0.1589</td><td>0.1392</td><td>0.0987</td><td>0.0955</td></tr><tr><th>15</th><td>0.0124</td><td>0.0433</td><td>0.0604</td><td>0.0449</td><td>0.0597</td><td>0.0355</td><td>0.0531</td><td>0.0343</td><td>0.1052</td></tr><tr><th>16</th><td>0.0298</td><td>0.0615</td><td>0.065</td><td>0.0921</td><td>0.1615</td><td>0.2294</td><td>0.2176</td><td>0.2033</td><td>0.1459</td></tr><tr><th>17</th><td>0.0352</td><td>0.0116</td><td>0.0191</td><td>0.0469</td><td>0.0737</td><td>0.1185</td><td>0.1683</td><td>0.1541</td><td>0.1466</td></tr><tr><th>18</th><td>0.0192</td><td>0.0607</td><td>0.0378</td><td>0.0774</td><td>0.1388</td><td>0.0809</td><td>0.0568</td><td>0.0219</td><td>0.1037</td></tr><tr><th>19</th><td>0.027</td><td>0.0092</td><td>0.0145</td><td>0.0278</td><td>0.0412</td><td>0.0757</td><td>0.1026</td><td>0.1138</td><td>0.0794</td></tr><tr><th>20</th><td>0.0126</td><td>0.0149</td><td>0.0641</td><td>0.1732</td><td>0.2565</td><td>0.2559</td><td>0.2947</td><td>0.411</td><td>0.4983</td></tr><tr><th>21</th><td>0.0473</td><td>0.0509</td><td>0.0819</td><td>0.1252</td><td>0.1783</td><td>0.307</td><td>0.3008</td><td>0.2362</td><td>0.383</td></tr><tr><th>22</th><td>0.0664</td><td>0.0575</td><td>0.0842</td><td>0.0372</td><td>0.0458</td><td>0.0771</td><td>0.0771</td><td>0.113</td><td>0.2353</td></tr><tr><th>23</th><td>0.0099</td><td>0.0484</td><td>0.0299</td><td>0.0297</td><td>0.0652</td><td>0.1077</td><td>0.2363</td><td>0.2385</td><td>0.0075</td></tr><tr><th>24</th><td>0.0115</td><td>0.015</td><td>0.0136</td><td>0.0076</td><td>0.0211</td><td>0.1058</td><td>0.1023</td><td>0.044</td><td>0.0931</td></tr><tr><th>25</th><td>0.0293</td><td>0.0644</td><td>0.039</td><td>0.0173</td><td>0.0476</td><td>0.0816</td><td>0.0993</td><td>0.0315</td><td>0.0736</td></tr><tr><th>26</th><td>0.0201</td><td>0.0026</td><td>0.0138</td><td>0.0062</td><td>0.0133</td><td>0.0151</td><td>0.0541</td><td>0.021</td><td>0.0505</td></tr><tr><th>27</th><td>0.0151</td><td>0.032</td><td>0.0599</td><td>0.105</td><td>0.1163</td><td>0.1734</td><td>0.1679</td><td>0.1119</td><td>0.0889</td></tr><tr><th>28</th><td>0.0177</td><td>0.03</td><td>0.0288</td><td>0.0394</td><td>0.063</td><td>0.0526</td><td>0.0688</td><td>0.0633</td><td>0.0624</td></tr><tr><th>29</th><td>0.01</td><td>0.0275</td><td>0.019</td><td>0.0371</td><td>0.0416</td><td>0.0201</td><td>0.0314</td><td>0.0651</td><td>0.1896</td></tr><tr><th>30</th><td>0.0189</td><td>0.0308</td><td>0.0197</td><td>0.0622</td><td>0.008</td><td>0.0789</td><td>0.144</td><td>0.1451</td><td>0.1789</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccc}\n",
       "\t& v1 & v2 & v3 & v4 & v5 & v6 & v7 & v8 & v9 & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 0.02 & 0.0371 & 0.0428 & 0.0207 & 0.0954 & 0.0986 & 0.1539 & 0.1601 & 0.3109 & $\\dots$ \\\\\n",
       "\t2 & 0.0453 & 0.0523 & 0.0843 & 0.0689 & 0.1183 & 0.2583 & 0.2156 & 0.3481 & 0.3337 & $\\dots$ \\\\\n",
       "\t3 & 0.0262 & 0.0582 & 0.1099 & 0.1083 & 0.0974 & 0.228 & 0.2431 & 0.3771 & 0.5598 & $\\dots$ \\\\\n",
       "\t4 & 0.01 & 0.0171 & 0.0623 & 0.0205 & 0.0205 & 0.0368 & 0.1098 & 0.1276 & 0.0598 & $\\dots$ \\\\\n",
       "\t5 & 0.0762 & 0.0666 & 0.0481 & 0.0394 & 0.059 & 0.0649 & 0.1209 & 0.2467 & 0.3564 & $\\dots$ \\\\\n",
       "\t6 & 0.0286 & 0.0453 & 0.0277 & 0.0174 & 0.0384 & 0.099 & 0.1201 & 0.1833 & 0.2105 & $\\dots$ \\\\\n",
       "\t7 & 0.0317 & 0.0956 & 0.1321 & 0.1408 & 0.1674 & 0.171 & 0.0731 & 0.1401 & 0.2083 & $\\dots$ \\\\\n",
       "\t8 & 0.0519 & 0.0548 & 0.0842 & 0.0319 & 0.1158 & 0.0922 & 0.1027 & 0.0613 & 0.1465 & $\\dots$ \\\\\n",
       "\t9 & 0.0223 & 0.0375 & 0.0484 & 0.0475 & 0.0647 & 0.0591 & 0.0753 & 0.0098 & 0.0684 & $\\dots$ \\\\\n",
       "\t10 & 0.0164 & 0.0173 & 0.0347 & 0.007 & 0.0187 & 0.0671 & 0.1056 & 0.0697 & 0.0962 & $\\dots$ \\\\\n",
       "\t11 & 0.0039 & 0.0063 & 0.0152 & 0.0336 & 0.031 & 0.0284 & 0.0396 & 0.0272 & 0.0323 & $\\dots$ \\\\\n",
       "\t12 & 0.0123 & 0.0309 & 0.0169 & 0.0313 & 0.0358 & 0.0102 & 0.0182 & 0.0579 & 0.1122 & $\\dots$ \\\\\n",
       "\t13 & 0.0079 & 0.0086 & 0.0055 & 0.025 & 0.0344 & 0.0546 & 0.0528 & 0.0958 & 0.1009 & $\\dots$ \\\\\n",
       "\t14 & 0.009 & 0.0062 & 0.0253 & 0.0489 & 0.1197 & 0.1589 & 0.1392 & 0.0987 & 0.0955 & $\\dots$ \\\\\n",
       "\t15 & 0.0124 & 0.0433 & 0.0604 & 0.0449 & 0.0597 & 0.0355 & 0.0531 & 0.0343 & 0.1052 & $\\dots$ \\\\\n",
       "\t16 & 0.0298 & 0.0615 & 0.065 & 0.0921 & 0.1615 & 0.2294 & 0.2176 & 0.2033 & 0.1459 & $\\dots$ \\\\\n",
       "\t17 & 0.0352 & 0.0116 & 0.0191 & 0.0469 & 0.0737 & 0.1185 & 0.1683 & 0.1541 & 0.1466 & $\\dots$ \\\\\n",
       "\t18 & 0.0192 & 0.0607 & 0.0378 & 0.0774 & 0.1388 & 0.0809 & 0.0568 & 0.0219 & 0.1037 & $\\dots$ \\\\\n",
       "\t19 & 0.027 & 0.0092 & 0.0145 & 0.0278 & 0.0412 & 0.0757 & 0.1026 & 0.1138 & 0.0794 & $\\dots$ \\\\\n",
       "\t20 & 0.0126 & 0.0149 & 0.0641 & 0.1732 & 0.2565 & 0.2559 & 0.2947 & 0.411 & 0.4983 & $\\dots$ \\\\\n",
       "\t21 & 0.0473 & 0.0509 & 0.0819 & 0.1252 & 0.1783 & 0.307 & 0.3008 & 0.2362 & 0.383 & $\\dots$ \\\\\n",
       "\t22 & 0.0664 & 0.0575 & 0.0842 & 0.0372 & 0.0458 & 0.0771 & 0.0771 & 0.113 & 0.2353 & $\\dots$ \\\\\n",
       "\t23 & 0.0099 & 0.0484 & 0.0299 & 0.0297 & 0.0652 & 0.1077 & 0.2363 & 0.2385 & 0.0075 & $\\dots$ \\\\\n",
       "\t24 & 0.0115 & 0.015 & 0.0136 & 0.0076 & 0.0211 & 0.1058 & 0.1023 & 0.044 & 0.0931 & $\\dots$ \\\\\n",
       "\t25 & 0.0293 & 0.0644 & 0.039 & 0.0173 & 0.0476 & 0.0816 & 0.0993 & 0.0315 & 0.0736 & $\\dots$ \\\\\n",
       "\t26 & 0.0201 & 0.0026 & 0.0138 & 0.0062 & 0.0133 & 0.0151 & 0.0541 & 0.021 & 0.0505 & $\\dots$ \\\\\n",
       "\t27 & 0.0151 & 0.032 & 0.0599 & 0.105 & 0.1163 & 0.1734 & 0.1679 & 0.1119 & 0.0889 & $\\dots$ \\\\\n",
       "\t28 & 0.0177 & 0.03 & 0.0288 & 0.0394 & 0.063 & 0.0526 & 0.0688 & 0.0633 & 0.0624 & $\\dots$ \\\\\n",
       "\t29 & 0.01 & 0.0275 & 0.019 & 0.0371 & 0.0416 & 0.0201 & 0.0314 & 0.0651 & 0.1896 & $\\dots$ \\\\\n",
       "\t30 & 0.0189 & 0.0308 & 0.0197 & 0.0622 & 0.008 & 0.0789 & 0.144 & 0.1451 & 0.1789 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m208×61 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m v1      \u001b[0m\u001b[1m v2      \u001b[0m\u001b[1m v3      \u001b[0m\u001b[1m v4      \u001b[0m\u001b[1m v5      \u001b[0m\u001b[1m v6      \u001b[0m\u001b[1m v7      \u001b[0m\u001b[1m v8      \u001b[0m\u001b[1m\u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │  0.02     0.0371   0.0428   0.0207   0.0954   0.0986   0.1539   0.1601  ⋯\n",
       "   2 │  0.0453   0.0523   0.0843   0.0689   0.1183   0.2583   0.2156   0.3481\n",
       "   3 │  0.0262   0.0582   0.1099   0.1083   0.0974   0.228    0.2431   0.3771\n",
       "   4 │  0.01     0.0171   0.0623   0.0205   0.0205   0.0368   0.1098   0.1276\n",
       "   5 │  0.0762   0.0666   0.0481   0.0394   0.059    0.0649   0.1209   0.2467  ⋯\n",
       "   6 │  0.0286   0.0453   0.0277   0.0174   0.0384   0.099    0.1201   0.1833\n",
       "   7 │  0.0317   0.0956   0.1321   0.1408   0.1674   0.171    0.0731   0.1401\n",
       "   8 │  0.0519   0.0548   0.0842   0.0319   0.1158   0.0922   0.1027   0.0613\n",
       "   9 │  0.0223   0.0375   0.0484   0.0475   0.0647   0.0591   0.0753   0.0098  ⋯\n",
       "  10 │  0.0164   0.0173   0.0347   0.007    0.0187   0.0671   0.1056   0.0697\n",
       "  11 │  0.0039   0.0063   0.0152   0.0336   0.031    0.0284   0.0396   0.0272\n",
       "  ⋮  │    ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮     ⋱\n",
       " 199 │  0.0238   0.0318   0.0422   0.0399   0.0788   0.0766   0.0881   0.1143\n",
       " 200 │  0.0116   0.0744   0.0367   0.0225   0.0076   0.0545   0.111    0.1069  ⋯\n",
       " 201 │  0.0131   0.0387   0.0329   0.0078   0.0721   0.1341   0.1626   0.1902\n",
       " 202 │  0.0335   0.0258   0.0398   0.057    0.0529   0.1091   0.1709   0.1684\n",
       " 203 │  0.0272   0.0378   0.0488   0.0848   0.1127   0.1103   0.1349   0.2337\n",
       " 204 │  0.0187   0.0346   0.0168   0.0177   0.0393   0.163    0.2028   0.1694  ⋯\n",
       " 205 │  0.0323   0.0101   0.0298   0.0564   0.076    0.0958   0.099    0.1018\n",
       " 206 │  0.0522   0.0437   0.018    0.0292   0.0351   0.1171   0.1257   0.1178\n",
       " 207 │  0.0303   0.0353   0.049    0.0608   0.0167   0.1354   0.1465   0.1123\n",
       " 208 │  0.026    0.0363   0.0136   0.0272   0.0214   0.0338   0.0655   0.14    ⋯\n",
       "\u001b[36m                                                 53 columns and 187 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV, DataFrames\n",
    "data1 = DataFrame(CSV.read(\"sonar1.csv\", DataFrame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "45ab594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we replace the categorical labels for numerical ones\n",
    "data1.t .= replace.(data1.t, \"R\" => \"0\");  \n",
    "data1.t .= replace.(data1.t, \"M\" => \"1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e24f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we parse the last column to float number so that the models treat the values as numbers and not strings\n",
    "data1.t = parse.(Float64, data1.t);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "47cd7916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see the data as a matrix and note that the data is of type Float64\n",
    "M = Matrix(data1[:, 1:60]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77133a3f",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction (PCA)\n",
    "\n",
    "We see that it's often better to work with a low dimension characteristic set. For that, we use PCA in order to project into a lower dimension while still keeping around 90% of the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5db98f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208×60 Matrix{Float64}:\n",
       " -0.39859    -0.0405504   -0.0268608  …   0.0697021    0.171265  -0.657361\n",
       "  0.701845    0.420616     1.05308       -0.471269    -0.443484  -0.418842\n",
       " -0.128918    0.599621     1.71926        1.30621      0.252153   0.256962\n",
       " -0.833544   -0.647348     0.48058       -0.548551    -0.637616   1.03215\n",
       "  2.04585     0.854476     0.111059      -0.486726     0.446284   0.574988\n",
       " -0.0245289   0.208237    -0.419802   …  -0.811309    -0.459662  -0.0610632\n",
       "  0.110307    1.73433      2.29696        0.981626    -0.702326   0.753877\n",
       "  0.988915    0.496465     1.05048       -0.502182    -0.508195  -0.239953\n",
       " -0.29855    -0.0284145    0.118866       0.208809    -0.330241  -0.856127\n",
       " -0.555173   -0.64128     -0.237644      -0.687658    -0.378774  -0.498348\n",
       " -1.09887    -0.975018    -0.745085   …  -1.18226     -0.427307  -0.577855\n",
       " -0.733505   -0.228658    -0.700847       0.193353    -1.13912   -0.418842\n",
       " -0.924885   -0.905237    -0.997504      -0.332162    -0.330241  -0.657361\n",
       "  ⋮                                   ⋱                          \n",
       " -1.05102    -1.11458     -0.438018      -0.254881    -1.0097    -0.736867\n",
       "  0.323435    0.111149     0.170911      -0.965872    -0.847925  -0.756744\n",
       " -0.233307   -0.201352    -0.0424744     -1.0277      -0.718504  -0.100816\n",
       " -0.763951    1.09113     -0.185599      -0.548551    -0.362596  -0.597731\n",
       " -0.698708    0.00799339  -0.284485   …  -1.08952     -1.04206    0.396098\n",
       "  0.188599   -0.383391    -0.104929      -0.88859     -1.20383   -0.677238\n",
       " -0.0854225  -0.0193125    0.129275      -0.440357    -0.233175   0.753877\n",
       " -0.455134   -0.1164      -0.703449       0.548848     1.83756    1.82721\n",
       "  0.136404   -0.859727    -0.365155      -0.734027    -0.281708   0.0383198\n",
       "  1.00196     0.159693    -0.672222   …   0.904344    -0.039044  -0.677238\n",
       "  0.0494133  -0.0951622    0.134479      -0.00757955  -0.702326  -0.339335\n",
       " -0.137617   -0.0648223   -0.786721      -0.672202    -0.297886   0.992396"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Apply PCA to the iris dataset\n",
    "using MultivariateStats\n",
    "using Statistics\n",
    "# center and normalize the data\n",
    "data = M\n",
    "data = (data .- mean(data,dims = 1))./ std(data,dims = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97271ea",
   "metadata": {},
   "source": [
    "# Detecting outliers\n",
    "Before we proceed to the dimensionality reduction, we need to identify the characteristics on the observations that are away from the rest because they can affect the training process, that is, we look for the observations that are more than 3 standard deviations away from the mean and consider them as outliers, then we replace them by the mean of the corresponding column(characteristic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "319eb9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208×60 Matrix{Float64}:\n",
       " -0.39859    -0.0405504   -0.0268608  …   0.0697021    0.171265  -0.657361\n",
       "  0.701845    0.420616     1.05308       -0.471269    -0.443484  -0.418842\n",
       " -0.128918    0.599621     1.71926        1.30621      0.252153   0.256962\n",
       " -0.833544   -0.647348     0.48058       -0.548551    -0.637616   1.03215\n",
       "  2.04585     0.854476     0.111059      -0.486726     0.446284   0.574988\n",
       " -0.0245289   0.208237    -0.419802   …  -0.811309    -0.459662  -0.0610632\n",
       "  0.110307    1.73433      2.29696        0.981626    -0.702326   0.753877\n",
       "  0.988915    0.496465     1.05048       -0.502182    -0.508195  -0.239953\n",
       " -0.29855    -0.0284145    0.118866       0.208809    -0.330241  -0.856127\n",
       " -0.555173   -0.64128     -0.237644      -0.687658    -0.378774  -0.498348\n",
       " -1.09887    -0.975018    -0.745085   …  -1.18226     -0.427307  -0.577855\n",
       " -0.733505   -0.228658    -0.700847       0.193353    -1.13912   -0.418842\n",
       " -0.924885   -0.905237    -0.997504      -0.332162    -0.330241  -0.657361\n",
       "  ⋮                                   ⋱                          \n",
       " -1.05102    -1.11458     -0.438018      -0.254881    -1.0097    -0.736867\n",
       "  0.323435    0.111149     0.170911      -0.965872    -0.847925  -0.756744\n",
       " -0.233307   -0.201352    -0.0424744     -1.0277      -0.718504  -0.100816\n",
       " -0.763951    1.09113     -0.185599      -0.548551    -0.362596  -0.597731\n",
       " -0.698708    0.00799339  -0.284485   …  -1.08952     -1.04206    0.396098\n",
       "  0.188599   -0.383391    -0.104929      -0.88859     -1.20383   -0.677238\n",
       " -0.0854225  -0.0193125    0.129275      -0.440357    -0.233175   0.753877\n",
       " -0.455134   -0.1164      -0.703449       0.548848     1.83756    1.82721\n",
       "  0.136404   -0.859727    -0.365155      -0.734027    -0.281708   0.0383198\n",
       "  1.00196     0.159693    -0.672222   …   0.904344    -0.039044  -0.677238\n",
       "  0.0494133  -0.0951622    0.134479      -0.00757955  -0.702326  -0.339335\n",
       " -0.137617   -0.0648223   -0.786721      -0.672202    -0.297886   0.992396"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i = 1:60\n",
    "    for j = 1:208\n",
    "        if abs(data[j, i] .> 3)\n",
    "            data[j, i] = mean(data[:,i])\n",
    "        end\n",
    "    end\n",
    "end\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5767e5a2",
   "metadata": {},
   "source": [
    "Now we proceed to apply dimensionality reduction on the characteristics dimension. PCA expects the data matrix to be transposed: each row to be a characteristic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "30c61477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60×208 adjoint(::Matrix{Float64}) with eltype Float64:\n",
       " -0.39859     0.701845   -0.128918     …   1.00196     0.0494133   -0.137617\n",
       " -0.0405504   0.420616    0.599621         0.159693   -0.0951622   -0.0648223\n",
       " -0.0268608   1.05308     1.71926         -0.672222    0.134479    -0.786721\n",
       " -0.713384    0.322552    1.16935         -0.530698    0.148463    -0.573683\n",
       "  0.363579    0.775804    0.399581        -0.721887   -1.05311     -0.968502\n",
       " -0.101009    2.60094     2.0883       …   0.211991    0.521607    -1.19736\n",
       "  0.520383    1.51896     1.96403          0.0639829   0.400618    -0.910318\n",
       "  0.297126    2.50494     2.8455          -0.199631   -0.264222     0.0610784\n",
       "  1.12256     1.31515    -3.83207e-16     -0.44095     0.139349     0.0531909\n",
       "  0.0211349   0.587289    2.12704e-16      0.33211     0.201917     0.201917\n",
       " -0.566016    1.92749     2.99377      …   0.268167    0.405314     0.271181\n",
       " -0.656956    2.89122    -3.85009e-16     -0.0915326   0.221164    -0.0429861\n",
       " -0.351196    2.96956     1.99412         -0.606584   -0.819407    -0.757688\n",
       "  ⋮                                    ⋱   ⋮                       \n",
       " -0.379064   -0.306749   -1.08274          0.043699   -0.0842423   -1.01599\n",
       "  0.876396   -1.04823    -0.718919         0.225098    0.269005    -0.64574\n",
       "  0.59385    -0.297185   -1.06331      …  -0.0473623  -0.988363     0.169151\n",
       " -1.11275    -0.521092    1.01514          0.267782   -0.500332     0.122463\n",
       " -0.596166   -0.256239    0.83436         -1.10606    -0.865275     0.310306\n",
       "  0.679259   -0.841122   -0.197357        -0.80003     0.227254    -0.854819\n",
       " -0.294934    0.0154657   1.22885         -0.436025   -0.802861    -0.760534\n",
       "  1.47807     1.89647     2.82044      …   0.118262   -0.823142    -0.369873\n",
       "  1.75954     1.06816    -2.5407e-16       1.06816    -0.764013    -0.660305\n",
       "  0.0697021  -0.471269    1.30621          0.904344   -0.00757955  -0.672202\n",
       "  0.171265   -0.443484    0.252153        -0.039044   -0.702326    -0.297886\n",
       " -0.657361   -0.418842    0.256962        -0.677238   -0.339335     0.992396"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each obs is now a column, PCA takes features - by - samples matrix\n",
    "data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "09bda235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(indim = 60, outdim = 25, principalratio = 0.9067132194194307)\n",
       "\n",
       "Pattern matrix (unstandardized loadings):\n",
       "─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
       "            PC1         PC2          PC3           PC4         PC5          PC6          PC7          PC8          PC9         PC10         PC11          PC12          PC13         PC14         PC15         PC16         PC17         PC18          PC19          PC20          PC21          PC22          PC23         PC24         PC25\n",
       "─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
       "1    0.082827     0.358692    0.120163    -0.0558756    -0.135079    0.104782     0.0186033   -0.0340367    0.0966793   -0.0368675   -0.156471    -0.118664      0.233273     -0.0406281    0.0223378   -0.121809     0.134271    -0.0801844   -0.0336684    -0.00975443    0.141552     -0.0596837     0.0637659     0.0495071    0.0481619\n",
       "2    0.0587084    0.528864    0.0723349   -0.131003     -0.130821    0.1362       0.114179     0.0936582    0.159492     0.00509244  -0.146694    -0.206513      0.0984866     0.0214154   -0.0398559   -0.108691     0.181843    -0.100123    -0.0279918    -0.0897057     0.135641     -0.0886553    -0.0184388     0.116584     0.134361\n",
       "3    0.108987     0.448642    0.0902608   -0.10421      -0.137789    0.121386     0.0836047   -0.0322664    0.0978928    0.00316136  -0.134064    -0.315137      0.170899      0.044623     0.0439856    0.0555657    0.0790321    0.00280204   0.0400943     0.0107736    -0.0882658    -0.00252581   -0.0555513    -0.00487359   0.0580554\n",
       "4    0.180699     0.42458     0.0346845   -0.0490582    -0.117542    0.159385     0.110188    -0.00619449   0.0428468   -0.0733548    0.0560818   -0.277622      0.105413     -0.016269    -0.0208363    0.114594     0.0931185    0.0233028    0.111372      0.0326375    -0.146866      0.104386      0.0787246    -0.0604295   -0.250366\n",
       "5    0.26518      0.304419    0.102102     0.0164683    -0.0199418   0.176017     0.0197251   -0.085093    -0.0138699    0.0381732    0.280112    -0.279283     -0.0404327    -0.0824215   -0.144612     0.150122     0.210851    -0.0242342   -0.00627729   -0.0458567     0.0354151     0.116647      0.017805     -0.0673633   -0.198812\n",
       "6    0.295294     0.360969    0.234981    -0.0611969     0.0816755   0.345667     0.147184    -0.0811018   -0.162722    -0.123587     0.334318    -0.038668     -0.206453     -0.142193    -0.127485     0.0831342    0.0848236   -0.129576    -0.0160636    -0.0347008     0.0550255    -0.0349892     0.0470277    -0.0677902    0.211031\n",
       "7    0.275577     0.351724    0.266211    -0.0662633     0.165688    0.221629     0.1307       0.112911    -0.352637    -0.275676     0.240958     0.0756083    -0.196186     -0.161852     0.00903947  -0.0403628   -0.00680419  -0.0963654   -0.0899104     0.00663277   -0.0516261    -0.0598349     0.0831745     0.072216    -0.00699931\n",
       "8    0.185991     0.334593    0.327244    -0.126909      0.134172    0.152387     0.126493     0.0488241   -0.221569    -0.164122     0.0383891    0.0543493     0.0826046    -0.0444623    0.0607505   -0.294652    -0.0318528   -0.00365021  -0.186811     -0.00251977   -0.0897001    -0.00813743   -0.0404798     0.0867136   -0.0968425\n",
       "9    0.175472     0.230761    0.422967    -0.107645      0.284868   -0.0566117    0.0957643    0.0461252   -0.108889    -0.210643    -0.051697    -0.0489506     0.121367      0.164898    -0.0685413   -0.105199    -0.125046    -0.0046268   -0.00854504   -0.0381904    -0.0645057     0.0765214    -0.0585006     0.00117402   0.0242432\n",
       "10   0.158293     0.378368    0.413011    -0.124104      0.386505   -0.170199     0.231516     0.118536    -0.0844865   -0.227407    -0.0663241   -0.0874495     0.0316273     0.159339    -0.0387399    0.0562463   -0.170898     0.0534479    0.0734579    -0.107282      0.0144723     0.0520095     0.0109927    -0.0402192    0.0309592\n",
       "11   0.307376     0.41518     0.437866    -0.192617      0.399921   -0.0684247    0.198785    -0.0449429    0.0827204   -0.0837935   -0.134671    -0.0111692     0.0444883     0.0529223    0.0752016    0.024349    -0.0492392    0.0924868   -0.00712866    0.0339662    -0.00285019    0.192776     -0.0250955    -0.00731626  -0.0122305\n",
       "12   0.30776      0.322883    0.43429     -0.200383      0.322576   -0.0745475    0.0930639   -0.391001     0.268053     0.0172082   -0.0546726    0.110745     -0.0269303    -0.00352221   0.102057    -0.0361615    0.0992234    0.020304     0.0222604     0.231215      0.117348      0.10694       0.0505044    -0.0271033   -0.00170532\n",
       "13   0.430224     0.352646    0.395811    -0.201687      0.260167   -0.00367302  -0.0203365   -0.292866     0.160747     0.13981     -0.0226632    0.197532     -0.0585761    -0.0604226    0.0690764   -0.00527329   0.110658     0.0347598    0.0697331     0.0840382     0.0700343    -0.104893      0.0163268    -0.070637     0.065585\n",
       "14   0.583448     0.421       0.184104    -0.17519       0.0841199   0.0610536   -0.128885    -0.101267    -0.0293663    0.205279    -0.0520016    0.0383327     0.0436785    -0.0134209   -0.0325696   -0.00127386   0.0790907    0.19126      0.0543867    -0.116941      0.0233835    -0.185224     -0.0026408    -0.00534038  -0.0673536\n",
       "15   0.657338     0.44039     0.0161815   -0.143368      0.0175284   0.106992    -0.174739     0.0862043   -0.202254     0.200533    -0.0872068   -0.00639466    0.041392     -0.0138435   -0.0889422   -0.0760513    0.0144227    0.208283    -0.00180399   -0.162945      0.0175451    -0.138472     -0.0741799    -0.0283377   -0.0395333\n",
       "16   0.697532     0.437111   -0.0724291   -0.0474074     0.0796216   0.114419    -0.266896     0.0757549   -0.258638     0.249867    -0.100213     0.000427653   0.00449485    0.03813     -0.0545941    0.00677921  -0.0484356    0.0507726   -0.0202766    -0.060978      0.0185146    -0.0436453    -0.033417     -0.0345372    0.0183551\n",
       "17   0.729822     0.316991   -0.110124     0.0659939     0.143363    0.159562    -0.334331     0.00947905  -0.22227      0.230183    -0.0938091   -0.00313587   -0.00051955    0.0713736   -0.00714217   0.0514116   -0.062166    -0.0961623    0.0334503     0.0696495     0.00965396    0.0823939     9.55843e-5   -0.0235309    0.0230873\n",
       "18   0.765576     0.221757   -0.166948     0.175697      0.181807    0.096581    -0.346576    -0.00166168  -0.109091     0.140698     0.0246157    0.0322721     0.0057072     0.0399823    0.0447065    0.0681433   -0.0646232   -0.117757     0.024856      0.139056     -0.00508475    0.115513      0.0370177     0.0471224   -0.008215\n",
       "19   0.777566     0.208106   -0.110928     0.262678      0.153797    0.00159486  -0.251789     0.100181     0.146673     0.081705     0.145104     0.039643     -0.044269     -0.0314035    0.0135578    0.0775963   -0.0676314   -0.0973318    0.0230803     0.0941434    -0.0448149     0.0865459    -0.0168964     0.137476     0.0261361\n",
       "20   0.709221     0.244443   -0.0487448    0.315075      0.189019   -0.12452     -0.0454754    0.226974     0.361302     0.00804392   0.10883     -0.0284357     0.0153651    -0.0861174   -0.040346     0.0426289   -0.0117685    0.0305473    0.0264174    -0.0229789    -0.0759238    -0.0158348    -0.0314285     0.113562     0.0747332\n",
       "21   0.688515     0.145664    0.025922     0.415478      0.107395   -0.116917     0.0508738    0.292773     0.342488    -0.0443646    0.0484551   -0.0638811     0.0848204    -0.114908    -0.0433756   -0.0268895   -0.0236409    0.052788    -0.000681671  -0.0515131    -0.0317126    -0.0415918     0.0161176     0.0316889    0.0526923\n",
       "22   0.660924    -0.135668    0.175014     0.562919     -0.0108461   0.0613049   -0.0220195    0.175598     0.197191    -0.0965578    0.00988857  -0.0351825     0.0483385     0.0169625    0.0731579   -0.0953789   -0.0179008   -0.00214145   0.0111196    -0.0205182     0.0661436    -0.0176945     0.0727567    -0.155527     0.0115602\n",
       "23   0.504905    -0.357546    0.233092     0.568322     -0.0970457   0.236512    -0.0048741   -0.00756635   0.0382695   -0.174558     0.0127072    0.062371      0.0368933     0.0934781    0.166146    -0.121579     0.0386829   -0.0377755    0.0694449    -0.000386428   0.0120182    -0.0361223     0.000961763  -0.168559    -0.0469555\n",
       "24   0.353693    -0.498882    0.305879     0.482154     -0.127148    0.312266     0.08695     -0.0693176   -0.0958796   -0.120425    -0.0739616    0.125241      0.0231871     0.0654578    0.136255    -0.0657163    0.13962      0.0313612    0.00909136    0.0122985    -0.041263     -0.0455481    -0.141641      0.0207317   -0.0657622\n",
       "25   0.219711    -0.617287    0.373348     0.389725     -0.052496    0.266317     0.144008    -0.120435    -0.11554      0.0363971   -0.139649     0.0871348    -0.0216503     0.0187652   -0.0165958    0.100618     0.0922831    0.0949207   -0.111447      0.00114475    0.0213907     0.032962     -0.109009      0.136916    -0.0318806\n",
       "26   0.0771757   -0.665039    0.436943     0.26419       0.0228947   0.109134     0.238473    -0.07364     -0.0964591    0.170866    -0.0859714   -0.0537988     0.0109799     0.0297069   -0.129104     0.214195    -0.036528     0.133063    -0.0843587    -0.061225      0.0402223     0.0244989     0.0368205     0.104866     0.0353753\n",
       "27  -0.056742    -0.657289    0.490554     0.086648      0.0556145   0.0574151    0.262679    -0.00121227  -0.0663835    0.314275     0.0305938   -0.0505982     0.101723      0.0764642   -0.0682065    0.154601    -0.0716418    0.0281824    0.0453778    -0.0305258    -0.0185314     0.014236      0.134843      0.0172955    0.10247\n",
       "28  -0.330783    -0.533618    0.482131    -0.159462      0.103223    0.141355     0.205855     0.21632      0.04523      0.296723     0.142133     0.0139099     0.0833814    -0.0410294   -0.0158449    0.0364519   -0.0238956   -0.0149902    0.131016      0.0366962    -0.0188083     0.00336286    0.0332082    -0.00803539   0.028828\n",
       "29  -0.485656    -0.444242    0.365613    -0.260643      0.124523    0.183055    -0.0619817    0.295598     0.114963     0.175274     0.177199     0.0755557     0.0436575    -0.197094     0.0304139   -0.117333     0.0553377    0.0430736    0.133826      0.0938651     0.0493699     0.0319221    -0.0687869    -0.0217522   -0.0505987\n",
       "30  -0.620435    -0.187041    0.270002    -0.214456      0.146372    0.24143     -0.253559     0.367851     0.0772718    0.10738      0.102012     0.0329356     0.123187     -0.125976     0.10719     -0.110757    -0.0217298    0.0751708    0.0219631    -0.0236789    -0.0351184     0.0729475    -0.0534671    -0.0201396   -0.0210557\n",
       "31  -0.645689    -0.0900794   0.247892    -0.177814      0.127885    0.400911    -0.393068     0.0839398    0.043651    -0.00935888  -0.0128116    0.0268017     0.144215      0.129109     0.0631554    0.0123578    0.00945308  -0.0921039   -0.0971024    -0.0495421    -0.00936572    0.000262631  -0.000691679  -0.0673936    0.0287664\n",
       "32  -0.654882     0.0251534   0.187014    -0.000479342   0.226833    0.361507    -0.324661    -0.0573837    0.169849    -0.104271    -0.139502     0.00379471   -0.0165789     0.206773    -0.019949     0.180106     0.0204043   -0.162828    -0.0912168    -0.0523408    -0.0521456    -0.0892717     0.0949727    -0.0411136    0.0172124\n",
       "33  -0.706082     0.129975   -0.0552099    0.131502      0.306373    0.225521    -0.245271    -0.0751004    0.214851    -0.230079    -0.0797846    0.0408907    -0.0791778     0.0573486   -0.0854244    0.140871     0.0126097    0.0762759    0.0522526    -0.0130858    -0.0294524    -0.083465      0.0803632     0.0492235   -0.0617189\n",
       "34  -0.661171     0.270932   -0.278569     0.214614      0.317114    0.208984    -0.0684994   -0.0249206    0.168108    -0.173292    -0.0147607    0.0011168    -0.108507     -0.0742015   -0.101929    -0.0022947    0.0282956    0.238048     0.0659172    -0.0058019     0.0357356     0.0118496    -0.0249174     0.123876     0.0058181\n",
       "35  -0.608167     0.346237   -0.367942     0.280916      0.224799    0.241666     0.0644968    0.0509212   -0.0743348    0.0139576   -0.0550735   -0.0895211    -0.0992698    -0.0301084   -0.0565207   -0.0874659   -0.00244047   0.191184     0.0743174     0.000634575   0.0754475     0.0958574    -0.0354672     0.100877     0.05535\n",
       "36  -0.580252     0.377146   -0.339327     0.341956      0.229984    0.131142     0.180985     0.0232813   -0.247757     0.109764    -0.0567894   -0.0243828    -0.0224155     0.037941     0.00656208  -0.122493     0.037392     0.0394784    0.149724      0.091946      0.0600735     0.0137563    -0.016145     -0.00345646   0.0403766\n",
       "37  -0.503023     0.418425   -0.296426     0.322102      0.237989    0.0118801    0.211557     0.0885418   -0.257754     0.204516    -0.0588972   -0.0470532     0.0924895     0.115345     0.0749137   -0.0679972    0.11163     -0.0858858    0.0620019     0.156266      0.0380308     0.0129908     0.0780046    -0.0119493    0.0155578\n",
       "38  -0.480932     0.503143   -0.215102     0.241033      0.202135   -0.137066     0.216426     0.21783     -0.0752263    0.188214     0.0397151   -0.00325376    0.0235745     0.102296     0.127522     0.0406       0.119383    -0.105016    -0.101116      0.0241203    -0.0397462    -0.0818581     0.103625     -0.0346891   -0.0713296\n",
       "39  -0.404717     0.51523    -0.0600107    0.183686      0.221018   -0.206652     0.193208     0.0518085    0.0860608    0.0993146    0.184281     0.119915      0.183787     -0.0144651    0.0625606    0.194186     0.0438976   -0.0864723   -0.103434     -0.0660189     0.0395383    -0.17192      -0.124664     -0.0562911   -0.0622698\n",
       "40  -0.382219     0.460076   -0.00721721   0.274456      0.178183   -0.15559      0.0392943   -0.192688    -0.0570172   -0.018924     0.249434     0.173673      0.24774      -0.0303454   -0.0234577    0.10462     -0.0669826    0.0951716   -0.0959487    -0.0381884     0.0280619    -0.0774621    -0.119755     -0.107472     0.0124292\n",
       "41  -0.45879      0.487671    0.0611153    0.258089     -0.204588    0.0502117   -0.191717    -0.290361    -0.0857082    0.00320559   0.182159     0.0234295     0.179476     -0.0628357    0.0570216   -0.00413269  -0.0993098    0.131087    -0.114125     -0.089546     -0.0858122     0.142482      0.0259806    -0.0911843    0.142891\n",
       "42  -0.398086     0.405562    0.211375     0.228458     -0.263065   -0.0264961   -0.161688    -0.360683     0.00511984   0.125921     0.119027    -0.0531046     0.124729     -0.124337     0.0517177   -0.0977869   -0.0164033    0.0460188    0.0888608    -0.0787942    -0.117056      0.0849103     0.0955933     0.0911453    0.0755932\n",
       "43  -0.422085     0.323221    0.385847     0.23483      -0.105969   -0.0359431   -0.0214781   -0.314116     0.10552      0.171423     0.0334436   -0.0395233    -0.0526765    -0.059088    -0.0336513   -0.248927    -0.128007    -0.109681     0.121699     -0.037254     -0.0105032    -0.0575572     0.0306385     0.0828007   -0.127882\n",
       "44  -0.416485     0.322537    0.488238     0.248569      0.0706005  -0.0651048   -0.00470859  -0.0764226    0.101932     0.175054    -0.143727    -0.0732551    -0.234311     -0.0257689   -0.130894    -0.0733808   -0.198448    -0.221004     0.00344998   -0.00987722   -0.0536141    -0.0616888    -0.149358     -0.0225179   -0.101176\n",
       "45  -0.412167     0.447487    0.49283      0.193481     -0.0652928  -0.128484    -0.031599     0.149958     0.0599612    0.159866    -0.155568    -0.0838437    -0.28337      -0.080616     0.0144549    0.00593287  -0.00216966  -0.0989046   -0.102145     -0.129493     -0.0548102     0.0328361    -0.0807482    -0.0496755    0.0328966\n",
       "46  -0.347409     0.290201    0.312311     0.15703      -0.112707   -0.213257    -0.0570495    0.147256    -0.0654434    0.0408517   -0.145276    -0.0584864    -0.214499     -0.101664     0.0868378    0.0278193    0.130365     0.0907526   -0.128732      0.0190162     0.00234777    0.127346     -0.0253075    -0.11883      0.0279527\n",
       "47  -0.23257      0.2302      0.396357     0.11928      -0.296512   -0.116554    -0.223167     0.153847    -0.0778568   -0.0751867   -0.0595966   -0.0449667    -0.0614869    -0.0670662    0.0827256    0.105664     0.118386     0.065022    -0.0404068     0.0930433     0.123518      0.0716863     0.00144887   -0.0741522    0.104687\n",
       "48  -0.280379     0.339088    0.505202     0.0420528    -0.207603   -0.227027    -0.203382     0.132676    -0.161052    -0.080017     0.0180314    0.0841922     0.0858454     0.0352273   -0.0472065    0.118722    -0.0346024   -0.0259049    0.076821      0.116902      0.000765605  -0.0393376    -0.00773203    0.191989     0.0213816\n",
       "49  -0.184837     0.364099    0.483529     0.10638      -0.227724   -0.221438    -0.162114     0.104892    -0.184249    -0.173265     0.00967874   0.103432      0.161205      0.029607    -0.0542732    0.107181     0.0848501   -0.0710651    0.134008      0.100593      0.0862401    -0.0277131    -0.012514      0.144917    -0.0718649\n",
       "50  -0.133381     0.303758    0.22108      0.101096     -0.164115   -0.159524    -0.169838     0.211702    -0.025212    -0.0831885   -0.00915398   0.105763      0.0118741     0.0776183   -0.207432    -0.0272829   -0.0181436    0.173613    -0.0425022     0.0150761     0.186252      0.0991069     0.101256     -0.0988944   -0.140541\n",
       "51  -0.0178752    0.332663    0.270644    -0.0315156    -0.233036   -0.0365276    0.0117907    0.101062    -0.0154407   -0.0485216    0.0718814    0.00343973   -0.222261      0.130868     0.074878    -0.0110111   -0.00126081   0.195827     0.155548      0.107589     -0.0684158    -0.270034      0.162563     -0.0312158    0.0790244\n",
       "52   0.0316261    0.302346    0.12676     -0.0838875    -0.171844    0.00265494   0.011674     0.0692708    0.0794217    0.0884674    0.224065    -0.0228011    -0.102181      0.377933     0.144166    -0.0338566    0.0682426    0.138446     0.00412334    0.0249986    -0.281892      0.0114344    -0.0142239     0.0976563    0.0116094\n",
       "53  -0.0192499    0.350828   -0.0627735   -0.103538     -0.351017    0.255817     0.11376     -0.00981314   0.217163     0.0939031    0.297216    -0.0899619    -0.0948509     0.334157    -0.0972249   -0.0413279   -0.149585     0.0495397   -0.114966      0.179106      0.209362      0.0156694    -0.252924     -0.0247302    0.024714\n",
       "54   0.0874625    0.413193   -0.06555      0.0519246    -0.268375    0.164382     0.214369     0.104601     0.160268     0.111187    -0.0107866    0.394223     -0.000223557   0.14106     -0.294871    -0.127659    -0.00586242  -0.0774489    0.0159789    -0.139059      0.0255094     0.132366      0.237895     -0.0391529   -0.0386106\n",
       "55   0.0519365    0.458001   -0.137484    -0.165142     -0.0654905   0.00397142   0.141388     0.0483496    0.112556     0.109972    -0.082901     0.350599     -0.109279      0.0369442    0.0718524    0.0700065    0.281421     0.010723    -0.0370559    -0.123879     -0.140621      0.155036     -0.100367      0.0479262    0.00868418\n",
       "56   0.0468802    0.381754   -0.0394721   -0.0472044    -0.172355    0.239845     0.140291     0.0642373    0.130268     0.0950482   -0.131818     0.028392      0.0776434    -0.179074    -0.00670012   0.0455454   -0.135703     0.0810475   -0.401399      0.247774     -0.0405219    -0.0837442     0.138697      0.119163    -0.0830084\n",
       "57  -0.00783937   0.379396   -0.104231    -0.00697851   -0.216043    0.235658     0.185573     0.0455438   -0.00766935  -0.0748894   -0.25319      0.110244      0.100075     -0.182295    -0.238782     0.0993485   -0.0851564    0.0197855    0.155019      0.234674     -0.262495     -0.0524156    -0.124776     -0.222319     0.0624109\n",
       "58  -0.0275799    0.425096   -0.0581614   -0.0427966    -0.228169    0.180634     0.147659     0.118498     0.047503    -0.105119    -0.0614614    0.12986       0.112618     -0.0202249    0.0288382    0.0151282   -0.0564103   -0.140719     0.0857905    -0.0609776     0.0267495     0.0878012    -0.0470719     0.066497     0.148818\n",
       "59   0.0541248    0.480982   -0.0935761   -0.0794136    -0.24574     0.250449     0.144384     0.0121052   -0.0106127   -0.041211    -0.036565     0.107196     -0.0466969    -0.0712421    0.215728     0.243185    -0.132876    -0.0527646    0.235112     -0.135579      0.140901     -0.000236505  -0.0875164     0.0798241   -0.134359\n",
       "60   0.0180399    0.360611   -0.0886718   -0.0657562    -0.124847    0.179075     0.156648     0.0576058    0.037315     0.03823     -0.0698897   -0.0192582    -0.0760322    -0.0677064    0.398459     0.103328    -0.299814     0.0878919   -0.0032924    -0.0767152     0.0937768     0.0220645     0.127003     -0.0400262   -0.0264531\n",
       "─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "Importance of components:\n",
       "───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
       "                                 PC1       PC2        PC3        PC4        PC5        PC6        PC7        PC8        PC9       PC10       PC11       PC12       PC13       PC14       PC15       PC16       PC17       PC18       PC19       PC20        PC21       PC22        PC23        PC24        PC25\n",
       "───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
       "SS Loadings (Eigenvalues)  10.8134    8.94171   4.62771    2.83403    2.28639    1.8743     1.78906    1.41982    1.37805    1.20761    1.00052    0.870766   0.808375   0.742263   0.705705   0.685483   0.634902   0.625405   0.602253   0.517069   0.480987    0.474777   0.439858    0.425538    0.395443\n",
       "Variance explained          0.210484  0.174052  0.090079   0.0551648  0.0445049  0.0364834  0.0348243  0.0276369  0.0268239  0.0235064  0.0194753  0.0169496  0.0157351  0.0144483  0.0137366  0.013343   0.0123584  0.0121736  0.0117229  0.0100648  0.00936248  0.0092416  0.00856189  0.00828315  0.00769736\n",
       "Cumulative variance         0.210484  0.384535  0.474614   0.529779   0.574284   0.610768   0.645592   0.673229   0.700053   0.723559   0.743034   0.759984   0.775719   0.790167   0.803904   0.817247   0.829605   0.841779   0.853502   0.863567   0.872929    0.882171   0.890733    0.899016    0.906713\n",
       "Proportion explained        0.232139  0.191959  0.0993467  0.0608404  0.0490838  0.040237   0.0384072  0.0304803  0.0295837  0.0259248  0.021479   0.0186934  0.017354   0.0159348  0.0151499  0.0147158  0.0136299  0.0134261  0.012929   0.0111003  0.0103257   0.0101924  0.00944278  0.00913535  0.00848929\n",
       "Cumulative proportion       0.232139  0.424098  0.523445   0.584285   0.633369   0.673606   0.712013   0.742494   0.772077   0.798002   0.819481   0.838175   0.855529   0.871463   0.886613   0.901329   0.914959   0.928385   0.941314   0.952414   0.96274     0.972933   0.982375    0.991511    1.0\n",
       "───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = fit(PCA,data',maxoutdim=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5979c75d",
   "metadata": {},
   "source": [
    "We can see that after getting rid of $35$ columns, we still managed to retain $~90\\%$ of the original information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c51912a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#projection matrix of the transformation\n",
    "P = projection(p);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d240b175",
   "metadata": {},
   "source": [
    "Now we use the transform function to transform the data to the new dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "52c6db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = MultivariateStats.transform(p, data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4cfe58d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we transpose again since we had transposed before to apply PCA\n",
    "data_transform';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "14853c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we add the column of labels\n",
    "A = [data_transform' data1[:,61]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "047d8dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>208 rows × 26 columns (omitted printing of 19 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>x1</th><th>x2</th><th>x3</th><th>x4</th><th>x5</th><th>x6</th><th>x7</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>2.32433</td><td>-1.29447</td><td>-0.989159</td><td>1.40737</td><td>0.65821</td><td>0.546536</td><td>-2.70052</td></tr><tr><th>2</th><td>-6.21793</td><td>-4.50146</td><td>-0.0313274</td><td>-4.04019</td><td>2.93452</td><td>-0.685373</td><td>0.496122</td></tr><tr><th>3</th><td>-2.28185</td><td>-5.42039</td><td>0.591383</td><td>-2.05963</td><td>2.27768</td><td>-2.57987</td><td>-0.268366</td></tr><tr><th>4</th><td>4.0393</td><td>-1.81448</td><td>-0.80612</td><td>1.21011</td><td>-2.05167</td><td>2.61708</td><td>1.52967</td></tr><tr><th>5</th><td>-1.55175</td><td>-0.952711</td><td>-0.0203731</td><td>-3.58624</td><td>2.23203</td><td>0.00912788</td><td>-0.738891</td></tr><tr><th>6</th><td>-2.38318</td><td>-1.25401</td><td>-1.35613</td><td>-2.31626</td><td>2.80004</td><td>1.25014</td><td>2.0785</td></tr><tr><th>7</th><td>-0.901444</td><td>-3.16973</td><td>-0.905512</td><td>0.986358</td><td>-1.84549</td><td>-1.24892</td><td>-1.28336</td></tr><tr><th>8</th><td>-3.00601</td><td>0.183992</td><td>-0.466662</td><td>0.39736</td><td>-0.505127</td><td>1.79756</td><td>-2.35194</td></tr><tr><th>9</th><td>-1.19713</td><td>0.903343</td><td>-0.757028</td><td>0.819366</td><td>-1.43894</td><td>2.47346</td><td>-0.0262408</td></tr><tr><th>10</th><td>-0.407971</td><td>5.00032</td><td>-0.49162</td><td>-0.445078</td><td>-2.31082</td><td>-0.868407</td><td>-0.392271</td></tr><tr><th>11</th><td>4.76302</td><td>3.93428</td><td>-2.39693</td><td>-1.36115</td><td>0.86535</td><td>-0.628754</td><td>0.3973</td></tr><tr><th>12</th><td>-0.251739</td><td>2.35749</td><td>-2.37897</td><td>-0.0666843</td><td>-1.50924</td><td>1.37047</td><td>-1.42986</td></tr><tr><th>13</th><td>-3.66282</td><td>1.71054</td><td>-3.14136</td><td>1.48383</td><td>-1.45635</td><td>2.07498</td><td>0.676893</td></tr><tr><th>14</th><td>1.05706</td><td>-0.744529</td><td>-0.876632</td><td>-0.377064</td><td>0.116217</td><td>-1.73971</td><td>-1.74484</td></tr><tr><th>15</th><td>0.321653</td><td>-0.478661</td><td>-2.46761</td><td>0.543321</td><td>-0.645402</td><td>1.79911</td><td>-2.23386</td></tr><tr><th>16</th><td>0.998572</td><td>-2.73791</td><td>-1.5674</td><td>-0.542391</td><td>1.19895</td><td>-3.49693</td><td>-0.898472</td></tr><tr><th>17</th><td>4.10274</td><td>-2.6777</td><td>0.215356</td><td>0.441585</td><td>-0.416714</td><td>-0.0505573</td><td>0.584057</td></tr><tr><th>18</th><td>-0.868197</td><td>-3.17976</td><td>-2.71549</td><td>2.51305</td><td>-1.46832</td><td>1.14831</td><td>0.872867</td></tr><tr><th>19</th><td>2.37274</td><td>2.13498</td><td>-1.10513</td><td>0.364807</td><td>1.6839</td><td>-1.95071</td><td>0.691587</td></tr><tr><th>20</th><td>-7.09732</td><td>-3.32112</td><td>0.804132</td><td>-1.17159</td><td>3.01013</td><td>0.776592</td><td>-1.79196</td></tr><tr><th>21</th><td>0.678815</td><td>-3.36986</td><td>-0.764385</td><td>1.63204</td><td>3.60669</td><td>-1.14593</td><td>0.0869551</td></tr><tr><th>22</th><td>4.38225</td><td>-5.02433</td><td>-1.23436</td><td>-0.360916</td><td>1.85134</td><td>0.295121</td><td>2.35571</td></tr><tr><th>23</th><td>5.31635</td><td>-4.49719</td><td>-1.48498</td><td>-1.65669</td><td>-0.368586</td><td>0.524175</td><td>-0.495589</td></tr><tr><th>24</th><td>4.02011</td><td>2.20781</td><td>-1.03727</td><td>-0.969331</td><td>1.13373</td><td>-0.154631</td><td>2.10118</td></tr><tr><th>25</th><td>5.46166</td><td>-0.86587</td><td>-3.86234</td><td>-0.429597</td><td>-0.366336</td><td>-0.911635</td><td>2.0366</td></tr><tr><th>26</th><td>5.77955</td><td>3.78868</td><td>-0.41197</td><td>1.31197</td><td>0.953718</td><td>-1.34443</td><td>2.17585</td></tr><tr><th>27</th><td>-4.03255</td><td>1.05401</td><td>-1.86609</td><td>0.601357</td><td>-0.365068</td><td>0.459425</td><td>0.522591</td></tr><tr><th>28</th><td>-1.72368</td><td>2.16292</td><td>0.343966</td><td>-1.35125</td><td>-0.863455</td><td>-0.959135</td><td>1.6193</td></tr><tr><th>29</th><td>-2.35208</td><td>-0.111245</td><td>-2.33969</td><td>1.59061</td><td>0.527445</td><td>2.61007</td><td>0.458849</td></tr><tr><th>30</th><td>-2.25466</td><td>0.820172</td><td>-1.08928</td><td>-3.12001</td><td>-0.136379</td><td>0.760911</td><td>-1.4063</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& x1 & x2 & x3 & x4 & x5 & x6 & x7 & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 2.32433 & -1.29447 & -0.989159 & 1.40737 & 0.65821 & 0.546536 & -2.70052 & $\\dots$ \\\\\n",
       "\t2 & -6.21793 & -4.50146 & -0.0313274 & -4.04019 & 2.93452 & -0.685373 & 0.496122 & $\\dots$ \\\\\n",
       "\t3 & -2.28185 & -5.42039 & 0.591383 & -2.05963 & 2.27768 & -2.57987 & -0.268366 & $\\dots$ \\\\\n",
       "\t4 & 4.0393 & -1.81448 & -0.80612 & 1.21011 & -2.05167 & 2.61708 & 1.52967 & $\\dots$ \\\\\n",
       "\t5 & -1.55175 & -0.952711 & -0.0203731 & -3.58624 & 2.23203 & 0.00912788 & -0.738891 & $\\dots$ \\\\\n",
       "\t6 & -2.38318 & -1.25401 & -1.35613 & -2.31626 & 2.80004 & 1.25014 & 2.0785 & $\\dots$ \\\\\n",
       "\t7 & -0.901444 & -3.16973 & -0.905512 & 0.986358 & -1.84549 & -1.24892 & -1.28336 & $\\dots$ \\\\\n",
       "\t8 & -3.00601 & 0.183992 & -0.466662 & 0.39736 & -0.505127 & 1.79756 & -2.35194 & $\\dots$ \\\\\n",
       "\t9 & -1.19713 & 0.903343 & -0.757028 & 0.819366 & -1.43894 & 2.47346 & -0.0262408 & $\\dots$ \\\\\n",
       "\t10 & -0.407971 & 5.00032 & -0.49162 & -0.445078 & -2.31082 & -0.868407 & -0.392271 & $\\dots$ \\\\\n",
       "\t11 & 4.76302 & 3.93428 & -2.39693 & -1.36115 & 0.86535 & -0.628754 & 0.3973 & $\\dots$ \\\\\n",
       "\t12 & -0.251739 & 2.35749 & -2.37897 & -0.0666843 & -1.50924 & 1.37047 & -1.42986 & $\\dots$ \\\\\n",
       "\t13 & -3.66282 & 1.71054 & -3.14136 & 1.48383 & -1.45635 & 2.07498 & 0.676893 & $\\dots$ \\\\\n",
       "\t14 & 1.05706 & -0.744529 & -0.876632 & -0.377064 & 0.116217 & -1.73971 & -1.74484 & $\\dots$ \\\\\n",
       "\t15 & 0.321653 & -0.478661 & -2.46761 & 0.543321 & -0.645402 & 1.79911 & -2.23386 & $\\dots$ \\\\\n",
       "\t16 & 0.998572 & -2.73791 & -1.5674 & -0.542391 & 1.19895 & -3.49693 & -0.898472 & $\\dots$ \\\\\n",
       "\t17 & 4.10274 & -2.6777 & 0.215356 & 0.441585 & -0.416714 & -0.0505573 & 0.584057 & $\\dots$ \\\\\n",
       "\t18 & -0.868197 & -3.17976 & -2.71549 & 2.51305 & -1.46832 & 1.14831 & 0.872867 & $\\dots$ \\\\\n",
       "\t19 & 2.37274 & 2.13498 & -1.10513 & 0.364807 & 1.6839 & -1.95071 & 0.691587 & $\\dots$ \\\\\n",
       "\t20 & -7.09732 & -3.32112 & 0.804132 & -1.17159 & 3.01013 & 0.776592 & -1.79196 & $\\dots$ \\\\\n",
       "\t21 & 0.678815 & -3.36986 & -0.764385 & 1.63204 & 3.60669 & -1.14593 & 0.0869551 & $\\dots$ \\\\\n",
       "\t22 & 4.38225 & -5.02433 & -1.23436 & -0.360916 & 1.85134 & 0.295121 & 2.35571 & $\\dots$ \\\\\n",
       "\t23 & 5.31635 & -4.49719 & -1.48498 & -1.65669 & -0.368586 & 0.524175 & -0.495589 & $\\dots$ \\\\\n",
       "\t24 & 4.02011 & 2.20781 & -1.03727 & -0.969331 & 1.13373 & -0.154631 & 2.10118 & $\\dots$ \\\\\n",
       "\t25 & 5.46166 & -0.86587 & -3.86234 & -0.429597 & -0.366336 & -0.911635 & 2.0366 & $\\dots$ \\\\\n",
       "\t26 & 5.77955 & 3.78868 & -0.41197 & 1.31197 & 0.953718 & -1.34443 & 2.17585 & $\\dots$ \\\\\n",
       "\t27 & -4.03255 & 1.05401 & -1.86609 & 0.601357 & -0.365068 & 0.459425 & 0.522591 & $\\dots$ \\\\\n",
       "\t28 & -1.72368 & 2.16292 & 0.343966 & -1.35125 & -0.863455 & -0.959135 & 1.6193 & $\\dots$ \\\\\n",
       "\t29 & -2.35208 & -0.111245 & -2.33969 & 1.59061 & 0.527445 & 2.61007 & 0.458849 & $\\dots$ \\\\\n",
       "\t30 & -2.25466 & 0.820172 & -1.08928 & -3.12001 & -0.136379 & 0.760911 & -1.4063 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m208×26 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m x1        \u001b[0m\u001b[1m x2        \u001b[0m\u001b[1m x3         \u001b[0m\u001b[1m x4         \u001b[0m\u001b[1m x5         \u001b[0m\u001b[1m x6          \u001b[0m\u001b[1m \u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m \u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │  2.32433   -1.29447   -0.989159    1.40737     0.65821     0.546536     ⋯\n",
       "   2 │ -6.21793   -4.50146   -0.0313274  -4.04019     2.93452    -0.685373\n",
       "   3 │ -2.28185   -5.42039    0.591383   -2.05963     2.27768    -2.57987\n",
       "   4 │  4.0393    -1.81448   -0.80612     1.21011    -2.05167     2.61708\n",
       "   5 │ -1.55175   -0.952711  -0.0203731  -3.58624     2.23203     0.00912788   ⋯\n",
       "   6 │ -2.38318   -1.25401   -1.35613    -2.31626     2.80004     1.25014\n",
       "   7 │ -0.901444  -3.16973   -0.905512    0.986358   -1.84549    -1.24892\n",
       "   8 │ -3.00601    0.183992  -0.466662    0.39736    -0.505127    1.79756\n",
       "   9 │ -1.19713    0.903343  -0.757028    0.819366   -1.43894     2.47346      ⋯\n",
       "  10 │ -0.407971   5.00032   -0.49162    -0.445078   -2.31082    -0.868407\n",
       "  11 │  4.76302    3.93428   -2.39693    -1.36115     0.86535    -0.628754\n",
       "  ⋮  │     ⋮          ⋮          ⋮           ⋮           ⋮            ⋮        ⋱\n",
       " 199 │ -1.1409     3.87731    2.28685    -0.295289   -0.585343    1.06373\n",
       " 200 │ -1.68593    3.83466    1.59167    -0.332821    0.0801284   0.429526     ⋯\n",
       " 201 │ -2.14181    2.96207    3.30831     0.0863791   0.449266    0.722704\n",
       " 202 │ -2.28015    3.29377    2.84355    -0.609393    0.133314    1.02176\n",
       " 203 │ -0.195082   2.47236    2.59916    -2.40871     0.706896   -0.892192\n",
       " 204 │ -0.080234   2.17526    2.77579    -1.72889    -1.57226    -1.5912       ⋯\n",
       " 205 │  0.715998   4.24788    2.06547    -1.70396    -0.0660816  -1.39252\n",
       " 206 │  1.07656    3.88276    1.75825    -2.00728    -0.731419   -1.58874\n",
       " 207 │ -0.211471   3.9162     1.81923    -1.16097    -0.443833    0.0587462\n",
       " 208 │  0.021832   4.15227    0.933204   -0.570063   -0.555162   -0.418483     ⋯\n",
       "\u001b[36m                                                 20 columns and 187 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#and we transform again into a dataframe\n",
    "df = DataFrame(A, :auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8debbd",
   "metadata": {},
   "source": [
    "# Splitting the data\n",
    "\n",
    "The next step is to divide the data into two sets, namely Training and Testing sets; we adjust the models to the training data and then study the performance of the model on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9d2ad147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], \u001b[1m208×25 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m x1        \u001b[0m\u001b[1m x2        \u001b[0m\u001b[1m x3         \u001b[0m\u001b[1m x4         \u001b[0m\u001b[1m x5         \u001b[0m\u001b[1m x6          \u001b[0m\u001b[1m \u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m \u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │  2.32433   -1.29447   -0.989159    1.40737     0.65821     0.546536     ⋯\n",
       "   2 │ -6.21793   -4.50146   -0.0313274  -4.04019     2.93452    -0.685373\n",
       "   3 │ -2.28185   -5.42039    0.591383   -2.05963     2.27768    -2.57987\n",
       "   4 │  4.0393    -1.81448   -0.80612     1.21011    -2.05167     2.61708\n",
       "   5 │ -1.55175   -0.952711  -0.0203731  -3.58624     2.23203     0.00912788   ⋯\n",
       "   6 │ -2.38318   -1.25401   -1.35613    -2.31626     2.80004     1.25014\n",
       "   7 │ -0.901444  -3.16973   -0.905512    0.986358   -1.84549    -1.24892\n",
       "   8 │ -3.00601    0.183992  -0.466662    0.39736    -0.505127    1.79756\n",
       "   9 │ -1.19713    0.903343  -0.757028    0.819366   -1.43894     2.47346      ⋯\n",
       "  10 │ -0.407971   5.00032   -0.49162    -0.445078   -2.31082    -0.868407\n",
       "  11 │  4.76302    3.93428   -2.39693    -1.36115     0.86535    -0.628754\n",
       "  ⋮  │     ⋮          ⋮          ⋮           ⋮           ⋮            ⋮        ⋱\n",
       " 199 │ -1.1409     3.87731    2.28685    -0.295289   -0.585343    1.06373\n",
       " 200 │ -1.68593    3.83466    1.59167    -0.332821    0.0801284   0.429526     ⋯\n",
       " 201 │ -2.14181    2.96207    3.30831     0.0863791   0.449266    0.722704\n",
       " 202 │ -2.28015    3.29377    2.84355    -0.609393    0.133314    1.02176\n",
       " 203 │ -0.195082   2.47236    2.59916    -2.40871     0.706896   -0.892192\n",
       " 204 │ -0.080234   2.17526    2.77579    -1.72889    -1.57226    -1.5912       ⋯\n",
       " 205 │  0.715998   4.24788    2.06547    -1.70396    -0.0660816  -1.39252\n",
       " 206 │  1.07656    3.88276    1.75825    -2.00728    -0.731419   -1.58874\n",
       " 207 │ -0.211471   3.9162     1.81923    -1.16097    -0.443833    0.0587462\n",
       " 208 │  0.021832   4.15227    0.933204   -0.570063   -0.555162   -0.418483     ⋯\n",
       "\u001b[36m                                                 19 columns and 187 rows omitted\u001b[0m)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using StableRNGs\n",
    "using MLJ\n",
    "\n",
    "y, X = unpack(df, ==(:x26)) #split characteristics and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "64431520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rocks indices\n",
    "rocks_idx = findall(==(0),y)\n",
    "# Get mines indices\n",
    "mines_idx = findall(==(1),y)\n",
    "\n",
    "# Set a random seed\n",
    "rng = StableRNG(0)\n",
    "# We take 70% of mine indices \n",
    "train_mines , test_mines = partition( mines_idx , 0.7, shuffle=true, rng=rng)\n",
    "# Also 70% of rocks indices \n",
    "train_rocks , test_rocks = partition( rocks_idx , 0.7, shuffle=true, rng=rng)\n",
    "\n",
    "# join mines and rocks (indices) for each set\n",
    "train = [train_mines ; train_rocks]\n",
    "test = [test_mines ; test_rocks];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "09e26d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>146 rows × 26 columns (omitted printing of 19 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>x1</th><th>x2</th><th>x3</th><th>x4</th><th>x5</th><th>x6</th><th>x7</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>0.633641</td><td>3.2539</td><td>1.29587</td><td>-2.32477</td><td>-0.446616</td><td>0.754128</td><td>-0.0382978</td></tr><tr><th>2</th><td>-3.62409</td><td>-1.12001</td><td>-1.43103</td><td>2.00521</td><td>0.00899368</td><td>0.330671</td><td>0.0881543</td></tr><tr><th>3</th><td>1.56841</td><td>0.354458</td><td>0.30547</td><td>-0.998069</td><td>2.36949</td><td>0.249138</td><td>-0.0492682</td></tr><tr><th>4</th><td>0.0218187</td><td>-3.06259</td><td>2.04794</td><td>-0.544933</td><td>-5.11363</td><td>-3.47219</td><td>-1.97489</td></tr><tr><th>5</th><td>-4.51129</td><td>-0.222258</td><td>1.71048</td><td>-0.704169</td><td>-0.120704</td><td>-1.85756</td><td>-1.5639</td></tr><tr><th>6</th><td>-2.95275</td><td>0.648978</td><td>-2.24889</td><td>0.362719</td><td>0.547628</td><td>0.73992</td><td>3.40731</td></tr><tr><th>7</th><td>-1.77587</td><td>0.267366</td><td>0.00617518</td><td>3.29647</td><td>-3.379</td><td>-0.454783</td><td>0.288406</td></tr><tr><th>8</th><td>7.49687</td><td>-1.93111</td><td>3.86</td><td>1.06463</td><td>0.73247</td><td>0.636394</td><td>1.55107</td></tr><tr><th>9</th><td>-0.080234</td><td>2.17526</td><td>2.77579</td><td>-1.72889</td><td>-1.57226</td><td>-1.5912</td><td>-1.59815</td></tr><tr><th>10</th><td>-2.20029</td><td>1.13747</td><td>-0.187079</td><td>-0.740668</td><td>0.54066</td><td>1.12691</td><td>-1.87431</td></tr><tr><th>11</th><td>-1.09299</td><td>-1.63013</td><td>-2.39579</td><td>2.47584</td><td>-1.66294</td><td>0.15286</td><td>0.906892</td></tr><tr><th>12</th><td>2.81671</td><td>-2.19741</td><td>6.18079</td><td>1.14791</td><td>-0.667498</td><td>2.52958</td><td>1.38946</td></tr><tr><th>13</th><td>-6.8145</td><td>-3.2487</td><td>1.69477</td><td>-1.71105</td><td>1.5327</td><td>-0.360098</td><td>0.524402</td></tr><tr><th>14</th><td>-4.04737</td><td>0.781474</td><td>-0.752807</td><td>0.535135</td><td>0.0451397</td><td>-0.215749</td><td>2.82647</td></tr><tr><th>15</th><td>-1.68593</td><td>3.83466</td><td>1.59167</td><td>-0.332821</td><td>0.0801284</td><td>0.429526</td><td>-1.08563</td></tr><tr><th>16</th><td>-0.198966</td><td>-0.9553</td><td>5.23654</td><td>0.362668</td><td>2.23845</td><td>2.00697</td><td>-0.843195</td></tr><tr><th>17</th><td>0.952814</td><td>1.17277</td><td>1.49512</td><td>-1.15505</td><td>0.54867</td><td>-1.52467</td><td>0.591973</td></tr><tr><th>18</th><td>-0.211471</td><td>3.9162</td><td>1.81923</td><td>-1.16097</td><td>-0.443833</td><td>0.0587462</td><td>-0.974524</td></tr><tr><th>19</th><td>-0.744045</td><td>-0.43468</td><td>4.75069</td><td>-0.107903</td><td>2.3853</td><td>1.73625</td><td>-1.31358</td></tr><tr><th>20</th><td>-7.07328</td><td>-5.63014</td><td>-0.441783</td><td>-2.62693</td><td>-0.172368</td><td>0.512215</td><td>0.761887</td></tr><tr><th>21</th><td>-4.16403</td><td>1.16253</td><td>2.05345</td><td>0.84936</td><td>-0.332438</td><td>-1.36781</td><td>-0.0722263</td></tr><tr><th>22</th><td>1.69848</td><td>2.23758</td><td>-0.358831</td><td>3.61204</td><td>2.00237</td><td>0.77596</td><td>-1.93702</td></tr><tr><th>23</th><td>2.01607</td><td>2.0688</td><td>1.91951</td><td>-2.19764</td><td>-0.326676</td><td>0.97028</td><td>0.519828</td></tr><tr><th>24</th><td>-4.51245</td><td>-5.39713</td><td>-2.83699</td><td>-2.88933</td><td>-0.677632</td><td>0.697478</td><td>1.2124</td></tr><tr><th>25</th><td>0.633173</td><td>0.56733</td><td>-0.278975</td><td>3.02816</td><td>1.90217</td><td>-1.01085</td><td>2.52367</td></tr><tr><th>26</th><td>-2.93163</td><td>0.831103</td><td>1.88136</td><td>1.77695</td><td>-0.476651</td><td>0.0567922</td><td>-0.288882</td></tr><tr><th>27</th><td>5.88609</td><td>-0.0512059</td><td>3.37232</td><td>-0.221589</td><td>1.18334</td><td>-0.717169</td><td>0.373214</td></tr><tr><th>28</th><td>1.1998</td><td>-1.42542</td><td>5.09475</td><td>0.258232</td><td>1.59503</td><td>2.32953</td><td>0.373351</td></tr><tr><th>29</th><td>-2.96542</td><td>1.27183</td><td>-3.23104</td><td>0.858683</td><td>0.604943</td><td>1.629</td><td>1.03003</td></tr><tr><th>30</th><td>-1.862</td><td>3.95086</td><td>2.93277</td><td>0.0576421</td><td>-0.188659</td><td>1.16153</td><td>-0.305613</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& x1 & x2 & x3 & x4 & x5 & x6 & x7 & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 0.633641 & 3.2539 & 1.29587 & -2.32477 & -0.446616 & 0.754128 & -0.0382978 & $\\dots$ \\\\\n",
       "\t2 & -3.62409 & -1.12001 & -1.43103 & 2.00521 & 0.00899368 & 0.330671 & 0.0881543 & $\\dots$ \\\\\n",
       "\t3 & 1.56841 & 0.354458 & 0.30547 & -0.998069 & 2.36949 & 0.249138 & -0.0492682 & $\\dots$ \\\\\n",
       "\t4 & 0.0218187 & -3.06259 & 2.04794 & -0.544933 & -5.11363 & -3.47219 & -1.97489 & $\\dots$ \\\\\n",
       "\t5 & -4.51129 & -0.222258 & 1.71048 & -0.704169 & -0.120704 & -1.85756 & -1.5639 & $\\dots$ \\\\\n",
       "\t6 & -2.95275 & 0.648978 & -2.24889 & 0.362719 & 0.547628 & 0.73992 & 3.40731 & $\\dots$ \\\\\n",
       "\t7 & -1.77587 & 0.267366 & 0.00617518 & 3.29647 & -3.379 & -0.454783 & 0.288406 & $\\dots$ \\\\\n",
       "\t8 & 7.49687 & -1.93111 & 3.86 & 1.06463 & 0.73247 & 0.636394 & 1.55107 & $\\dots$ \\\\\n",
       "\t9 & -0.080234 & 2.17526 & 2.77579 & -1.72889 & -1.57226 & -1.5912 & -1.59815 & $\\dots$ \\\\\n",
       "\t10 & -2.20029 & 1.13747 & -0.187079 & -0.740668 & 0.54066 & 1.12691 & -1.87431 & $\\dots$ \\\\\n",
       "\t11 & -1.09299 & -1.63013 & -2.39579 & 2.47584 & -1.66294 & 0.15286 & 0.906892 & $\\dots$ \\\\\n",
       "\t12 & 2.81671 & -2.19741 & 6.18079 & 1.14791 & -0.667498 & 2.52958 & 1.38946 & $\\dots$ \\\\\n",
       "\t13 & -6.8145 & -3.2487 & 1.69477 & -1.71105 & 1.5327 & -0.360098 & 0.524402 & $\\dots$ \\\\\n",
       "\t14 & -4.04737 & 0.781474 & -0.752807 & 0.535135 & 0.0451397 & -0.215749 & 2.82647 & $\\dots$ \\\\\n",
       "\t15 & -1.68593 & 3.83466 & 1.59167 & -0.332821 & 0.0801284 & 0.429526 & -1.08563 & $\\dots$ \\\\\n",
       "\t16 & -0.198966 & -0.9553 & 5.23654 & 0.362668 & 2.23845 & 2.00697 & -0.843195 & $\\dots$ \\\\\n",
       "\t17 & 0.952814 & 1.17277 & 1.49512 & -1.15505 & 0.54867 & -1.52467 & 0.591973 & $\\dots$ \\\\\n",
       "\t18 & -0.211471 & 3.9162 & 1.81923 & -1.16097 & -0.443833 & 0.0587462 & -0.974524 & $\\dots$ \\\\\n",
       "\t19 & -0.744045 & -0.43468 & 4.75069 & -0.107903 & 2.3853 & 1.73625 & -1.31358 & $\\dots$ \\\\\n",
       "\t20 & -7.07328 & -5.63014 & -0.441783 & -2.62693 & -0.172368 & 0.512215 & 0.761887 & $\\dots$ \\\\\n",
       "\t21 & -4.16403 & 1.16253 & 2.05345 & 0.84936 & -0.332438 & -1.36781 & -0.0722263 & $\\dots$ \\\\\n",
       "\t22 & 1.69848 & 2.23758 & -0.358831 & 3.61204 & 2.00237 & 0.77596 & -1.93702 & $\\dots$ \\\\\n",
       "\t23 & 2.01607 & 2.0688 & 1.91951 & -2.19764 & -0.326676 & 0.97028 & 0.519828 & $\\dots$ \\\\\n",
       "\t24 & -4.51245 & -5.39713 & -2.83699 & -2.88933 & -0.677632 & 0.697478 & 1.2124 & $\\dots$ \\\\\n",
       "\t25 & 0.633173 & 0.56733 & -0.278975 & 3.02816 & 1.90217 & -1.01085 & 2.52367 & $\\dots$ \\\\\n",
       "\t26 & -2.93163 & 0.831103 & 1.88136 & 1.77695 & -0.476651 & 0.0567922 & -0.288882 & $\\dots$ \\\\\n",
       "\t27 & 5.88609 & -0.0512059 & 3.37232 & -0.221589 & 1.18334 & -0.717169 & 0.373214 & $\\dots$ \\\\\n",
       "\t28 & 1.1998 & -1.42542 & 5.09475 & 0.258232 & 1.59503 & 2.32953 & 0.373351 & $\\dots$ \\\\\n",
       "\t29 & -2.96542 & 1.27183 & -3.23104 & 0.858683 & 0.604943 & 1.629 & 1.03003 & $\\dots$ \\\\\n",
       "\t30 & -1.862 & 3.95086 & 2.93277 & 0.0576421 & -0.188659 & 1.16153 & -0.305613 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m146×26 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m x1         \u001b[0m\u001b[1m x2         \u001b[0m\u001b[1m x3          \u001b[0m\u001b[1m x4        \u001b[0m\u001b[1m x5          \u001b[0m\u001b[1m x6        \u001b[0m\u001b[1m\u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64    \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │  0.633641    3.2539      1.29587     -2.32477   -0.446616     0.754128  ⋯\n",
       "   2 │ -3.62409    -1.12001    -1.43103      2.00521    0.00899368   0.330671\n",
       "   3 │  1.56841     0.354458    0.30547     -0.998069   2.36949      0.249138\n",
       "   4 │  0.0218187  -3.06259     2.04794     -0.544933  -5.11363     -3.47219\n",
       "   5 │ -4.51129    -0.222258    1.71048     -0.704169  -0.120704    -1.85756   ⋯\n",
       "   6 │ -2.95275     0.648978   -2.24889      0.362719   0.547628     0.73992\n",
       "   7 │ -1.77587     0.267366    0.00617518   3.29647   -3.379       -0.454783\n",
       "   8 │  7.49687    -1.93111     3.86         1.06463    0.73247      0.636394\n",
       "   9 │ -0.080234    2.17526     2.77579     -1.72889   -1.57226     -1.5912    ⋯\n",
       "  10 │ -2.20029     1.13747    -0.187079    -0.740668   0.54066      1.12691\n",
       "  11 │ -1.09299    -1.63013    -2.39579      2.47584   -1.66294      0.15286\n",
       "  ⋮  │     ⋮           ⋮            ⋮           ⋮           ⋮           ⋮      ⋱\n",
       " 137 │ -4.48041    -0.0858475  -0.8322      -1.31392    1.7253      -0.130849\n",
       " 138 │ -2.35208    -0.111245   -2.33969      1.59061    0.527445     2.61007   ⋯\n",
       " 139 │ -2.21344    -2.54396    -1.47268      3.2006     1.92731     -0.282603\n",
       " 140 │  1.05706    -0.744529   -0.876632    -0.377064   0.116217    -1.73971\n",
       " 141 │  5.31635    -4.49719    -1.48498     -1.65669   -0.368586     0.524175\n",
       " 142 │ -1.37214     2.79173    -2.21689     -1.37789   -0.691184     0.177893  ⋯\n",
       " 143 │  0.32505     4.23699    -1.57578     -1.56009   -2.13666      1.88952\n",
       " 144 │  0.877846    4.07244     0.685756    -0.219544   0.200625     0.748529\n",
       " 145 │  4.76447    -4.63458     0.0077125    1.36325   -3.15929     -0.484717\n",
       " 146 │ -2.25466     0.820172   -1.08928     -3.12001   -0.136379     0.760911  ⋯\n",
       "\u001b[36m                                                 20 columns and 125 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the train set as a dataframe\n",
    "Train = DataFrame(A[train,:], :auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "badddfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>62 rows × 26 columns (omitted printing of 19 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>x1</th><th>x2</th><th>x3</th><th>x4</th><th>x5</th><th>x6</th><th>x7</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>2.67422</td><td>-2.58367</td><td>1.84918</td><td>2.94977</td><td>2.25763</td><td>0.158967</td><td>3.19904</td></tr><tr><th>2</th><td>2.22217</td><td>-3.54915</td><td>3.27733</td><td>-1.42662</td><td>-2.76808</td><td>-1.50555</td><td>0.455601</td></tr><tr><th>3</th><td>-4.80093</td><td>-1.11237</td><td>1.64456</td><td>0.43256</td><td>0.150882</td><td>-1.96967</td><td>-1.00953</td></tr><tr><th>4</th><td>-0.9105</td><td>3.6121</td><td>3.04094</td><td>-0.193027</td><td>-0.503773</td><td>1.19834</td><td>0.0913277</td></tr><tr><th>5</th><td>-2.0373</td><td>-1.95942</td><td>-1.89185</td><td>2.83901</td><td>-0.57782</td><td>-1.20167</td><td>0.610392</td></tr><tr><th>6</th><td>-3.51161</td><td>-2.6956</td><td>-1.02456</td><td>2.84096</td><td>-2.98448</td><td>-1.10298</td><td>-2.11035</td></tr><tr><th>7</th><td>-0.195082</td><td>2.47236</td><td>2.59916</td><td>-2.40871</td><td>0.706896</td><td>-0.892192</td><td>-1.2048</td></tr><tr><th>8</th><td>-0.797876</td><td>1.84563</td><td>-0.726482</td><td>2.81892</td><td>0.720546</td><td>0.830249</td><td>0.397184</td></tr><tr><th>9</th><td>-4.86489</td><td>-4.25104</td><td>-1.29747</td><td>-4.83094</td><td>1.45364</td><td>0.540341</td><td>0.0524396</td></tr><tr><th>10</th><td>6.15691</td><td>-1.93771</td><td>2.08371</td><td>-0.372279</td><td>2.47822</td><td>0.316091</td><td>0.610564</td></tr><tr><th>11</th><td>1.50171</td><td>2.08525</td><td>1.65071</td><td>1.49292</td><td>-0.696798</td><td>1.20394</td><td>1.3742</td></tr><tr><th>12</th><td>-2.7647</td><td>-1.72891</td><td>0.788022</td><td>-3.02694</td><td>-1.44399</td><td>1.14553</td><td>-1.12226</td></tr><tr><th>13</th><td>4.36994</td><td>-3.40533</td><td>3.23551</td><td>0.373285</td><td>-2.6191</td><td>-1.73785</td><td>1.13972</td></tr><tr><th>14</th><td>1.7031</td><td>-7.0001</td><td>-1.61518</td><td>-0.286012</td><td>-0.534901</td><td>2.0245</td><td>-2.39803</td></tr><tr><th>15</th><td>3.99259</td><td>0.981788</td><td>-0.233576</td><td>2.48452</td><td>-0.618901</td><td>4.17176</td><td>-1.01331</td></tr><tr><th>16</th><td>-1.23352</td><td>0.0710929</td><td>-0.364712</td><td>1.09898</td><td>-3.22086</td><td>2.25045</td><td>-0.335665</td></tr><tr><th>17</th><td>0.344227</td><td>-3.93579</td><td>4.107</td><td>1.53589</td><td>-1.66505</td><td>-0.87872</td><td>1.70189</td></tr><tr><th>18</th><td>-3.91414</td><td>1.94747</td><td>-0.041728</td><td>0.0531202</td><td>0.876757</td><td>0.531566</td><td>1.9668</td></tr><tr><th>19</th><td>3.43669</td><td>-7.42354</td><td>-0.104201</td><td>0.0271859</td><td>1.34692</td><td>-0.450634</td><td>-3.08143</td></tr><tr><th>20</th><td>-4.95677</td><td>-2.69303</td><td>-4.23915</td><td>-0.618086</td><td>1.86556</td><td>0.855444</td><td>2.04613</td></tr><tr><th>21</th><td>1.33936</td><td>2.45737</td><td>1.43216</td><td>-2.19311</td><td>-0.587342</td><td>0.600994</td><td>0.399861</td></tr><tr><th>22</th><td>-3.30661</td><td>0.905751</td><td>1.62176</td><td>0.52339</td><td>-1.37562</td><td>-1.08017</td><td>-0.0834587</td></tr><tr><th>23</th><td>-1.06801</td><td>4.04239</td><td>2.14627</td><td>0.219342</td><td>-0.372899</td><td>1.47997</td><td>-0.143439</td></tr><tr><th>24</th><td>2.2324</td><td>-4.16226</td><td>1.89727</td><td>1.13633</td><td>-2.51106</td><td>-0.436396</td><td>-2.14677</td></tr><tr><th>25</th><td>-0.251205</td><td>2.75158</td><td>0.416979</td><td>0.75235</td><td>0.994299</td><td>-0.0057112</td><td>-2.15724</td></tr><tr><th>26</th><td>-4.9656</td><td>-1.48232</td><td>-2.52856</td><td>0.229238</td><td>0.588992</td><td>-0.447228</td><td>0.472838</td></tr><tr><th>27</th><td>1.77511</td><td>3.37361</td><td>1.17972</td><td>-1.84829</td><td>-0.49384</td><td>0.873874</td><td>0.361035</td></tr><tr><th>28</th><td>-1.22507</td><td>-0.780639</td><td>2.66728</td><td>1.87134</td><td>0.0966342</td><td>1.26229</td><td>-1.88422</td></tr><tr><th>29</th><td>2.85916</td><td>-1.77317</td><td>4.86359</td><td>-1.17662</td><td>-0.292665</td><td>1.103</td><td>2.09684</td></tr><tr><th>30</th><td>1.07933</td><td>3.25281</td><td>-0.0256047</td><td>-1.54861</td><td>0.305284</td><td>-0.13893</td><td>0.0801751</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& x1 & x2 & x3 & x4 & x5 & x6 & x7 & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 2.67422 & -2.58367 & 1.84918 & 2.94977 & 2.25763 & 0.158967 & 3.19904 & $\\dots$ \\\\\n",
       "\t2 & 2.22217 & -3.54915 & 3.27733 & -1.42662 & -2.76808 & -1.50555 & 0.455601 & $\\dots$ \\\\\n",
       "\t3 & -4.80093 & -1.11237 & 1.64456 & 0.43256 & 0.150882 & -1.96967 & -1.00953 & $\\dots$ \\\\\n",
       "\t4 & -0.9105 & 3.6121 & 3.04094 & -0.193027 & -0.503773 & 1.19834 & 0.0913277 & $\\dots$ \\\\\n",
       "\t5 & -2.0373 & -1.95942 & -1.89185 & 2.83901 & -0.57782 & -1.20167 & 0.610392 & $\\dots$ \\\\\n",
       "\t6 & -3.51161 & -2.6956 & -1.02456 & 2.84096 & -2.98448 & -1.10298 & -2.11035 & $\\dots$ \\\\\n",
       "\t7 & -0.195082 & 2.47236 & 2.59916 & -2.40871 & 0.706896 & -0.892192 & -1.2048 & $\\dots$ \\\\\n",
       "\t8 & -0.797876 & 1.84563 & -0.726482 & 2.81892 & 0.720546 & 0.830249 & 0.397184 & $\\dots$ \\\\\n",
       "\t9 & -4.86489 & -4.25104 & -1.29747 & -4.83094 & 1.45364 & 0.540341 & 0.0524396 & $\\dots$ \\\\\n",
       "\t10 & 6.15691 & -1.93771 & 2.08371 & -0.372279 & 2.47822 & 0.316091 & 0.610564 & $\\dots$ \\\\\n",
       "\t11 & 1.50171 & 2.08525 & 1.65071 & 1.49292 & -0.696798 & 1.20394 & 1.3742 & $\\dots$ \\\\\n",
       "\t12 & -2.7647 & -1.72891 & 0.788022 & -3.02694 & -1.44399 & 1.14553 & -1.12226 & $\\dots$ \\\\\n",
       "\t13 & 4.36994 & -3.40533 & 3.23551 & 0.373285 & -2.6191 & -1.73785 & 1.13972 & $\\dots$ \\\\\n",
       "\t14 & 1.7031 & -7.0001 & -1.61518 & -0.286012 & -0.534901 & 2.0245 & -2.39803 & $\\dots$ \\\\\n",
       "\t15 & 3.99259 & 0.981788 & -0.233576 & 2.48452 & -0.618901 & 4.17176 & -1.01331 & $\\dots$ \\\\\n",
       "\t16 & -1.23352 & 0.0710929 & -0.364712 & 1.09898 & -3.22086 & 2.25045 & -0.335665 & $\\dots$ \\\\\n",
       "\t17 & 0.344227 & -3.93579 & 4.107 & 1.53589 & -1.66505 & -0.87872 & 1.70189 & $\\dots$ \\\\\n",
       "\t18 & -3.91414 & 1.94747 & -0.041728 & 0.0531202 & 0.876757 & 0.531566 & 1.9668 & $\\dots$ \\\\\n",
       "\t19 & 3.43669 & -7.42354 & -0.104201 & 0.0271859 & 1.34692 & -0.450634 & -3.08143 & $\\dots$ \\\\\n",
       "\t20 & -4.95677 & -2.69303 & -4.23915 & -0.618086 & 1.86556 & 0.855444 & 2.04613 & $\\dots$ \\\\\n",
       "\t21 & 1.33936 & 2.45737 & 1.43216 & -2.19311 & -0.587342 & 0.600994 & 0.399861 & $\\dots$ \\\\\n",
       "\t22 & -3.30661 & 0.905751 & 1.62176 & 0.52339 & -1.37562 & -1.08017 & -0.0834587 & $\\dots$ \\\\\n",
       "\t23 & -1.06801 & 4.04239 & 2.14627 & 0.219342 & -0.372899 & 1.47997 & -0.143439 & $\\dots$ \\\\\n",
       "\t24 & 2.2324 & -4.16226 & 1.89727 & 1.13633 & -2.51106 & -0.436396 & -2.14677 & $\\dots$ \\\\\n",
       "\t25 & -0.251205 & 2.75158 & 0.416979 & 0.75235 & 0.994299 & -0.0057112 & -2.15724 & $\\dots$ \\\\\n",
       "\t26 & -4.9656 & -1.48232 & -2.52856 & 0.229238 & 0.588992 & -0.447228 & 0.472838 & $\\dots$ \\\\\n",
       "\t27 & 1.77511 & 3.37361 & 1.17972 & -1.84829 & -0.49384 & 0.873874 & 0.361035 & $\\dots$ \\\\\n",
       "\t28 & -1.22507 & -0.780639 & 2.66728 & 1.87134 & 0.0966342 & 1.26229 & -1.88422 & $\\dots$ \\\\\n",
       "\t29 & 2.85916 & -1.77317 & 4.86359 & -1.17662 & -0.292665 & 1.103 & 2.09684 & $\\dots$ \\\\\n",
       "\t30 & 1.07933 & 3.25281 & -0.0256047 & -1.54861 & 0.305284 & -0.13893 & 0.0801751 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m62×26 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m x1        \u001b[0m\u001b[1m x2        \u001b[0m\u001b[1m x3         \u001b[0m\u001b[1m x4        \u001b[0m\u001b[1m x5         \u001b[0m\u001b[1m x6        \u001b[0m\u001b[1m x7 \u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Flo\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │  2.67422   -2.58367    1.84918     2.94977    2.25763     0.158967   3. ⋯\n",
       "   2 │  2.22217   -3.54915    3.27733    -1.42662   -2.76808    -1.50555    0.\n",
       "   3 │ -4.80093   -1.11237    1.64456     0.43256    0.150882   -1.96967   -1.\n",
       "   4 │ -0.9105     3.6121     3.04094    -0.193027  -0.503773    1.19834    0.\n",
       "   5 │ -2.0373    -1.95942   -1.89185     2.83901   -0.57782    -1.20167    0. ⋯\n",
       "   6 │ -3.51161   -2.6956    -1.02456     2.84096   -2.98448    -1.10298   -2.\n",
       "   7 │ -0.195082   2.47236    2.59916    -2.40871    0.706896   -0.892192  -1.\n",
       "   8 │ -0.797876   1.84563   -0.726482    2.81892    0.720546    0.830249   0.\n",
       "   9 │ -4.86489   -4.25104   -1.29747    -4.83094    1.45364     0.540341   0. ⋯\n",
       "  10 │  6.15691   -1.93771    2.08371    -0.372279   2.47822     0.316091   0.\n",
       "  11 │  1.50171    2.08525    1.65071     1.49292   -0.696798    1.20394    1.\n",
       "  ⋮  │     ⋮          ⋮          ⋮           ⋮          ⋮           ⋮          ⋱\n",
       "  53 │ -1.72368    2.16292    0.343966   -1.35125   -0.863455   -0.959135   1.\n",
       "  54 │ -6.21793   -4.50146   -0.0313274  -4.04019    2.93452    -0.685373   0. ⋯\n",
       "  55 │ -2.81615   -2.56034    1.29948     1.06481   -1.40427    -0.543933  -0.\n",
       "  56 │  0.448191   4.47613   -1.4542      1.1148     1.5241     -1.12173   -1.\n",
       "  57 │  0.684289   4.34004   -1.17939    -1.94878   -1.13129     1.45554   -0.\n",
       "  58 │  3.15653    2.80687   -3.02804    -1.146      1.11313    -1.24845    1. ⋯\n",
       "  59 │  2.48956    4.44447   -1.87949     1.55145    2.18462    -1.78627   -0.\n",
       "  60 │  2.87887    4.49478   -1.68724    -2.18313   -0.961492   -0.666446   0.\n",
       "  61 │  4.38225   -5.02433   -1.23436    -0.360916   1.85134     0.295121   2.\n",
       "  62 │  0.321653  -0.478661  -2.46761     0.543321  -0.645402    1.79911   -2. ⋯\n",
       "\u001b[36m                                                  20 columns and 41 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take the Test set as a dataframe\n",
    "Test = DataFrame(A[test,:],:auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c2ce94",
   "metadata": {},
   "source": [
    "# Logistic Regression \n",
    "The first classification model we are going to use is Linear Regression. The model basically assigns to each observation a probability of being of one or another type.<br>\n",
    "\n",
    "For this model we make use of the package GLM (Generalized Linear Models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "08617884",
   "metadata": {},
   "outputs": [],
   "source": [
    "using GLM\n",
    "# We build the model on the train data\n",
    "fm = @formula(x26 ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x15 + x16 + x17 + x18 + x19 + x20 + x21 + x22 + x23 + x24 + x25)\n",
    "logit = glm(fm, Train, Binomial(), ProbitLink())\n",
    "\n",
    "# We make the prediction on the test data\n",
    "prediction = GLM.predict(logit,Test)\n",
    "\n",
    "# Now we convert the probability that the model assigned to one of the label using 0.5 as the threshold\n",
    "prediction_class = [if x < 0.5 0 else 1 end for x in prediction]\n",
    "\n",
    "#here we can contrast the actual data (1st col) with the prediction made by the model(2nd col)\n",
    "prediction_df = DataFrame(y_actual = Test.x26, y_predicted = prediction_class, prob_predicted = prediction); #model predicted correctly and 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d697517",
   "metadata": {},
   "source": [
    "# Performance measure: Recall\n",
    "\n",
    "But what does the above information tells us about how good the model worked? In order to measure the performance of the model, one way to evaluate the performance is the recall: it tells you the true positive ratio of the labels that we tried to predict, that is\n",
    "$$\n",
    "Recall = \\frac{TP}{TP+FN}.\n",
    "$$\n",
    " We choose this measure beacuse we want to focus more on detecting true mines rather than worrying too much about false positive calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "95b9d07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall in Logistic Regression: 0.8484848484848485"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='0.0' and positive='1.0'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase C:\\Users\\jaime\\.julia\\packages\\MLJBase\\IVwqP\\src\\measures\\confusion_matrix.jl:116\n"
     ]
    }
   ],
   "source": [
    "# and finally we measure the performance of the model\n",
    "logit_recall = recall(prediction_class,Test.x26);\n",
    "print(\"Recall in Logistic Regression: $logit_recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f07360",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f4a22c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = @formula(x26 ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x15 + x16 + x17 + x18 + x19 + x20 + x21 + x22 + x23 + x24 + x25)\n",
    "linearRegressor = lm(fm, Train);\n",
    "prediction1 = GLM.predict(linearRegressor, Test);\n",
    "\n",
    "#Clasificación\n",
    "prediction_class1 = [if x < 0.5 0 else 1 end for x in prediction1];\n",
    "prediction1 =  prediction_class1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5edfa3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall in lin reg: 0.8181818181818182"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='0.0' and positive='1.0'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase C:\\Users\\jaime\\.julia\\packages\\MLJBase\\IVwqP\\src\\measures\\confusion_matrix.jl:116\n"
     ]
    }
   ],
   "source": [
    "# and finally we measure the performance of the model\n",
    "linear_recall = recall(prediction1,Test.x26);\n",
    "print(\"Recall in lin reg: $linear_recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8897b0c",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c40aaabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall in svm: 0.9090909090909091"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='0.0' and positive='1.0'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase C:\\Users\\jaime\\.julia\\packages\\MLJBase\\IVwqP\\src\\measures\\confusion_matrix.jl:116\n"
     ]
    }
   ],
   "source": [
    "using LIBSVM # SVM library\n",
    "using RDatasets\n",
    "\n",
    "# Here we set up the model on the train data\n",
    "X = Matrix(Train[:, 1:25])' # the X input has to be transposed\n",
    "y = Train.x26\n",
    "X0 = Matrix(Test[:, 1:25])'\n",
    "model = svmtrain(X, y); # svmtrain makes the svm model\n",
    "\n",
    "# Now we test on the test data\n",
    "(predicted_labels, decision_values) = svmpredict(model, X0);\n",
    "\n",
    "# And finally we measure performance\n",
    "svm_recall = recall(predicted_labels,Test.x26);\n",
    "print(\"Recall in svm: $svm_recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352d6132",
   "metadata": {},
   "source": [
    "# DecisionTree\n",
    "Next up we have Decision Trees: the model creates a tree which classifies the data by determining which features and with what thresholds split the data into uniform subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d0a33a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 3, Threshold 0.9266514009751665\n",
      "L-> Feature 9, Threshold -0.8352455675323365\n",
      "    L-> Feature 2, Threshold 0.7724766883400427\n",
      "        L-> Feature 22, Threshold 0.6548853999226476\n",
      "            L-> 1.0 : 15/15\n",
      "            R-> 0.0 : 2/2\n",
      "        R-> 0.0 : 5/5\n",
      "    R-> Feature 1, Threshold -2.4358247013729377\n",
      "        L-> Feature 19, Threshold -0.1044406986948024\n",
      "            L-> Feature 10, Threshold -1.9352262271401863\n",
      "                L-> 1.0 : 1/1\n",
      "                R-> 0.0 : 7/7\n",
      "            R-> Feature 2, Threshold 1.2460079662085652\n",
      "                L-> Feature 5, Threshold 1.1364641583657369\n",
      "                    L-> 1.0 : 9/9\n",
      "                    R-> 0.0 : 1/1\n",
      "                R-> 0.0 : 2/2\n",
      "        R-> Feature 22, Threshold -0.49042343958770407\n",
      "            L-> Feature 22, Threshold -0.9938535132870014\n",
      "                L-> 0.0 : 5/5\n",
      "                R-> Feature 24, Threshold -0.4217667681550584\n",
      "                    L-> 0.0 : 2/2\n",
      "                    R-> 1.0 : 6/6\n",
      "            R-> Feature 8, Threshold -3.3177003828574687\n",
      "                L-> 1.0 : 1/1\n",
      "                R-> Feature 15, Threshold -1.4295282520762609\n",
      "                    L-> Feature 5, Threshold -0.4697097614013564\n",
      "                        L-> 1.0 : 1/1\n",
      "                        R-> 0.0 : 1/1\n",
      "                    R-> 0.0 : 41/41\n",
      "R-> Feature 7, Threshold 3.000514705560902\n",
      "    L-> Feature 16, Threshold -2.180415785702394\n",
      "        L-> 0.0 : 1/1\n",
      "        R-> 1.0 : 45/45\n",
      "    R-> 0.0 : 1/1\n"
     ]
    }
   ],
   "source": [
    "using DecisionTree # This is the Julia library for Decision Trees model\n",
    "\n",
    "Xt = Matrix(Train[:, 1:25]);\n",
    "yt = Train.x26;\n",
    "\n",
    "tmodel = DecisionTreeClassifier() #this function creates a tree model to which one can specify or not the desired depht of the tree.\n",
    "\n",
    "DecisionTree.fit!(tmodel, Xt, yt) # we fit the previous model to the train data\n",
    "# and we can see the tree, which features it took and the corresponding thresholds\n",
    "print_tree(tmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "47d09e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall in DecisionTree: 0.6363636363636364"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='0.0' and positive='1.0'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase C:\\Users\\jaime\\.julia\\packages\\MLJBase\\IVwqP\\src\\measures\\confusion_matrix.jl:116\n"
     ]
    }
   ],
   "source": [
    "# here we test the model on the test data\n",
    "Xt0 = Matrix(Test[:,1:25])\n",
    "yt = DecisionTree.predict(tmodel, Xt0) # the DecisionTree Package has its own predict function\n",
    "\n",
    "# and finally we measure the performance of the model\n",
    "tree_recall = recall(yt,Test.x26);\n",
    "print(\"Recall in DecisionTree: $tree_recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209ae16e",
   "metadata": {},
   "source": [
    "# KNN\n",
    "Finally, we have the K-Nearest Neighbors model for classification: given a point $x$ that we want to classify, what the model does is to take the nearest $k$ neighbors of $x$, and see of which type are most of those neighbors in order to assign to $x$ such label. In order to avoid equal amount of neighbors of different type, one should choose and odd value of $k$. Also $k$ shall not be too big since taking too many neighbors can be counterproductive.<br>\n",
    "\n",
    "The algorithm itself sounds and is indeed very expensive in terms of computation time, but thanks to the $k$-dimensional tree structure (see https://en.wikipedia.org/wiki/K-d_tree), we can partition the set of point in order to search for neighbors in efficient time.<br>\n",
    "\n",
    "The NearestNeighbors package of julia implements the concepts mentioned above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "972c2d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62×5 Matrix{Int64}:\n",
       "  25  107   94   17   83\n",
       "  63   43   78   56   45\n",
       "   5   48   52   34   21\n",
       "  66   30   51   15   37\n",
       "  11    2  139   71   73\n",
       "   2   73   11   34    7\n",
       "  75   18   68   41    9\n",
       " 119   25  138   92   29\n",
       "  35   24   20   70   13\n",
       "  65   27   50    8  103\n",
       " 130  144   23   59  119\n",
       "  70  120    5   10  146\n",
       "  32   43   78   63   60\n",
       "   ⋮                 \n",
       "   2   10   93   85   26\n",
       " 104  109  111  110  130\n",
       "  55  130   77    1   44\n",
       "  13   70   35  101   67\n",
       " 136   84   74   52    5\n",
       " 110  104  109  111  144\n",
       " 105  143   82   59    1\n",
       "  96  118  134  111   90\n",
       " 104  110  109  111   96\n",
       " 130  118  126  105  112\n",
       "  87  102  103   65   47\n",
       " 108   93  140   71  115"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using NearestNeighbors, StatsBase #we add the knn package for julia\n",
    "# remember  X = Matrix(train[:, 1:60])'\n",
    "#           y = train.x61\n",
    "#           X0 = Matrix(test[:, 1:60])'\n",
    "\n",
    "kdtree = KDTree(X) # create the k-dimensional tree of the data\n",
    "\n",
    "k = 5 # choose an odd and not too big value for k\n",
    "\n",
    "index_knn, distances = knn(kdtree, X0, k, true) #this gives the indexes of the respective 5 nearest neighbors and the given distances to each point on the test data X0\n",
    "\n",
    "# now we put back that information in ma matrix where we can read it\n",
    "\n",
    "index_knn_matrix = hcat(index_knn...)\n",
    "# each row are the index of the nearest neighbors to the respective observation of the test dataset\n",
    "index_knn_matrix_t = permutedims(index_knn_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c3986ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we take the classes (labels) of the nearest neighbors to each observation from the test data\n",
    "knn_classes = y[index_knn_matrix_t]\n",
    "\n",
    "# now we make the prediction y_hat by taking the classes that appear the most on those 5 neighbors, \n",
    "# we do that by counting the classes with countmap function and then with the argmax function from StastBase package:\n",
    "\n",
    "y_hat = [\n",
    "    argmax(countmap(knn_classes[i, :], alg = :dict))\n",
    "    for i in 1:62\n",
    "];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9a5f2e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall in KNN: 0.7575757575757576"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='0.0' and positive='1.0'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase C:\\Users\\jaime\\.julia\\packages\\MLJBase\\IVwqP\\src\\measures\\confusion_matrix.jl:116\n"
     ]
    }
   ],
   "source": [
    "# and finally we measure the performance of the model\n",
    "knn_recall = recall(y_hat,Test.x26);\n",
    "print(\"Recall in KNN: $knn_recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c17e039",
   "metadata": {},
   "source": [
    "# Which model did better?\n",
    "\n",
    "We can see, based merely on the performance measure of each model, that the one that did better was the SVM. For further studies and in order to determine which of the models work better in a more deep way, we can try hyperparameters tuning and try cross-validation with a subset of the data in order to train a better model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
